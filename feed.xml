<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>YeonjeeJung's Blog</title>
    <description></description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sun, 27 Oct 2019 22:45:45 +0900</pubDate>
    <lastBuildDate>Sun, 27 Oct 2019 22:45:45 +0900</lastBuildDate>
    <generator>Jekyll v3.8.6</generator>
    
      <item>
        <title>SGDR - Stochastic Gradient Descent with warm Restarts</title>
        <description>&lt;hr /&gt;

&lt;h2 id=&quot;abstract&quot;&gt;[Abstract]&lt;/h2&gt;
&lt;p&gt;재시작 방법은 gradient-free 최적화에서 멀티모달 함수에 적용할 때 자주 쓰인다. 부분적 재시작 또한 gradient기반 최적화에서 ill-conditioned 함수에서 수렴도를 개선하기 위해 자주 쓰이는 추세이다. 이 논문에서는 SGD를 위한 간단한 재시작 테크닉을 소개하는데, 딥네트워크를 학습시킬 때 항상(anytime) 결과를 향상시킬 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;1-introduction&quot;&gt;[1] Introduction&lt;/h2&gt;
&lt;p&gt;GD를 쓸때 hessian을 쓰면 더 좋은데 계산량이 많다. AdaDelta와 Adam은 hessian을 잘 줄여서 사용한 좋은 예이다. 그런데 sota 결과는 사실 특별한 방법을 쓴게 아니라 SGD에 momentum만 추가한 것이었다.&lt;/p&gt;

&lt;p&gt;보통의 learning rate schedule은 정해진 상수를 일정 간격의 상수로 나누는 것이었는데, 이 논문에서 제안하는 새로운 learning rate schedule은 주기적으로 SGD를 재시작하는 방법이다. 실험 결과에 의하면 재시작 방법은 원래 쓰이던 방법보다 2배에서 4배정도 적은 epoch만으로 비슷한 결과를 낼 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;2-related-work&quot;&gt;[2] Related Work&lt;/h2&gt;
&lt;p&gt;gradient-free optimization에서는 많은 local optima를 찾는 것이 목적이다. niching 방법 기반 방법들은 local optimizer를 전체 space에 다 적용시킬 수 있는데, 차원의 저주 때문에 확장시킬 수는 없다. 최근에는 다양한 재시작 매커니즘들을 사용하는데, 한 방법에서는 많은 후보를 쓰면 더 글로벌한 검색이 가능한데, 각 재시작 처음엔 적은 후보를 쓰고 각 재시작 후에는 키우는 방법을 사용하는 것이 일반적이다.&lt;/p&gt;

&lt;p&gt;gradient-based optimization에서는 gradient-free 에서보다 $n$배의 속도 향상이 있다. 이때 재시작 방법은 multimodality를 해결하기 위함보다는 수렴도를 개선하기 위해 사용된다.&lt;/p&gt;

&lt;h2 id=&quot;3-sgdr&quot;&gt;[3] SGDR&lt;/h2&gt;
&lt;p&gt;현존하는 재시작 방법은 SGD에도 적용될 수 있다. 데이터의 덩어리에 따라 loss value와 기울기가 다양할 수 있어서, 기울기나 loss의 평균을 내는 등의 노이즈의 제거가 필요하다.&lt;/p&gt;

&lt;p&gt;이 논문에서는 지정된 epoch까지 도달하면 다시 재시작을 하는 가장 간단한 재시작 방법을 사용한다. 그리고 제안된 cosine annealing이라고도 불리는 learning rate schedule은 다음과 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\eta_t=\eta_{min}^i+\frac{1}{2}(\eta_{max}^i−\eta_{min}^i)(1 + \cos(\frac{T_{cur}}{T_i}\pi))&lt;/script&gt;

&lt;p&gt;여기서 $\eta_t$는 learning rate, $\eta_{min}^i$와 $\eta_{max}^i$는 learning rate의 범위, $T_{cur}$는 현재 epoch, $T_i$는 지정된 epoch(이만큼 지나면 재시작)이다.&lt;/p&gt;

&lt;p&gt;재시작은 learning rate ($\eta_t$)을 증가시키므로써 수행되고, $x_t$는 초기 해로 사용된다. learning rate는 $\eta_{max}^i$부터 $\eta_{min}^i$까지 줄어들고, 정해진 epoch를 돌면 다시 처음부터 시작한다. $T_{mult}$라는 변수를 이용하여 재시작마다 줄어드는 간격을 점점 넓힐 수도 있다.&lt;/p&gt;

&lt;p&gt;처음 재시작 전에는 $x_t$를 초기 해로 사용하지만, 그 다음에는 이전의 최소 learning rate로부터 얻어진 $x$를 초기 해로 사용한다. (이 점이 중요한 부분임)
&lt;em&gt;그런데 계속 저렇게 사용하는거? 아님 재시작마다 저렇게 사용하는거?&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;4-experimental-results&quot;&gt;[4] Experimental Results&lt;/h2&gt;
&lt;h3 id=&quot;42-single-model-result&quot;&gt;[4.2] Single-Model Result&lt;/h3&gt;
&lt;p&gt;$T_0=200$의 결과가 가장 좋은데, 가장 마지막 몇 epoch에서 좋아진다. $T_{mult}=2$는 재시작 후 주기를 2배로 늘려주는데, 이렇게 하는 이유는 좋은 테스트 에러에 가장 빨리 도달하기 위함이다. SGDR이 좋은 성능에 빠르게 도달할 수 있기 때문에, 더 큰 신경망을 학습시킬 수 있다. 따라서 WRN을 2배 넓게 만들어서 학습시켰다.&lt;/p&gt;

&lt;p&gt;SGDR 자체 실험에서는 SGDR과 기본 스케줄을 비교했는데, 120 epoch까지는 더 빨리 training loss가 줄었다. 이 이후에 기본 스케줄은 overfit되었다. 결론적으로, SGDR은 overfit이 잘 되지 않는다.&lt;/p&gt;

&lt;h3 id=&quot;43-ensemble-results&quot;&gt;[4.3] Ensemble Results&lt;/h3&gt;
&lt;p&gt;SGDR은 WRN논문의 follow-up study에서 영감을 얻었다.  여기서는 재시작 전마다 snapshot을 찍고 그것으로 앙상블 모델을 만든다. 결과로는, 3번 돌려서 앙상블하는 것보다 한번 돌려서 3번 스냅샷 찍어서 앙상블하는게 낫다. SGDR에서 찍은 스냅샷은 앙상블을 할 때의 유용한 다양성을 제공해 준다. 이 결과는 WRN보다 더 좋은 모델에서 더 좋은 결과를 낼 것이다.&lt;/p&gt;

&lt;h3 id=&quot;45-preliminary-experiments-on-a-downsampled-imagenet-dataset&quot;&gt;[4.5] Preliminary Experiments on a Downsampled ImageNet Dataset&lt;/h3&gt;
&lt;p&gt;다운샘플된 이미지넷 데이터는 원래보다 더 어렵고 이미지의 대부분을 대상이 차지하는 CIFAR-10보다도 더 어렵다.&lt;/p&gt;

&lt;h2 id=&quot;5-discussion&quot;&gt;[5] Discussion&lt;/h2&gt;
&lt;p&gt;이 learnin rate schedule은 재시작 없이도 충분히 경쟁적이고, 단 두 개의 파라미터(초기 lr, epoch 수)만을 필요로 한다. 재시작 방법의 목적은 ‘항상(anytime)’ 좋은 성능을 내기 위함이다. 매 재시작마다 $\eta_{max}$와 $\eta_{min}$을 줄이는 방법도 가능하다. SGDR 중 얻어진 중간 모델은 앙상블에 사용될 수 있고, cost도 들지 않는다는 점을 이용했다.&lt;/p&gt;

&lt;h2 id=&quot;6-conclusion&quot;&gt;[6] Conclusion&lt;/h2&gt;
&lt;p&gt;WRN에서는 더 넓은 모델을 사용하고 스냅샷을 앙상블에 사용해 sota 결과를 만들어냈고, EEG에서는 더 재시작을 많이 하고 더 스냅샷을 많이 찍으면 더 좋은 결과를 낸다는 것을 알았다. Downsampled ImageNet 데이터에서는 SGDR이 scan을 통해 lr을 선택하는 문제를 줄여준다는 것을 알았다. 다음 연구는 AdaDelta나 Adam에 적용하는 것이 될 것이다.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;다음에 읽어볼 논문 : &lt;a href=&quot;https://arxiv.org/abs/1704.00109&quot;&gt;Snapshot&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1605.07146&quot;&gt;WRN&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Sun, 27 Oct 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/thesis/2019/10/27/SGDR.html</link>
        <guid isPermaLink="true">http://localhost:4000/thesis/2019/10/27/SGDR.html</guid>
        
        <category>Optimization</category>
        
        
        <category>Thesis</category>
        
      </item>
    
      <item>
        <title>Optlecture12</title>
        <description>
</description>
        <pubDate>Tue, 08 Oct 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/lecture/optimization/2019/10/08/OptLecture12.html</link>
        <guid isPermaLink="true">http://localhost:4000/lecture/optimization/2019/10/08/OptLecture12.html</guid>
        
        
        <category>Lecture</category>
        
        <category>Optimization</category>
        
      </item>
    
      <item>
        <title>Optlecture11</title>
        <description>
</description>
        <pubDate>Sun, 06 Oct 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/lecture/optimization/2019/10/06/OptLecture11.html</link>
        <guid isPermaLink="true">http://localhost:4000/lecture/optimization/2019/10/06/OptLecture11.html</guid>
        
        
        <category>Lecture</category>
        
        <category>Optimization</category>
        
      </item>
    
      <item>
        <title>Optlecture10</title>
        <description>
</description>
        <pubDate>Tue, 01 Oct 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/lecture/optimization/2019/10/01/OptLecture10.html</link>
        <guid isPermaLink="true">http://localhost:4000/lecture/optimization/2019/10/01/OptLecture10.html</guid>
        
        
        <category>Lecture</category>
        
        <category>Optimization</category>
        
      </item>
    
      <item>
        <title>Optlecture9</title>
        <description>
</description>
        <pubDate>Mon, 30 Sep 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/lecture/optimization/2019/09/30/OptLecture9.html</link>
        <guid isPermaLink="true">http://localhost:4000/lecture/optimization/2019/09/30/OptLecture9.html</guid>
        
        
        <category>Lecture</category>
        
        <category>Optimization</category>
        
      </item>
    
      <item>
        <title>Optlecture8</title>
        <description>
</description>
        <pubDate>Wed, 25 Sep 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/lecture/optimization/2019/09/25/OptLecture8.html</link>
        <guid isPermaLink="true">http://localhost:4000/lecture/optimization/2019/09/25/OptLecture8.html</guid>
        
        
        <category>Lecture</category>
        
        <category>Optimization</category>
        
      </item>
    
      <item>
        <title>Optimization Lecture 7</title>
        <description>&lt;h1 id=&quot;barrier-method--proximal-gradient&quot;&gt;Barrier Method &amp;amp; Proximal Gradient&lt;/h1&gt;

&lt;h2 id=&quot;barrier-method&quot;&gt;Barrier Method&lt;/h2&gt;

&lt;h3 id=&quot;inequality-constrained-problems&quot;&gt;Inequality Constrained Problems&lt;/h3&gt;

&lt;p&gt;최적화해야 하는 함수 $f(x)$가 제한된 집합 $X$에서 정의될 때 projected gradient descent 외의 다른 방법을 소개한다. 이런 constrained optimization 문제를 다르게 말하면, ‘어떤 함수 $h, g$에 대하여 모든 $1\le i \le m$인 $i$들에 대해 $h_i(x)=0$을 만족하고, 모든 $1\le j \le r$인 $j$들에 대해 $g_j(x)\le 0$을 만족하면서 $f(x)$를 최소화시켜라’라고 표현할 수 있다. 이것을 Inequality Constrained Problem이라고 한다.&lt;/p&gt;

&lt;h3 id=&quot;barrier-method-1&quot;&gt;Barrier Method&lt;/h3&gt;

&lt;p&gt;Barrier Method란, constrained 문제를 unconstrained 문제로 바꾸는 방법을 말한다. 이 때 원래의 최적화해야 하는 함수 뒤에 penalty function을 더하게 된다. 그런데 여기서, 과연 이렇게 찾은 해가 우리가 실제로 원하던 해일까? 라는 의문을 갖게 된다.&lt;/p&gt;

&lt;h3 id=&quot;lagrange-multipliers&quot;&gt;Lagrange Multipliers&lt;/h3&gt;

&lt;p&gt;라그랑지 계수로 이를 해결할 수 있는데, 위에서 소개한 Inequality Constrained Problem을 이용하자. 먼저 Lagrange function을 다음과 같이 정의할 수 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Lambda(x, \mu, \lambda):=f(x) + \sum_{i=1}^m{\mu_ih_i(x)} + \sum_{j=1}^r{\lambda_jg_j(x)}&lt;/script&gt;

&lt;p&gt;여기서 $\mu_1, \cdots, \mu_m, \lambda_1, \cdots, \lambda_r$을 Lagrange multiplier라고 한다.&lt;/p&gt;

&lt;h3 id=&quot;lagrange-dual&quot;&gt;Lagrange Dual&lt;/h3&gt;

&lt;p&gt;다음은 Lagrange function을 최적화하는 방법이다. 먼저 Lagrange dual function과 Lagrange dual problem을 정의한다.&lt;/p&gt;

&lt;p&gt;Lagrange dual function :&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L(\mu, \lambda) := \min_x\Lambda(x, \mu, \lambda)&lt;/script&gt;

&lt;p&gt;Lagrange dual problem :&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\max_{\mu, \lambda}L(\mu, \lambda) \text{ s.t. }\lambda_j\ge, \forall 1 \le j\le r&lt;/script&gt;

&lt;p&gt;먼저 Lagrange dual function 값을 구한 뒤, Lagrange dual problem을 푼다. 그 후 $L(\mu, \lambda)$를 최대화시키는 $\lambda$와 $\mu$를 찾아서 $\Lambda(x, \mu, \lambda)$에 넣고 다시 Lagrange dual function을 풀고 $\cdots$를 반복한다. 최종 나오는 $x$값이 우리가 원하던 $x^* $값이다.&lt;/p&gt;

&lt;p&gt;Lagrange dual problem을 푸는 과정에서는, $L(\mu, \lambda)$가 concave function이라는 점을 활용하면 gradient ascent를 이용해서 maximization을 할 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;proof-lmu-lambda-is-convex&quot;&gt;(Proof) $L(\mu, \lambda)$ is convex&lt;/h3&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}&amp;L(\alpha \mu^{(1)}+(1-\alpha)\mu^{(2)}, \alpha\lambda^{(1)}+(1-\alpha)\lambda^{(2)})\\
&amp;=\min_x(f(x)+\sum(\alpha\mu^{(1)}+(1-\alpha)\mu^{(2)})h_i(x) + \sum(\alpha\lambda^{(1)}+(1-\alpha)\lambda^{(2)})g_j(x))\\
&amp;\ge \alpha(\min(f(x)+\sum\mu^{(1)}h_i(x))+\sum\lambda^{(1)}g_j(x))+(1-\alpha)(\min(f(x)+\sum\mu^{(2)}h_i(x))+\sum\lambda^{(2)}g_j(x))\\
&amp;=\alpha L(\mu^{(1)}, \lambda^{(1)})+(1-\alpha)L(\mu^{(2)}, \lambda^{(2)})\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;따라서 $L(\mu, \lambda)$는 concave 함수이다.&lt;/p&gt;

&lt;h3 id=&quot;lagrange-dual-and-barrier&quot;&gt;Lagrange Dual and Barrier&lt;/h3&gt;

&lt;p&gt;다시 Barrier로 돌아오면, Lagrange dual에서 penalty function은 $\sum\mu_ih_i(x)+\sum\lambda_jg_j(x)$이다. Lagrange dual problem에서 우리는 $L(\mu, \lambda)$를 최대화시켜야 하므로,&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;$g_j(x)\lt 0$이면 $\lambda_j^* =0$이어야 하고,&lt;/li&gt;
  &lt;li&gt;$g_j(x) \gt 0$이면 $\lambda_j^* = \infty$,&lt;/li&gt;
  &lt;li&gt;$g_j(x)=0$이면 $\lambda_j^* $는 아무 숫자나 되어도 상관없으므로 $\lambda_j^* \gt 0$이게 된다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;barrier의 관점에서 봐도 말이 된다. barrier를 만족하지 못하면 penalty function이 무한대로 가게 되므로, 해가 constrained set 안으로 무조건 들어가도록 하게 된다. 따라서, Lagrange dual로 구한 $x^* $는 우리가 원하던 최적 해가 맞다.&lt;/p&gt;

&lt;p&gt;이렇게 구한 해인 $x^* (\mu^* , \lambda^* )$에는 몇 가지의 특성이 있다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;$\triangledown f(x^* (\mu^* , \lambda^* ))+\sum\mu_i^* \triangledown h_i(x^* (\mu^* , \lambda^* ))+\sum\lambda_j^* \triangledown g_j(x^* (\mu^* , \lambda^* ))=0$&lt;/li&gt;
  &lt;li&gt;$h_i(x^* (\mu^* , \lambda^* ))=0, \forall i$&lt;/li&gt;
  &lt;li&gt;$\lambda_j^* g_j(x^* (\mu^* , \lambda^* ))=0 \text{ when }\lambda_j^* =0, g_j(x^* (\mu^* , \lambda^* ))\lt 0$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;$f(x)$나 constrained set이 convex가 아니면 진짜 해와 우리가 구한 해가 차이가 나게 되는데, 이것을 Lagrange Gap이라고 한다.&lt;/p&gt;

&lt;h2 id=&quot;proximal-gradient&quot;&gt;Proximal Gradient&lt;/h2&gt;

&lt;h3 id=&quot;proximal-gradient-descent&quot;&gt;Proximal Gradient Descent&lt;/h3&gt;

&lt;p&gt;일반적으로 우리는 $f(x)$가 미분가능한 함수라고 생각하고 문제를 풀었지만, 사실 그렇지 않은 경우가 더 많다. 이런 경우를 잘 해결하기 위해 $f(x)$를 $g(x)$와 $h(x)$로 쪼갤 수 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(x) = g(x) + h(x)&lt;/script&gt;

&lt;p&gt;여기서 $g(x)$는 미분가능한 nice function이고, $h(x)$는 미분불가능할 수도 있지만 해석하기 쉬운 additional function 이다. $g(x)$와 $h(x)$는 둘다 convex이다.&lt;/p&gt;

&lt;p&gt;사실 gradient descent 방법의 함수 다음 $x$를 구하는 방법의 식은 다음과 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}x_{t+1} &amp;= x_t - \gamma\triangledown f(x)\\
&amp;= \arg\min_y\{f(x_t)+\triangledown f(x_t)^T(y-x_t)+\frac{1}{2\gamma}\|y-x_t\|^2\}\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;테일러 전개를 이용한 것인데, 마지막 항은 $\triangledown^2f(x)=\frac{1}{\gamma}I$로 대체한 것이다. &lt;em&gt;왜 이렇게 대체를 할 수 있는지는 모르겠다&lt;/em&gt; 다시 써보면,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}x_{t+1}&amp;=\arg\min_y\{g(y)+h(y)\}\\
&amp;=\arg\min_y\{g(x_t) + \triangledown g(x_t)^T(y-x_t)+\frac{1}{2\gamma}\|y-x_t\|^2 + h(y)\}\\
&amp;=\arg\min_y\{\triangledown g(x_t)^T(y-x_t)+\frac{1}{2\gamma}\|y-x_t\|^2 \frac{\gamma}{2}\|\triangledown g(x_t)\|^2+ h(y)\}\\
&amp;=\arg\min_y\{\frac{1}{2\gamma}(\|y-x_t\|^2 + 2\gamma\triangledown g(x_t)^T(y-x_t)+\gamma^2\|\triangledown g(x_t)\|^2)+h(y)\}\\
&amp;=\arg\min_y\{\frac{1}{2\gamma}\|y-(x_t-\gamma\triangledown g(x_t))\|^2+h(y)\}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;이다. 여기서 &lt;script type=&quot;math/tex&quot;&gt;\text{prox}_{h, \gamma}(z) = \arg\min_y\{\frac{1}{2\gamma}\|y-z\|^2+h(y)\}&lt;/script&gt;라고 정의하면,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x_{t+1} = \text{prox}_{h, \gamma}(x_t-\gamma\triangledown g(x_t))&lt;/script&gt;

&lt;p&gt;라고 쓸 수 있다. 항상 이 방법이 좋은 것은 아니며, $f(x)$를 어떤 $g(x)$와 $h(x)$로 나누는지에 따라 효과가 달라진다.&lt;/p&gt;

&lt;h3 id=&quot;generalized-gradient&quot;&gt;Generalized Gradient&lt;/h3&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;G_{h, \gamma}(x) = \frac{1}{\gamma}(x-\text{prox}_{h, \gamma}(x-\gamma\triangledown g(x)))&lt;/script&gt;

&lt;p&gt;라고 하면, $x_{t+1}= x_t-\gamma G_{h, \gamma}(x)$라고 할 수 있다. 이 식으로 그냥 gradient descent와 projected gradient descent도 포함시킬 수 있는데, 만약 $h(x)=0$이면 그냥 gradient descent이고, &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
h(x)=\begin{cases}0 &amp; \text{if }x\in X \\ \infty &amp; \text{otherwise}\end{cases} %]]&gt;&lt;/script&gt; 라고 정의한다면 projected gradient descent이다.&lt;/p&gt;
</description>
        <pubDate>Mon, 23 Sep 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/lecture/optimization/2019/09/23/OptLecture7.html</link>
        <guid isPermaLink="true">http://localhost:4000/lecture/optimization/2019/09/23/OptLecture7.html</guid>
        
        
        <category>Lecture</category>
        
        <category>Optimization</category>
        
      </item>
    
      <item>
        <title>Optimization Lecture 6</title>
        <description>&lt;h1 id=&quot;projected-gradient-descent&quot;&gt;Projected Gradient Descent&lt;/h1&gt;

&lt;h2 id=&quot;projected-gradient--alpha-strongly-convex--beta-smooth&quot;&gt;Projected Gradient : $\alpha$-strongly convex &amp;amp; $\beta$-smooth&lt;/h2&gt;

&lt;h3 id=&quot;recall-unconstrained-vanilla-analysis&quot;&gt;(Recall) Unconstrained Vanilla Analysis&lt;/h3&gt;

&lt;p&gt;vanilla analysis에서는&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}\|x_{t+1}-x^* \|^2&amp;\le\frac{2}{\beta}(f(x^* )-f(x_t))+\frac{1}{\beta}\|\triangledown f(x_t)\|^2+(1-\frac{\alpha}{beta})\|x_t-x^* \|^2\\
&amp;\le(1-\frac{\alpha}{\beta})\|x_t-x^* \|^2\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;을 얻을 수 있었다. 이 때 마지막 부등식은&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(x^* )-f(x_t)\le f(x_{t+1})-f(x_t)\le-\frac{1}{2\beta}\|\triangledown f(x_t)\|^2\cdots(* 1)&lt;/script&gt;

&lt;p&gt;라는 성질에 의해&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{2}{\beta}(f(x^* )-f(x_t))\le -\frac{1}{\beta^2}\|\triangledown f(x_t)\|^2&lt;/script&gt;

&lt;p&gt;를 얻어서 성립한 것이다.&lt;/p&gt;

&lt;h3 id=&quot;constrained-vanilla-analyasis&quot;&gt;Constrained Vanilla Analyasis&lt;/h3&gt;

&lt;p&gt;constrained에서는 $(* 1)$가 아닌&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(x^* )-f(x_t)\le f(x_{t+1})- f(x_t)\le-\frac{1}{2\beta}\|\triangledown f(x_t)\|^2+\frac{\beta}{2}\|y_{t+1}-x_{t+1}\|^2\cdots(* 2)&lt;/script&gt;

&lt;p&gt;가 성립한다. 따라서,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}\|x_{t+1}-x^* \|^2&amp;\le\frac{2}{\beta}(f(x^* )-f(x_t))+\frac{1}{\beta^2}\|\triangledown f(x_t)\|^2-\|y_{t+1}-x_{t+1}\|^2+(1-\frac{\alpha}{\beta})\|x_t-x^* \|^2\\
&amp;\le(1-\frac{\alpha}{\beta})\|x_t-x^* \|^2(\text{by }(* 2))\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;가 성립하므로,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}\|x_T-x^* \|^2&amp;\le(1-\frac{\alpha}{\beta})^T\|x_0-x^* \|^2\\
\|x_T-x^* \|&amp;\le(1-\frac{\alpha}{\beta})^{\frac{T}{2}}\|x_0-x^* \|\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;이다. 따라서&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}f(x_T)-f(x^* )&amp;\le\triangledown f(x^* )^T(x_T-x^* )+\frac{\beta}{2}\|x_T-x^* \|^2\\
&amp;\le\|\triangledown f(x^* )\|\|x_T-x^* \|+\frac{\beta}{2}\|x_T-x^* \|^2\\
&amp;\le\|\triangledown f(x^* )\|(1-\frac{\alpha}{\beta})^{\frac{T}{2}}\|x_0-x^* \|+\frac{\beta}{2}(1-\frac{\alpha}{\beta})^T\|x_0-x^* \|^2\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;결론적으로, Projected Gradient Descent를 사용하면 Unconstrained에서와 거의 비슷하게 수렴하고 bound하게 된다. 하지만 Projection에 많은 계산량이 사용되기 때문에 이 방법은 거의 쓰지 않는다.&lt;/p&gt;
</description>
        <pubDate>Wed, 18 Sep 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/lecture/optimization/2019/09/18/OptLecture6.html</link>
        <guid isPermaLink="true">http://localhost:4000/lecture/optimization/2019/09/18/OptLecture6.html</guid>
        
        
        <category>Lecture</category>
        
        <category>Optimization</category>
        
      </item>
    
      <item>
        <title>Optimization Lecture 5</title>
        <description>&lt;h1 id=&quot;projected-gradient-descent&quot;&gt;Projected Gradient Descent&lt;/h1&gt;

&lt;h2 id=&quot;constrained-optimization&quot;&gt;Constrained Optimization&lt;/h2&gt;

&lt;p&gt;Constrained Problem은 $f(x)$를 최소화하는 $x$를 찾는 문제인데, $x$의 범위 $X$가 주어져 있다는 점에서 이전과 다르다. 이 문제를 해결하는 방법에는 두 가지가 있는데,&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Projected Gradient Descent를 이용하는 방법과&lt;/li&gt;
  &lt;li&gt;unconstrained problem으로 바꿔서 해결하는 방법이 있다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;이번 단원은 첫번째 방법에 대한 내용이다.&lt;/p&gt;

&lt;h2 id=&quot;projected-gradient-descent-1&quot;&gt;Projected Gradient Descent&lt;/h2&gt;

&lt;p&gt;Project 는 &lt;script type=&quot;math/tex&quot;&gt;\Pi_X(y):=\arg\min_{x\in X}\|x-y\|&lt;/script&gt;, 즉 $X$바깥에 있는 $y$와 가장 가까운 $x\in X$에 매칭시켜주는 것이다.&lt;/p&gt;

&lt;p&gt;Projected gradient를 업데이트 하는 방법은 &lt;script type=&quot;math/tex&quot;&gt;x_{t+1}=\Pi_X(x_t-\gamma\triangledown f(x_t))&lt;/script&gt;이다.&lt;/p&gt;

&lt;h2 id=&quot;propconvex-constrained-problem&quot;&gt;(Prop)Convex Constrained Problem&lt;/h2&gt;

&lt;p&gt;$f$가 convex function이고 $X$가 convex set이고, $x^* $가 $X$범위에 대해 $f$의 minimizer라면,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}f(x^* )\le f(y), \forall y\in X \Leftrightarrow \triangledown f(x^* )^T(x-x^* )\ge 0\cdots(* 1)\end{align}&lt;/script&gt;

&lt;p&gt;이 성립한다.&lt;/p&gt;

&lt;h3 id=&quot;proof-leftarrow&quot;&gt;(Proof) $\Leftarrow$&lt;/h3&gt;

&lt;p&gt;$f$가 convex function이라고 했으므로,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(x)\ge f(x^* ) + \triangledown f(x^* )^T(x-x^* )&lt;/script&gt;

&lt;p&gt;를 항상 만족한다. 따라서 $\triangledown f(x^* )^T(x-x^* )\ge 0$이면 $f(x)\ge f(x^* )$이므로 증명은 끝난다.&lt;/p&gt;

&lt;h3 id=&quot;proof-rightarrow&quot;&gt;(Proof) $\Rightarrow$&lt;/h3&gt;

&lt;p&gt;대우명제를 이용한다. $\triangledown f(x^* )^T(x-x^* )\lt 0$이라고 하자. 그러면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\triangledown f(x^* )^T(x-x^* )=\lim_{t\rightarrow 0+}\frac{f(x^* +t(x-x^* ))-f(x^* )}{t(x-x^* )}(x-x^* )\lt 0&lt;/script&gt;

&lt;p&gt;이 성립하므로,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(x^* +t(x-x^* ))-f(x^* )\lt 0&lt;/script&gt;

&lt;p&gt;이 성립하게 된다. 이는 $f(x^* )\le f(y), \forall y\in X$에 부합하지 않으므로, 증명은 끝난다.&lt;/p&gt;

&lt;h2 id=&quot;prop-properties-of-projection&quot;&gt;(Prop) Properties of Projection&lt;/h2&gt;

&lt;p&gt;$X$가 closed 이고 convex라고 하고, $x\in X, y\in \mathbb{R}^d$라고 하자. 그러면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}&amp;(x-\Pi_X(y))^T(y-\Pi_X(y))\le 0\cdots(* 2)\\
&amp;\|x-\Pi_X(y)\|^2+\|y-\Pi_X(y)\|^2\le\|x-y\|^2\cdots(* 3)\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;가 항상 성립한다.&lt;/p&gt;

&lt;h3 id=&quot;proof--2&quot;&gt;(Proof) $(* 2)$&lt;/h3&gt;

&lt;p&gt;$\Pi_X(y)$는 &lt;script type=&quot;math/tex&quot;&gt;d(x)=\|x-y\|^2&lt;/script&gt;의 minimizer이므로, $(* 1)$의 $\Rightarrow$를 이용하면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\triangledown d(\Pi_X(y))^T(x-\Pi_X(y))\ge 0&lt;/script&gt;

&lt;p&gt;이 항상 성립한다. &lt;script type=&quot;math/tex&quot;&gt;d(x) = \|x-y\|^2=(x-y)^T(x-y)&lt;/script&gt;이므로, &lt;script type=&quot;math/tex&quot;&gt;\triangledown d(x) = 2(x-y)&lt;/script&gt;이다. 따라서&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;(\Pi_X(y)-y)^T(x-\Pi_X(y))\ge 0&lt;/script&gt;

&lt;p&gt;이고, 바꿔 말하면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;(x-\Pi_X(y))^T(y-\Pi_X(y))\le 0&lt;/script&gt;

&lt;p&gt;이다.&lt;/p&gt;

&lt;h3 id=&quot;proof--3&quot;&gt;(Proof) $(* 3)$&lt;/h3&gt;

&lt;p&gt;$v:=\Pi_X(y), w:=y-\Pi_X(y)$라고 하자. $(* 2)$에 의해&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;0\le 2v^Tw = \|v\|^2+\|w\|^2-\|v-w\|^2&lt;/script&gt;

&lt;p&gt;이므로,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\|x-\Pi_X(y)\|^2+\|y-\Pi_X(y)\|^2\le\|x-y\|^2&lt;/script&gt;

&lt;p&gt;가 성립한다.&lt;/p&gt;

&lt;h2 id=&quot;projected-gradient--lipschitz-convex&quot;&gt;Projected Gradient : Lipschitz Convex&lt;/h2&gt;

&lt;p&gt;$f:\mathbb{R}^d\rightarrow\mathbb{R}$가 convex function이고 미분가능하다고 하자. $X\subset\mathbb{R}^d$는 closed 이고 convex라고 하고, $x^* \in X$는 $f$에 대한 minimizer라고 하자. &lt;script type=&quot;math/tex&quot;&gt;\|x_0-x^* \|\le R, x_0\in X&lt;/script&gt;이고, &lt;script type=&quot;math/tex&quot;&gt;\|\triangledown f(x)\|\le B, \forall x\in X&lt;/script&gt;라고 하자 (B-Lipschitz continuous). step size $\gamma = \frac{R}{B\sqrt{T}}$라고 지정하면,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(\bar{x})-f(x^* )\le\frac{1}{T}\sum_{t=0}^{T-1}f(x_t)-f(x^* )\le\frac{RB}{\sqrt{T}}&lt;/script&gt;

&lt;p&gt;의 boundary를 갖는다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Lipschitz Continuous에서는 $f(x_{t+1})\le f(x_t)$를 보장하지 못하기 때문에, $f(x_T)\le f(x_t)$라고 장담할 수가 없다. 따라서 $f(\bar{x})-f(x^ *)$를 bound시키는 것이다.&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;vanilla-analysis&quot;&gt;Vanilla Analysis&lt;/h3&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;y_{t+1}=x_t-\gamma\triangledown f(x_t), x_{t+1}=\Pi_X(y_{t+1})&lt;/script&gt;이라고 하자. vanilla anaylsis를 이용하면,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}f(x_t)-f(x^* ) &amp; \le \triangledown f(x_t)^T(x_t-x^* )\\
&amp;=\frac{1}{\gamma}(x_t-y_{t+1})^T(x_t-x^* )\\
&amp;=\frac{1}{2\gamma}(\|x_t-y_{t+1}\|^2+\|x_t-x^* \|^2-\|y_{t+1}-x^* \|^2)\\
&amp;\le\frac{\gamma}{2}\|\triangledown f(x_t)\|^2+\frac{1}{2\gamma}(\|x_t-x^* \|-\|x_{t+1}-x^* \|^2)-\frac{1}{2\gamma}\|y_{t+1}-x_{t+1}\|^2
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;(마지막 줄은 &lt;script type=&quot;math/tex&quot;&gt;(y_{t+1}=x_t-\gamma\triangledown f(x_t))&lt;/script&gt;와 $(3)$을 사용했다). 이 식은 원래의 vanilla anaylsis에서 $-\frac{1}{2\gamma}|y_{t+1}-x_{t+1}|^2$만 추가된 것이다. 모든 $t$에 대해서 다 더하면,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}\sum_{t=0}^{T-1}(f(x_t)-f(x^* ))&amp;\le\frac{1}{2\gamma}(\|x_0-x^* \|^2-\|x_T-x^* \|^2)+\sum_{t=0}^{T-1}\frac{\gamma}{2}\|\triangledown f(x_t)\|^2-\sum_{t=0}^{T-1}\frac{1}{2\gamma}\|y_{t+1}-x_{t+1}\|^2 \\
&amp;\le \frac{1}{2\gamma}(\|x_0-x^* \|^2-\|x_T-x^* \|^2)+\sum_{t=0}^{T-1}\frac{\gamma}{2}\|\triangledown f(x_t)\|^2\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;를 얻게 되는데, 이는 결국 vanilla anaylsis와 같은 결론이다.&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\bar{x}=\frac{x_0+\cdots+x_{T-1}}{T}&lt;/script&gt; 라고 하면,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}f(\bar{x})-f(x^* )&amp;\le\frac{1}{T}\sum_{t=0}^{T-1}(f(x_t)-f(x^* ))\text{(Jensen's inequality)}\\
&amp;\le\frac{1}{2\gamma T}R^2+\frac{1}{T}\sum_{t=0}^{T-1}\frac{\gamma}{2}\|\triangledown f(x_t)\|^2\\
&amp;\le\frac{RB}{2\sqrt{T}}+\frac{RB}{2\sqrt{T}}(\gamma = \frac{R}{B\sqrt{T}})\\
&amp;\le\frac{RB}{\sqrt{T}}\end{align} %]]&gt;&lt;/script&gt;

&lt;h2 id=&quot;projected-gradient--beta-smooth-functions&quot;&gt;Projected Gradient : $\beta$-smooth functions&lt;/h2&gt;

&lt;h3 id=&quot;recall-beta-smooth&quot;&gt;(Recall) $\beta$-smooth&lt;/h3&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}f(x_t)-f(x_{t+1})&amp;\ge\triangledown f(x_t-x_{t+1})-\frac{\beta}{2}\|x_t-x_{t+1}\|^2\\
&amp;=\gamma\|\triangledown f(x_t)\|^2-\frac{\gamma^2\beta}{2}\|\triangledown f(x_t)\|^2\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;unconstrained 에서와는 다르게, constrained에서는&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\gamma\triangledown f(x_t)\neq x_t-x_{t+1}&lt;/script&gt;

&lt;p&gt;이다. 대신 $\gamma = \frac{1}{\beta}$를 대입하면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(x_{t+1})\le f(x_t)-\frac{1}{2\beta}\|\triangledown f(x_t)\|^2 + \frac{\beta}{2}\|y_{t+1}-x_{t+1}\|^2\cdots(* 4)&lt;/script&gt;

&lt;p&gt;를 얻을 수 있다. 주의할 점은, &lt;script type=&quot;math/tex&quot;&gt;\frac{\beta}{2}\|y_{t+1}-x_{t+1}\|^2&lt;/script&gt;때문에 monotone decrese (&lt;script type=&quot;math/tex&quot;&gt;f(x_{t+1}\le f(x_t))&lt;/script&gt;)를 확신할 수 없으므로 이를 먼저 증명해야 한다.&lt;/p&gt;

&lt;h3 id=&quot;proof-fx_t1le-fx_t&quot;&gt;(Proof) &lt;script type=&quot;math/tex&quot;&gt;f(x_{t+1})\le f(x_t)&lt;/script&gt;&lt;/h3&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}f(x_{t+1})&amp;\le f(x_t)+\triangledown f(x_t)^T(x_{t+1}-x_t)+\frac{\beta}{2}\|x_{t+1}-x_t\|^2 (\beta\text{-smooth})\\
&amp;=f(x_t)-\beta(y_{t+1}-x_t)^T(x_{t+1}-x_t)+\frac{\beta}{2}\|x_{t+1}-x_t\|^2(y_{t+1}-x_t=\frac{1}{\beta}\triangledown f(x_t))\\
&amp;=f(x_t)-\frac{\beta}{2}(\|y_{t+1}-x_t\|^2+\|x_{t+1}-x_t\|^2-\|y_{t+1}-x_{t+1}\|^2)+\frac{\beta}{2}\|x_{t+1}-x_t\|^2\\
&amp;=f(x_t)-\frac{\beta}{2}\|y_{t+1}-x_t\|^2+\frac{\beta}{2}\|y_{t+1}-x_{t+1}\|^2\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;$(* 3)$에 의해&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\|x_t-x_{t+1}\|^2+\|y_{t+1}-x_{t+1}\|^2\le\|x_t-y_{t+1}\|^2&lt;/script&gt;

&lt;p&gt;이므로&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;-\frac{\beta}{2}\|y_{t+1}-x_t\|^2+\frac{\beta}{2}\|y_{t+1}-x_{t+1}\|^2\le 0&lt;/script&gt;

&lt;p&gt;이 된다. 따라서&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}f(x_{t+1})-f(x_t)&amp;\le-\frac{\beta}{2}\|y_{t+1}-x_t\|^2+\frac{\beta}{2}\|y_{t+1}-x_{t+1}\|^2\\&amp;\le 0\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;이므로, $f(x_{t+1})\le f(x_t)$를 얻을 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;vanilla-analysis-1&quot;&gt;Vanilla Analysis&lt;/h3&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}\sum_{t=0}^{T-1}(f(x_t)-f(x^* ))&amp;\le\frac{\beta}{2}(\|x_0-x^* \|^2-\|x_T-x^* \|^2)+\sum_{t=0}^{T-1}\left(\frac{1}{2\beta}\|\triangledown f(x_t)\|^2-\frac{\beta}{2}\|y_{t+1}-x_{t+1}\|^2\right)\\
&amp;\le\frac{\beta}{2}\|x_0-x^* \|^2+\sum_{t=0}^{T-1}(f(x_t)-f(x_{t+1}))((* 4)\text{에 의해})\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;우변의 마지막 항을 좌변으로 넘기면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{t=1}^T(f(x_t)-f(x^* ))\le\frac{\beta}{2}\|x_0-x^* \|^2&lt;/script&gt;

&lt;p&gt;를 얻게 되고,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}f(x_T)-f(x^* )&amp;\le\frac{1}{T}\sum_{t=1}^T(f(x_t)-f(x^* ))(f(x_T)\le f(x_t),\forall t\lt T)\\
&amp;\le\frac{\beta}{2T}\|x_0-x^* \|^2\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;로 bound된다.&lt;/p&gt;
</description>
        <pubDate>Mon, 16 Sep 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/lecture/optimization/2019/09/16/OptLecture5.html</link>
        <guid isPermaLink="true">http://localhost:4000/lecture/optimization/2019/09/16/OptLecture5.html</guid>
        
        
        <category>Lecture</category>
        
        <category>Optimization</category>
        
      </item>
    
      <item>
        <title>Optimization Lecture 4</title>
        <description>&lt;h1 id=&quot;convex-optimization&quot;&gt;Convex Optimization&lt;/h1&gt;

&lt;h2 id=&quot;beta-smooth-functions--frac1t-learning-rate&quot;&gt;$\beta$-smooth functions : $\frac{1}{t}$ learning Rate&lt;/h2&gt;

&lt;p&gt;$f$가 $\beta$-smooth하다고 하면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}f(x_t)-f(x_{t+1}) &amp; \ge \triangledown f(x_t)^T(x_t-x_{t+1})-\frac{\beta}{2}\|x_t-x_{t+1}\|^2\\
 &amp; =\gamma\|\triangledown f(x_t)\|^2-\frac{\gamma^2\beta}{2}\|\triangledown f(x_t)\|^2 (x_{t+1}-x_t=-\gamma\triangledown f(x_t), \text{gradient descent})\\
 &amp; =(\gamma-\frac{\gamma^2\beta}{2})\|\triangledown f(x_t)\|^2\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;이다. 이 식에서의 극점은 $\gamma = \frac{1}{\beta}$일 때이므로 이를 대입하면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}f(x_t)-f(x_{t+1})\ge \frac{1}{2\beta}\|\triangledown f(x_t)\|^2\end{align}&lt;/script&gt;

&lt;p&gt;를 얻게 된다. 이 때 $t=0$부터 $t=T-1$까지 다 대입한 후 다 더하면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}f(x_0)-f(x_T) &amp; \ge \sum_{t=0}^{T-1}\frac{1}{2\beta}\|\triangledown f(x_t)\|^2\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;을 얻게 된다. 이전 단원의 Vanilla Analysis 중간에서 얻은 식에 $\gamma=\frac{1}{\beta}$를 대입하면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{t=0}^{T-1}{(f(x_t)-f(x^* ))}\le\frac{\beta}{2}\|x_0-x^* \|^2+\sum_{t=0}^{T-1}\frac{1}{2\beta}\|\triangledown f(x_t)\|^2&lt;/script&gt;

&lt;p&gt;을 얻는데, $(4)$을 이용하면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{t=0}^{T-1}{(f(x_t)-f(x^* ))}\le\frac{\beta}{2}\|x_0-x^* \|^2+\sum_{t=0}^{T-1}\frac{1}{2\beta}\|\triangledown f(x_t)\|^2 \le \frac{\beta}{2}\|x_0-x^* \|^2+ f(x_0)-f(x_T)&lt;/script&gt;

&lt;p&gt;이라고 할 수 있고, 맨 우측 두 항을 좌변으로 넘기면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{t=1}^T(f(x_t)-f(x^* ))\le\frac{\beta}{2}\|x_0-x^* \|^2&lt;/script&gt;

&lt;p&gt;를 얻는다. $f(x_T)\le f(x_t), 0\le \forall t\le T$이므로, 최종적으로&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(x_T)-f(x^* )\le\frac{1}{T}\sum_{t=1}^T(f(x_t)-f(x^* ))\le\frac{\beta}{2T}\|x_0-x^* \|^2&lt;/script&gt;

&lt;p&gt;를 얻는다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;upper bound와 convergence speed에 관한 설명 추가&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;def-linear-convergence&quot;&gt;(Def) Linear Convergence&lt;/h3&gt;

&lt;p&gt;만약&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\lim_{t\rightarrow\infty}{\frac{f(x_{t+1})-f(x^* )}{f(x_t)-f(x^* )}}=c&lt;/script&gt;

&lt;p&gt;를 만족하는 $c\in(0, 1)$가 존재한다면 $f$는 선형적으로 수렴한다고 말할 수 있다. 만약 $\frac{f(x_{t+1})-f(x^* )}{f(x_t)-f(x^* )}\le c, \forall t, \forall c\in (0, 1)$라면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}f(x_t)-f(x^* )\le c^t(f(x_0)-f(x^* ))\end{align}&lt;/script&gt;

&lt;p&gt;가 항상 성립한다.&lt;/p&gt;

&lt;h2 id=&quot;alpha-strongly-convex-and-beta-smooth-function&quot;&gt;$\alpha$-strongly convex and $\beta$-smooth function&lt;/h2&gt;

&lt;p&gt;Vanilla Analysis의 중간 과정에서&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\triangledown f(x_t)^T(x_t-x^* )=\frac{\gamma}{2}\|\triangledown f(x_t)\|^2+\frac{1}{\gamma}(\|x_t-x^* \|^2-\|x_{t+1}-x^* \|^2)&lt;/script&gt;

&lt;p&gt;의 식을 얻을 수 있었다. 그리고 $f$가 $\alpha$-strongly convex라고 하면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\triangledown f(x_t)^T(x_t-x^* )\ge f(x_t)-f(x^* )+\frac{\alpha}{2}\|x_t-x^* \|^2&lt;/script&gt;

&lt;p&gt;를 만족한다. 둘을 합치고 변형하면,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align} f(x_t)-f(x^* )+\frac{\alpha}{2}\|x_t-x^* \|^2&amp;\le\frac{\gamma}{2}\|\triangledown f(x_t)\|^2+\frac{1}{2\gamma}(\|x_t-x^* \|^2-\|x_{t+1}-x^* \|^2)\\
 -\frac{1}{2\gamma}(\|x_t-x^* \|^2-\|x_{t+1}-x^* \|^2)+\frac{\alpha}{2}\|x_t-x^* \|^2 &amp;\le f(x^* )-f(x_t )+ \frac{\gamma}{2}\|\triangledown f(x_t)\|^2\\
 -\|x_t-x^* \|^2+\|x_{t+1}-x^* \|^2+\alpha\gamma\|x_t-x^* \|^2 &amp;\le 2\gamma(f(x^* )-f(x_t ))+ \gamma^2\|\triangledown f(x_t)\|^2\\
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;을 얻을 수 있다. 이 때 $\gamma = \frac{1}{\beta}$를 대입하면 우변은&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}2\gamma(f(x^* )-f(x_t ))+ \gamma^2\|\triangledown f(x_t)\|^2&amp;=\frac{2}{\beta}(f(x^* )-f(x_t))+\frac{1}{\beta^2}\|f(x_t)\|^2\\
&amp;\le\frac{2}{\beta}(f(x_{t+1})-f(x_t))+\frac{1}{\beta^2}\|\triangledown f(x_t)\|^2\\
&amp;\le-\frac{1}{\beta^2}\|\triangledown f(x_t)\|^2+\frac{1}{\beta^2}\|\triangledown f(x_t)\|^2\\
&amp;=0\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;($(11)\rightarrow(12)$은 $(4)$ 때문이다.) $(8)$을 정리하면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\|x_{t+1}-x^* \|^2\le \left(1-\frac{\alpha}{\beta}\right)\|x_t-x^* \|^2&lt;/script&gt;

&lt;p&gt;이라고 할 수 있고, $(6)$에 의해&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\|x_T-x^* \|^2\le\left(1-\frac{\alpha}{\beta}\right)^T\|x_0-x^* \|^2&lt;/script&gt;

&lt;p&gt;가 성립한다.&lt;/p&gt;

&lt;h2 id=&quot;convergence&quot;&gt;Convergence&lt;/h2&gt;

&lt;p&gt;$f$가 $\beta$-smooth라면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}f(x_T)-f(x^* )&amp;\le\triangledown f(x^* )^T(x_T-x^* )+\frac{\beta}{2}\|x_T-x^* \|^2\\
&amp;=\frac{\beta}{2}\|x_T-x^* \|^2\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;이므로,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(x_T)-f(x^* )\le\frac{\beta}{2}\|x_T-x^* \|^2\le\frac{\beta}{2}\left(1-\frac{\alpha}{\beta}\right)^T\|x_0-x^* \|^2&lt;/script&gt;

&lt;p&gt;가 성립한다. 이 때 $\frac{\alpha}{\beta}$를 condition number라고 하는데, 이 값이 $1$이면, 즉 $\alpha = \beta$이면 한 번의 이터레이션으로 최적값을 찾을 수 있다.&lt;/p&gt;
</description>
        <pubDate>Wed, 11 Sep 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/lecture/optimization/2019/09/11/OptLecture4.html</link>
        <guid isPermaLink="true">http://localhost:4000/lecture/optimization/2019/09/11/OptLecture4.html</guid>
        
        
        <category>Lecture</category>
        
        <category>Optimization</category>
        
      </item>
    
  </channel>
</rss>
