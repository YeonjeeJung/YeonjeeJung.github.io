<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>YeonjeeJung's Blog</title>
    <description></description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sun, 17 Nov 2019 15:44:40 +0900</pubDate>
    <lastBuildDate>Sun, 17 Nov 2019 15:44:40 +0900</lastBuildDate>
    <generator>Jekyll v3.8.6</generator>
    
      <item>
        <title>Histograms of the Normalized Inverse Depth andLine Scanning for Urban Road Detection</title>
        <description>&lt;h2 id=&quot;abstract&quot;&gt;[Abstract]&lt;/h2&gt;

&lt;p&gt;이 논문에서는 3D LiDAR와 하나의 카메라의 기하 정보를 조합해 자동차 앞에 있는 도시의 도로 검출하는 방법을 제안한다. 이 방법의 장점은 3D LiDAR의 정확성과 도로의 연속성이다. 처음에는 LiDAR 데이터의 효과적인 표현과 2D 역 depth map을 얻는다. (3D LiDAR 포인트를 카메라 이미지 평면에 사영) 새로운 표현 방법을 이용하면 도로의 중간 표현을 얻을 수 있다. (정규화된 역 depth map의 vertical, horizontal histogram 추출) 근사된 도로 구역을 히스토그램 기반 방법으로 빠르게 찾을 수 있다. 근사된 도로 구역으로부터 더 정확하게 구역을 찾기 위해서 이 논문에서는 row and column scanning 방법을 제안한다.&lt;/p&gt;

&lt;h2 id=&quot;1-introduction&quot;&gt;[1] Introduction&lt;/h2&gt;

&lt;p&gt;이미지 기반의 검출 방법은 빛에 따라 결과가 달라진다. 특히 학습되지 않은 경우에는 LiDAR가 꼭 필요하다. 그러나 LiDAR만 사용하는 경우에는 point cloud가 매우 sparse하고 체계가 없다는 단점이 있다. 따라서 이 점들을 reorganize 하는 과정이 필요하다.&lt;/p&gt;

&lt;p&gt;원래는 3D LiDAR 점들을 이미지 평면에 사영시키는데, 그 사영은 sparse하므로 interpolation을 이용하여 dense하게 만든다. 그러나 이 논문에서는 Delaunay Triangulation 기반의 선형 평면 interpolation을 이용하여 dense한 역 depth map을 만든다. 그리고 이를 이용하여 horizontal histogram과 vertical histogram을 만드는데, horizontal histogram에서 도로 평면은 직선으로 사영된다. 반면 vertical histogram에서는 각 행마다 normalized inverse depth가 다르기 때문에 도로 픽셀의 누적 점수가 작다. 이 논문에서는 coarse to fine road detection 전략을 따랐는데, histogram은 이 coarse한 정보를 얻는 데에만 사용되었다. (기존의 다른 histogram을 사용한 방법에서는 이를 histogram에서 바로 예측하기 때문에 parameter setting에 노력이 많이 든다.)&lt;/p&gt;

&lt;p&gt;보행로와 도로의 구별이 쉽지 않은데, 이미지 평면의 기하적 특성을 사용하여 위 문제를 해결하기 위해 row and column scanning 전략을 사용하였다. 그리고 높이 차이 임계값을 설정하여 둘을 구분할 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;2-related-works&quot;&gt;[2] Related Works&lt;/h2&gt;

&lt;p&gt;한 &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.233.1475&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;서베이 논문&lt;/a&gt;에서 말하는 최근 트렌드는, 하나의 센서(only 카메라, 스테레오 카메라, LiDAR)를 이용하는 방법과 여러 센서를 이용하는 방법이다.&lt;/p&gt;

&lt;p&gt;하나의 카메라를 이용하는 방법은 보통 분류기를 학습하여 사용한다. 그러나 이런 방법들은 노이즈가 낄 경우 정확하지 않을 수 있다. 스테레오 카메라를 사용하는 방법은 disparity map을 사용한다. 그러나 이런 방법은 dense한 stereo matching을 사용하기 때문에 매우 시간이 오래 걸린다. 또한 마찬가지로 노이즈가 낄 경우 정확한 거리 예측이 어렵고, 연석을 구별하기도 어렵다. 반대로 3D LiDAR의 경우 더 정확한 거리 측정이 가능하다.&lt;/p&gt;

&lt;p&gt;하나의 센서를 이용하는 방법의 단점을 극복하기 위해 여러 센서를 동시에 사용하는 방법들도 있다. 이 논문에서도 카메라와 LiDAR를 이용하는데, 여기서 기하적 특성만 뽑아낸다. 여기서 색 정보와 질감 정보 등이 더 추가될 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;3-the-approximate-road-region-detection-by-histograms-of-the-normalized-inverse-depth&quot;&gt;[3] The Approximate Road Region Detection by Histograms of the Normalized Inverse Depth&lt;/h2&gt;

&lt;p&gt;이 방법은 세 가지의 단계로 이루어져 있는데, [A]와 [B], [4]가 그 단계이다.&lt;/p&gt;

&lt;h3 id=&quot;a-the-fusion-of-lidar-and-camera&quot;&gt;[A] The Fusion of LiDAR and Camera&lt;/h3&gt;

&lt;p&gt;LiDAR와 카메라 정보를 합칠 때 dense한 3D 정보를 얻기 위해서 &lt;a href=&quot;https://www.learnopencv.com/delaunay-triangulation-and-voronoi-diagram-using-opencv-c-python/&quot;&gt;Delaunay Triangulation&lt;/a&gt; 방법이 사용되었는데, normalized inverse depth map, height map($z$ axis), depth map($x$ axis), width map($y$ axis)가 만들어진다.&lt;/p&gt;

&lt;h3 id=&quot;b-the-maps-of-vertical-and-horizontal-histograms-for-rough-road-detection&quot;&gt;[B] The Maps of Vertical and Horizontal Histograms for Rough Road Detection&lt;/h3&gt;

&lt;p&gt;normalized inverse depth map은 stereo vision에서의 disparity map과 같도록 만들었다. horizontal과 vertical histogram은 normalized inverse depth map에서 각 열과 행에 값이 $t$인 값이 몇개가 있는지 세어 만든다.&lt;/p&gt;

&lt;p&gt;vertical histogram에서의 픽셀 수가 threshold보다 작으면 그 픽셀들은 도로에 해당한다고 예측한다. horizontal histogram에서는 도로 구역이 선형적인 구조로 표현되기 때문에, 선형 구조가 나타난다면 도로 평면으로 구분할 수 있다. 이 선은 가장 밑 행에서의 가장 많은 픽셀이 있는 점에서부터 시작해서 한 줄씩 올라가며 그보다 왼쪽에 있는 최대 개수의 픽셀이 있는 점으로 점점 올라가면 된다.&lt;/p&gt;

&lt;p&gt;이론상으로는 horizontal histogram에서 검출된 선을 가지고 바로 3D point cloud를 만들 수 있지만, 도로는 완벽한 평면이 아니기 때문에 margin을 지정해서 도로인 픽셀의 disparity 범위를 정한다. 만약 이 범위보다 $d(m, n)$이 작으면 더 멀리 있는 것이므로 negative obstacle이고, 이 범위보다 $d(m, n)$이 크면 가까이 있는 것이므로 positive obstacle이다. 이 범위에 $d(m, n)$이 있다면 도로로 판정한다.&lt;/p&gt;

&lt;p&gt;이렇게 범위를 정해서 판별하면 잘못 판별될 확률이 높으므로, 이 논문에서는 horizontal histogram 결과를 먼저 구하고, 그렇게 검출된 픽셀들에 한해서 vertical histogram 방법을 적용해서 최종 결과를 만든다.&lt;/p&gt;

&lt;h2 id=&quot;4-the-row-and-column-line-scanning-for-road-region-refinement&quot;&gt;[4] The Row and Column Line Scanning for Road Region Refinement&lt;/h2&gt;

&lt;p&gt;더 정확게 도로를 검출하기 위해 line scanning을 사용한다. 이 방법은 도로 구역으로 검출된 영역에서만 수행하는데, 도로 영역에서만 reference point를 선택하고, 장애물의 영향을 줄이기 위해서이다.&lt;/p&gt;

&lt;h3 id=&quot;a-the-height-difference&quot;&gt;[A] The Height Difference&lt;/h3&gt;

&lt;p&gt;reference point와의 높이 차이가 threshold보다 크면 도로가 아니고, 그보다 작으면 도로로 검출한다.&lt;/p&gt;

&lt;h3 id=&quot;b-the-reference-point-selection&quot;&gt;[B] The Reference Point Selection&lt;/h3&gt;

&lt;p&gt;reference point의 선택은 결과에 중요한 영향을 미친다. 이 reference point들은 도로가 확실한 영역에서 선택되어야 한다. 이 reference point selection을 위해 row and column scanning이라는 방법이 사용된다.&lt;/p&gt;

&lt;h3 id=&quot;c-the-row-scanning&quot;&gt;[C] The Row Scanning&lt;/h3&gt;

&lt;p&gt;row scanning은 가운데에서 시작해서 왼쪽과 오른쪽으로 진행되고, 도로가 아닌 점을 만날 때 종료된다. 각 row에서의 맨처음 reference point의 선택이 매우 중요한데, 따라서 도로임이 확실한 point여야 한다. 맨 처음 row의 첫 reference point는 그 row에서 가장 가운데에 있는 점으로 정하고, 그 이후 row에서는&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{Middle}_{i-1}=\frac{\text{Left}_i+\text{Right}_i+\text{Middle}_i}{3}&lt;/script&gt;

&lt;p&gt;으로 정한다. 만약 이렇게 예측된 점이 도로 영역에 포함되지 않는다면(앞에 차가 있는 경우 등) 같은 row의 가장 가까운 점을 첫 reference point로 정한다.&lt;/p&gt;

&lt;p&gt;그 후 좌우로 탐색하는데, 노이즈의 영향을 줄이기 위해 도로가 아닌 점이 연속적으로 나오면 탐색을 중지한다.&lt;/p&gt;

&lt;h3 id=&quot;d-the-column-scanning&quot;&gt;[D] The Column Scanning&lt;/h3&gt;

&lt;p&gt;column scanning의 경우에는 row scanning에서의 결과를 가지고 해당 column의 가장 밑 픽셀부터 scanning을 시작한다.&lt;/p&gt;

&lt;h3 id=&quot;e-the-row-and-column-scanning&quot;&gt;[E] The Row and Column Scanning&lt;/h3&gt;

&lt;p&gt;row scanning은 장애물이 있을 때 더 먼 곳까지 발견하지 못하고, column scanning은 첫 reference point가 정확하지 않으면 도로 검출에 실패할 수도 있다. 따라서 이 둘의 방법을 섞는데, row scanning 결과를 기반으로 하고, column scanning 결과로 보충한다. row 와 column scanning에서의 boundary를 계산한 후, threshold를 넘는 개수의 연속된 세로 점들을 찾아서 row scanning 결과를 보충한다. 이후 median blur를 사용한 후 가장 큰 connection을 찾아 최종 결과로 사용한다.&lt;/p&gt;
</description>
        <pubDate>Wed, 13 Nov 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/thesis/2019/11/13/HID.html</link>
        <guid isPermaLink="true">http://localhost:4000/thesis/2019/11/13/HID.html</guid>
        
        <category>RobotNavigation</category>
        
        
        <category>Thesis</category>
        
      </item>
    
      <item>
        <title>Scaling SGD Batch Size to 32K for ImageNet Training</title>
        <description>&lt;h2 id=&quot;abastract&quot;&gt;[Abastract]&lt;/h2&gt;

&lt;p&gt;큰 네트워크의 학습 속도를 높이는 자연스러운 방법은 여러개의 GPU를 사용하는 것이다. 확률 기반 방법을 더 많은 프로세서로 확장하려면 각 GPU의 컴퓨팅 파워를 최대로 사용하기 위해 batch size를 늘려야 한다. 그러나 batch size를 계속해서 올리면서 네트워크의 정확도를 유지하는 것은 불가능하다. 현재 sota방법은 batch size에 반비례하게 LR을 늘리고, 초기 최적화의 어려움을 극복하기 위해 LR에 ‘warm-up’이라는 방법을 사용한다.&lt;/p&gt;

&lt;p&gt;LR을 학습 중에 조절하므로써 ImagNet 학습에 큰 batch size를 효과적으로 사용할 수 있다. 그러나 ImageNet-1k 학습을 위해서는 현재 AlexNet이나 ResNet은 batch size를 크게 늘릴 수 없다. 이유는 큰 LR을 사용할 수 없기 때문이다. 큰 batch size를 일반적인 네트워크나 데이터에 대해 사용 가능하게 하기 위해, 이 논문에서는 Layer-wise Adaptive Rate Scaling (LARS)를 제안한다. LARS에서는 weight의 norm과 gradient의 norm을 기반으로 층마다 다른 LR을 사용한다. LARS를 사용하면 ResNet50과 AlexNet에 대해서 큰 batch size를 사용할 수 있다. 큰 batch size는 시스템의 컴퓨팅 파워를 최대로 사용할 수 있다. 이것은 속도의 향상으로 이어진다.&lt;/p&gt;

&lt;h2 id=&quot;1-introduction&quot;&gt;[1] Introduction&lt;/h2&gt;

&lt;p&gt;깊은 네트워크의 확장과 속도 향상은 딥러닝의 응용에서 매우 중요하다. 속도 향상을 위해서는 더 많은 프로세서로 확장시켜야 한다. 그러기 위해서는 batch size를 더 크게 해야 한다.  그러나 이전 연구에 따르면 batch size를 늘이는 것은 test accuracy에서 좋지 않은 결과가 나온다. 학습 중에 LR을 조절하면 큰 batch size에서도 좋은 결과를 유지할 수 있다. 하지만 현재까지의 연구들은 1024 이상의 크기의 batch size에서는 사용할 수 없다. 이 논문의 저자들은 batch normallization을 사용해 큰 batch size에서도 좋은 성능을 얻었지만 여전히 정확도를 잃었다. &lt;em&gt;어디에서 잃었다는건지 모르겠음… 이전에 작은 batch size로 했을 때랑 결과가 똑같은데..?&lt;/em&gt; 이를 더 개선하기 위해 LARS를 제안하는데, weight의 norm &lt;script type=&quot;math/tex&quot;&gt;\|w\|&lt;/script&gt;과 gradient의 norm &lt;script type=&quot;math/tex&quot;&gt;\triangledown w\|&lt;/script&gt;에 따라 층마다 다른 LR을 사용한다. 만약 같은 LR을 쓴다면 &lt;script type=&quot;math/tex&quot;&gt;\frac{\|w\|}{\|\triangledown w\|}&lt;/script&gt;이 큰 층은 수렴해도 작은 층은 발산할 수도 있다. LARS를 사용하면 더 큰 batch size를 사용해도 같은 정확도를 얻을 수 있다. 또한, GPU의 컴퓨팅 파워를 최대로 사용할 수 있기 때문에 속도 향상 또한 가능하다.&lt;/p&gt;

&lt;h2 id=&quot;2-background-and-related-work&quot;&gt;[2] Background and Related Work&lt;/h2&gt;

&lt;h3 id=&quot;21-data-parallelism-mini-batch-sgd&quot;&gt;[2.1] Data-Parallelism Mini-Batch SGD&lt;/h3&gt;

&lt;p&gt;보통의 mini-batch SGD는 update 식을&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;w_{t+1}=w_t-\frac{\eta}{b}\sum_{b\in B_t}\triangledown l(x, y, w)&lt;/script&gt;

&lt;p&gt;라고 쓰는데, 복잡하므로 이 논문에서는&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;w_{t+1}=w_t-\eta\triangledown w_t&lt;/script&gt;

&lt;p&gt;라고 쓴다.&lt;/p&gt;

&lt;h3 id=&quot;22-large-batch-training-difficulty&quot;&gt;[2.2] Large-Batch Training Difficulty&lt;/h3&gt;

&lt;p&gt;GPU를 사용하면 여러 개의 프로세서를 동시에 돌릴 수 있지만, 이를 최대로 사용하기 위해서는 batch size가 커야 하는데, 특정 크기보다 크게 되면 테스트 정확도가 학습 정확도보다 현저하게 낮다. 이전 연구에서는 큰 batch size에 대해 학습 loss가 작아도 test loss는 그것보다 훨씬 크고, 반면 작은 batch size에 대해서는 train loss와 test loss가 비슷하다고 결론지었다. 다른 연구에서는 더 오래 학습하는 것이 일반화에 더 도움이 된다고 했는데, 반면 또다른 연구에서는 LR을 잘 조절하는 것이 정확도를 유지하는데 더 도움이 된다고 했다.&lt;/p&gt;

&lt;h3 id=&quot;23-learning-rate-lr&quot;&gt;[2.3] Learning Rate (LR)&lt;/h3&gt;

&lt;p&gt;batch size를 크게 하면 기본 LR도 그에 따라 올려야 하는데, 그러면서 정확도는 내려가지 않아야 한다. 기존 연구에서는 sqrt scaling rule과 linear scaling rule이 있었다. 또한 warmup scheme도 많이 사용되고 있는데, 처음에 작은 LR을 설정해 놓고 몇 epoch가 지나면 점점 크게 하는 방법이다. 이것이 gradual warmup scheme이고, constant warmup scheme은 초기 몇 번의 epoch에서 constant한 LR을 사용하는 방법이다. &lt;em&gt;그뒤에 어떻게 한다는 건지 안나와있네…왜 이게 warmup이지&lt;/em&gt; constant warmup은 object detection이나 segmentation에 효과적이고, gradual warmup은 ResNet-50을 학습시키는 데에 좋다. batch size에 관계없이 LR에 상한이 있다는 또다른 연구도 있는데, 이 논문의 실험 결과는 위 논문의 결과와 같다.&lt;/p&gt;

&lt;h2 id=&quot;3-imagenet-1k-training&quot;&gt;[3] ImageNet-1k Training&lt;/h2&gt;

&lt;h3 id=&quot;31-reproduce-and-extend-facebooks-result&quot;&gt;[3.1] Reproduce and Extend Facebook’s result&lt;/h3&gt;

&lt;p&gt;이 논문에서의 첫 단계는 Facebook에서 한 연구의 결과를 따라하는 것이었는데, 그 연구에서는 multistep LR과 warmup, linear scaling LR을 사용했다. 이 연구에서도 그 연구와 비슷하게 warmup과 linear scaling을 사용했는데, Facebook의 연구와 다른 점은&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;LR을 더 높였다.&lt;/li&gt;
  &lt;li&gt;multistep rule 대신 poly rule &lt;em&gt;설명이 없네&lt;/em&gt; 을 사용하였다.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;32-train-imagenet-by-alexnet&quot;&gt;[3.2] Train ImageNet by AlexNet&lt;/h3&gt;

&lt;h4 id=&quot;321-linear-scaling-and-warmup-schemes-for-lr&quot;&gt;[3.2.1] Linear Scaling and Warmup schemes for LR&lt;/h4&gt;

&lt;p&gt;이 논문에서의 baseline은 Batch-512 AlexNet이고, 100 epoch동안 0.58의 정확도를 얻는 것이다. poly rule을 사용했고, base LR은 0.01이며 poly power는 2이다. 목표는 Batch-4096, Batch-8192이고 정확도 0.58을 100 epoch 안에 얻는 것이다.&lt;/p&gt;

&lt;p&gt;첫번째로 Batch-4096 AlexNet을 사용했고 linear scaling을 사용했으며 baseLR을 0.08로 사용했으나, Batch-4096은 0.01의 LR에도 수렴하지 않았다. 이후에는 같은 모델에 linear scaling과 warmup을 사용하였다. 하이퍼파라미터 튜닝을 거쳐 나온 가장 좋은 결과는 0.531에 그쳤다. 또한 Batch-8192의 정확도는 그것보다도 낮았다. 따라서 linear scaling과 warmup은 큰 batch size의 AlexNet을 학습시키는데 충분하지 않다는 것을 알았다.&lt;/p&gt;

&lt;h4 id=&quot;322-batch-normalization-for-large-batch-training&quot;&gt;[3.2.2] Batch Normalization for Large-Batch Training&lt;/h4&gt;

&lt;p&gt;여러 방법을 사용해본 결과, Batch Normalization만 성능을 개선시킨다는 것을 관찰했다. BN을 적용하고 0.01부터 결과가 발산할 때까지 LR을 늘렸는데, Batch-4096은 0.3에서 멈추었고, Batch-8192는 0.41에서 멈추었다. 그러나 이렇게 해도 Batch-512의 정확도에 도달하기 위해서는 정확도가 부족했다. momentum과 weight decay 파라미터를 조정해 보았으나 개선은 없었다.&lt;/p&gt;

&lt;p&gt;이후에는 성능이 개선되지 않는 문제가 일반화 문제 때문이 아님을 관찰하였다. 이유는 최적화 어려움 때문이었다.&lt;/p&gt;

&lt;h4 id=&quot;323-layer-wise-adaptive-rate-scaling-lars-for-large-batch-training&quot;&gt;[3.2.3] Layer-wise Adaptive Rate Scaling (LARS) for Large-Batch Training&lt;/h4&gt;

&lt;p&gt;따라서 새로운 LR updating rule을 디자인했다. 실험 결과에서 층마다 다른 LR이 필요함을 느꼈는데, 그 이유는 &lt;script type=&quot;math/tex&quot;&gt;\|w\|_ 2&lt;/script&gt;와 &lt;script type=&quot;math/tex&quot;&gt;\|\triangledown w\|_ 2&lt;/script&gt;의 비율이 층마다 매우 다르기 때문이었다. 이전에 이런 문제를 해결하기 위한 ResNet50을 위한 해결법 연구가 있었는데, AlexNet에는 효과가 없었다.&lt;/p&gt;

&lt;p&gt;이 논문에서 제안한 LARS의 업데이트 룰은 다음과 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\eta=l\times\gamma\times\frac{\|w\|_ 2}{\|\triangledown w\|_ 2}&lt;/script&gt;

&lt;p&gt;여기서 $l$은 scaling factor ($0.001$)이고, $\gamma$는 input LR ($1$~$50$)이다. $\mu$를 momentum이라고 하고 $\beta$를 weight decay 라고 하고 LARS 알고리즘을 살펴보면&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;각 층마다 local LR을 구한다. &lt;script type=&quot;math/tex&quot;&gt;\alpha = l\times\frac{\|w\|_ 2}{\|\triangledown w\|_ 2+\beta\|\triangledown w\|_ 2}&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;각 층마다 true LR을 구한다. &lt;script type=&quot;math/tex&quot;&gt;\eta = \gamma\times\alpha&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;gradient를 업데이트 한다. &lt;script type=&quot;math/tex&quot;&gt;\triangledown w=\triangledown w+\beta w&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;가속 항 $a$를 &lt;script type=&quot;math/tex&quot;&gt;a=\mu a+\eta\triangledown w&lt;/script&gt; 를 이용해 업데이트 한다.&lt;/li&gt;
  &lt;li&gt;weight를 업데이트 한다. $w=w-a$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;이다. LARS를 적용해서 baseline과 같은 결과를 얻을 수 있었다. 그 뒤에는 BN을 빼고 기본 AlexNet 모델을 사용했는데, Batch-4096에는 13 epoch, Batch-8192에는 8 epoch의 warmup range를 사용하였다. 큰 batch size를 사용해서 작은 batch size에서의 결과와 비슷한 결과를 얻으려면 BN만으로는 부족하고 LARS까지 사용해야 한다는 것을 알았다. 결과에서 보면 LARS만 사용한 결과도 둘 다를 사용한 결과보다 조금 더 낮다.&lt;/p&gt;

&lt;h2 id=&quot;4-experimental-results&quot;&gt;[4] Experimental Results&lt;/h2&gt;

&lt;h3 id=&quot;42-implementation-details&quot;&gt;[4.2] Implementation Details&lt;/h3&gt;

&lt;p&gt;8192정도의 Batch size를 한번에 저장할 수 있는 GPU가 없었기 때문에 하나의 Batch를 32개로 나누어 순차적으로 gradient를 계산했다. GPU의 메모리가 Batch 여러 개를 넣기에 충분한 경우에는 multi solver를 넣어 속도를 향상시켰다.&lt;/p&gt;

&lt;h3 id=&quot;43-state-of-the-art-results-for-resnet50-training&quot;&gt;[4.3] State-of-the-art Results for ResNet50 training&lt;/h3&gt;

&lt;p&gt;이 논문의 결과는 data augmentation을 하지 않았기 때문에 sota 결과보다는 정확도가 낮다. data augmentation을 추가하면 sota 결과가 나온다. 이 논문에서는 32768의 batch size를 사용했다.&lt;/p&gt;

&lt;h3 id=&quot;44-benefits-of-using-large-batch&quot;&gt;[4.4] Benefits of Using Large Batch&lt;/h3&gt;

&lt;p&gt;큰 batch size를 사용할 수 있게 되고 baseline과 같은 결과를 얻게 되자, 이 논문에서는 속도 비교에 중점을 두었다. AlexNet-BN의 batch size가 512에서 4096으로 커지자, 4개의 GPU에서는 속도가 비슷했지만, 8개의 GPU에서는 속도가 3배 빨라졌다.&lt;/p&gt;

&lt;h2 id=&quot;5-conclusion&quot;&gt;[5] Conclusion&lt;/h2&gt;

&lt;p&gt;최적화 어려움이 큰 batch size에 대해 학습을 어렵게 만들었다. linear scaling이나 warmup같은 방법들은 AlexNet등의 복잡한 모델에는 충분하지 않다. BN같은 모델 구조를 변형하는 방법들도 사용할 수 있겠지만 충분하지 않으므로, 이 논문에서 제안하는 LARS를 사용하면 충분히 baseline 정확도를 얻을 수 있다. LARS는 층마다 weight와 weight의 gradient의 norm에 따라 다른 LR을 사용하는데, 매우 높은 효율성을 보여준다. ImageNet을 학습시키는 AlexNet에서 Batch size를 128에서 8192로 키워도 정확도 손실이 없다. 또한 ResNet50에서는 batch size를 32768까지 키울 수 있었다.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;수학적인 증명은 별로 없는 논문이었다. 실험적인 결과로만 가설을 세우고 쓴 논문..&lt;/p&gt;

&lt;p&gt;설명이 더 필요한 개념 : LR scheduler에서의 poly rule, constant warmup&lt;/p&gt;
</description>
        <pubDate>Sun, 03 Nov 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/thesis/2019/11/03/SCALEBATCHSIZE.html</link>
        <guid isPermaLink="true">http://localhost:4000/thesis/2019/11/03/SCALEBATCHSIZE.html</guid>
        
        <category>Optimization</category>
        
        
        <category>Thesis</category>
        
      </item>
    
      <item>
        <title>SGD - General Analysis and Improved Rates</title>
        <description>&lt;h2 id=&quot;abstract&quot;&gt;[Abstract]&lt;/h2&gt;

&lt;p&gt;이 논문에서는 임의 샘플링에 대한 SGD의 수렴을 설명하는 이론을 제안한다. 이 이론은 SGD의 여러 종류의 수렴을 설명하는데, 각각은 mini-batch를 형성하는 데에 특정한 확률 분포에 연관되어 있다. 이런 분석을 한 것은 처음이며, 분석에 사용된 SGD는 대부분 이전에 논문에서는 명시적으로 고려되지 않았던 것들이다. 이 분석은 최근에 소개된 expcted smoothness에 의존하며, stochastic gradient들의 bound에는 의존하지 않는다. replacement sampling과 independent sampling 등과 같은 여러 다른 minibatch 전략에 대해 연구하므로써, 우리는 stepsize를 mini-batch의 크기에 대한 하나의 함수로써 표현해냈다. 이렇게 하면 전체 복잡도를 최소화하는 mini-batch의 크기를 정할 수 있으며, 최소로 evaluate된 stochastic gradient의 분산이 커지면 최적의 mini-batch 크기도 증가한다는 것을 명시적으로 보여주었다. 분산이 0이면 최적의 mini-batch 크기는 $1$이다. 더해서, 상수였던 stepsize를 언제 감소시켜야 하는지를 결정하는 stepsize-switching 규칙을 증명한다.&lt;/p&gt;

&lt;h2 id=&quot;1-introduction&quot;&gt;[1] Introduction&lt;/h2&gt;

&lt;p&gt;우선 이런 최적화 문제를 풀려고 한다. (이것은 감독학습 문제의 일반적인 형태이다)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x^* = \arg\min_{x\in \mathbb{R}^d}\left[f(x) =\frac{1}{n}\sum_{i=1}^n f_i(x)\right]&lt;/script&gt;

&lt;p&gt;모든 $f_i : \mathbb{R}^d\rightarrow\mathbb{R}$은 smooth이다. 그리고 $f$는 global minimizer $x^* $를 가지며, $\mu$-strongly quasi-convex이다. 따라서&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
f(x^* )\ge f(x) +\left&lt;\triangledown f(x),x^* −x\right&gt;+\frac{\mu}{2}\|x^* −x\|^2 %]]&gt;&lt;/script&gt;

&lt;p&gt;를 만족한다.&lt;/p&gt;

&lt;h3 id=&quot;11-background-and-contributions&quot;&gt;[1.1] Background and contributions&lt;/h3&gt;

&lt;h4 id=&quot;linear-convergence-of-sgd&quot;&gt;Linear convergence of SGD&lt;/h4&gt;

&lt;p&gt;2011년에는 strongly convex한 $f$를 특정 noise level까지 수렴하게 하는 비점근 분석의 연구가 있었다. 2016년에는 condition number에 대한 2차 의존성을 없애서 위 결과를 개선하고, importance sampling을 고려했다. 위 연구의 결과는 이후에 mini-batch 다양성에 대한 연구로 확장되었다. 이 논문에서는 위 결과를 대부분의 샘플링에 대해 적용할 수 있도록 했다. 분산 감소 방법의 관점에서 expected smoothness assumption을 도입했는데, 이 assumption은 SGD에서 사용하는 $f$와 $\mathcal{D}$의 연관 특징으로, 이 논문에서 일반적인 복잡도 결과(Thm 3.1)를 증명할 수 있게 해주었다. strong convexity 없이(사실은 quasi-convexity를 가정하였다) 선형 수렴도를 얻을 수 있었다. 또한, 함수 $f_i$들은 convex가 아니어도 된다.&lt;/p&gt;

&lt;h4 id=&quot;gradient-noise-assumptions&quot;&gt;Gradient noise assumptions&lt;/h4&gt;

&lt;p&gt;이전의 많은 SGD의 선형 수렴에 관한 연구들은 확률론적 gradient의 분산이 bound됨을 가정하고 증명했다. 이후에는 점점 가정을 약하게 하는 방향으로 가고 있다. 이 논문에서는 직접적으로 gradient의 bound는 가정하지 않고, 더 약한 가정인 expected smoothness assumption을 사용한다.&lt;/p&gt;

&lt;h4 id=&quot;optimal-mini-batch-size&quot;&gt;Optimal mini-batch size&lt;/h4&gt;

&lt;p&gt;이전의 연구 결과로 큰 mini-batch 크기를 사용하는 것이 넓은 범위의 non-convex문제를 푸는 것에 효과적이라는 것이 밝혀졌는데, 저자는 최적의 stepsize가 mini-batch 크기에 따라 선형적으로 늘어난다고 추측했다. 이 논문에서는 이 추측이 최적의 mini-batch 크기까지 커질 때에만 적용되며, mini-batch 크기에 따른 최적의 stepsize의 정확한 공식이 있음을 증명하고 그 공식을 제공한다.&lt;/p&gt;

&lt;h4 id=&quot;learning-schedules&quot;&gt;Learning schedules&lt;/h4&gt;

&lt;p&gt;이전 연구에서는 해 주위에서 SGD의 수렴을 감지하는 방법이 제안되었다. 이 논문에서는 stepsize를 언제 줄여야 하는지 알려주는 식을 제공한다. 또한, 샘플링 방법에 따라, 그리고 mini-batch 크기에 따라 stepsize와 반복 복잡도가 어떻게 증가하고 감소하는지 보여준다.&lt;/p&gt;

&lt;h4 id=&quot;over-parameterized-models&quot;&gt;Over-parameterized models&lt;/h4&gt;

&lt;p&gt;이전 연구에서 데이터보다 파라미터 수가 많을 때의 SGD의 수렴 속도를 분석한 연구가 있었다. 이 논문에서는 over-parametrized model의 경우 최적의 mini-batch 크기가 $1$임을 보여준다.&lt;/p&gt;

&lt;h3 id=&quot;12-stochastic-reformulation&quot;&gt;[1.2] Stochastic reformulation&lt;/h3&gt;

&lt;p&gt;sampling vector를 이용해서 각 함수와 gradient의 weight를 다르게 만든다. 평균을 취하면 결국 원래의 함수 또는 gradient와 같게 된다. 이 방법은 이전에도 분산을 줄이는 목적으로 사용되었다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x^{k+1}=x^k-\gamma^k\triangledown f_{c^k}(x^k)&lt;/script&gt;

&lt;p&gt;를 이용해서 gradient descent를 할 수 있다. 이 논문에서는 여러 다른 분포 $\mathcal{D}$에 대한 위 gradient descent의 수렴을 분석할 것이다. 또한 수렴을 얻기 위해 SGD를 변형한다.&lt;/p&gt;

&lt;h2 id=&quot;2-expected-smoothness-and-gradient-noise&quot;&gt;[2] Expected Smoothness and Gradient Noise&lt;/h2&gt;

&lt;h3 id=&quot;21-expected-smoothness&quot;&gt;[2.1] Expected smoothness&lt;/h3&gt;

&lt;p&gt;expected smoothness는 $\mathcal{D}$ 분포의 특성과 $f$의 smoothness 특성을 결합한 것이다.&lt;/p&gt;

&lt;h4 id=&quot;assumption-21-expected-smoothness&quot;&gt;Assumption 2.1 (Expected Smoothness)&lt;/h4&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbb{E}_\mathcal{D}[\|\triangledown f_v(x)-\triangledown f_v(x^* )\|^2]\le 2\mathcal{L}(f(x)-f(x^* )), \forall x\in\mathbb{R}^d&lt;/script&gt;

&lt;p&gt;인 $\mathcal{L}=\mathcal{L}(f, \mathcal{D})\gt0$이 존재할 때, $f$가 분포 $\mathcal{D}$에 대해 $\mathcal{L}$-smooth in expectation이라고 한다. 간단히, $(f, \mathcal{D})\sim ES(\mathcal{L})$이라고 표현한다. 이것이 expected smoothness의 정의이다. 이 expected smoothness는 $f_i$나 $f$가 convex가 아니어도 성립할 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;22-gradient-noise&quot;&gt;[2.2] Gradient noise&lt;/h3&gt;

&lt;p&gt;이 논문에서 쓰이는 중요 가정 중 두번째는 gradient noise의 유한성이다. 이 가정은 매우 약한 가정이고, $f$보다는 $\mathcal{D}$에 관한 가정이다.&lt;/p&gt;

&lt;h3 id=&quot;23-key-lemma-and-connection-to-the-weak-growth-condition&quot;&gt;[2.3] Key lemma and connection to the weak growth condition&lt;/h3&gt;

&lt;p&gt;보통 SGD의 수렴을 증명할 때는&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbb{E}\|\triangledown f_v(x)\|^2\le c&lt;/script&gt;

&lt;p&gt;라는 가정이 들어가는데, $f$가 strongly convex일 때는 보통 성립하지 않는다. 이 논문에서는 이 가정 대신에 확률론적 gradient의 기댓값을 bound하기 위해 expected smoothness를 사용한다.&lt;/p&gt;

&lt;h4 id=&quot;lem-24&quot;&gt;[Lem 2.4]&lt;/h4&gt;

&lt;p&gt;만약 $(f, \mathcal{D})\sim ES(\mathcal{L})$이면,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbb{E}_\mathcal{D}[\|\triangledown f_v(x)\|^2]\le 4\mathcal{L}(f(x)-f(x^* ))+2\sigma^2&lt;/script&gt;

&lt;p&gt;이다.&lt;/p&gt;

&lt;p&gt;$\sigma=0$인 zero-noise setting에서는 이 식이 weak growth condition이라고 알려져 있다. 이전에도 비슷한 가정이 있었는데, 이 논문에서 제시된 bound가 더 강력하다.&lt;/p&gt;

&lt;h2 id=&quot;3-convergence-analysis&quot;&gt;[3] Convergence Analysis&lt;/h2&gt;

&lt;h3 id=&quot;31-main-rules&quot;&gt;[3.1] Main rules&lt;/h3&gt;

&lt;h4 id=&quot;thm-31&quot;&gt;[Thm 3.1]&lt;/h4&gt;

&lt;p&gt;$f$가 $\mu$-quasi-strongly convex이고 $(f, \mathcal{D})\sim ES(\mathcal{L})$이라고 하자. 모든 $k$에 대하여 $\gamma^k=\gamma\in(0, \frac{1}{2\mathcal{L}}]$을 고른다. 그러면 SGD는 다음을 만족한다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbb{E}\|x^k-x^* \|^2\le(1-\gamma\mu)^k\|x^0-x^* \|^2+\frac{2\gamma\sigma^2}{\mu}&lt;/script&gt;

&lt;p&gt;따라서 모든 $\epsilon \gt 0$에 대해 $\gamma=\min{\frac{1}{2\mathcal{L}}, \frac{\epsilon\mu}{4\sigma^2}}$와 &lt;script type=&quot;math/tex&quot;&gt;k\ge\max\{\frac{2\mathcal{L}}{\mu}, \frac{4\sigma^2}{\epsilon\mu^2}\}\log\left(\frac{2\|x^0-x^* \|^2}{\epsilon}\right)&lt;/script&gt;을 고르면,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbb{E}\|x^k-x^* \|^2\le\epsilon&lt;/script&gt;

&lt;p&gt;을 만족한다.&lt;/p&gt;

&lt;p&gt;이 Thm이 말하는 바는 SGD가 gradient noise $\sigma^2$과 stepsize $\gamma$에 의존하는 $\frac{2\gamma\sigma^2}{\mu}$까지 선형적으로 수렴한다는 것이다. 더 작은 stepsize를 사용하면 더 수렴할 수 있지만, 수렴도가 떨어진다. $\mathcal{D}$를 조절할 수 있으므로, $\sigma^2$와 $\mathcal{L}$도 조절이 가능하다. stepsize 를 계속 감소시키면 위 항을 더 작게 만들 수도 있다.&lt;/p&gt;

&lt;h4 id=&quot;thm-32&quot;&gt;[Thm 3.2]&lt;/h4&gt;

&lt;p&gt;$f$가 $\mu$-quasi-strongly convex이고 $(f, \mathcal{D})\sim ES(\mathcal{L})$이라고 하자. $\mathcal{K}=\frac{\mathcal{L}}{\mu}$라고 하고,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\gamma^k=\begin{cases}\frac{1}{2\mathcal{L}} &amp; \text{for }k\le 4\lceil\mathcal{K}\rceil\\ \frac{2k+1}{(k+1)^2\mu} &amp; \text{for }k\gt 4\lceil\mathcal{\mathcal{K}}\rceil\end{cases} %]]&gt;&lt;/script&gt;

&lt;p&gt;라고 하자. 만약 $k\ge 4\lceil\mathcal{K}\rceil$이면 다음을 만족하며 수렴한다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbb{E}\|x^k-x^* \|^2\le\frac{\sigma^2}{\mu^2}\frac{8}{k}+\frac{16\lceil{\mathcal{K}\rceil^2}}{e^2k^2}\|x^0-x^* \|^2&lt;/script&gt;

&lt;h3 id=&quot;32-choosing-mathcald&quot;&gt;[3.2] Choosing $\mathcal{D}$&lt;/h3&gt;

&lt;p&gt;위 gradient descent가 효과적이려면, sampling vector $v$가 sparse 해야 한다.&lt;em&gt;왜?&lt;/em&gt; 이 논문에서는 proper sampling 만을 고려한다. sampling $S$가 proper 하다는 말은 모든 $i$에 대해, $i$가 $S$안에 존재할 확률 $p_i$, 즉 $i \in C$인 모든 $C$의 확률을 다 더한 것이 양수라는 말이다. ($\mathbb{P}[i\in S] = \sum_{C:i\in C}p_C \ge 0, \forall i$) 이 proper sampling은 sampling $S$에 의존하는 sampling vector를 특수화 한, 이 논문에서는 특수한 케이스이다. proper sampling의 정의를 기반으로 Independent sampling, Partition sampling, Single element sampling, $\tau$-nice sampling을 정의할 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;33-bounding-mathcall-and-sigma2&quot;&gt;[3.3] Bounding $\mathcal{L}$ and $\sigma^2$&lt;/h3&gt;

&lt;p&gt;$f$가 convex이고 smooth하다고 가정하면 expected smoothness $\mathcal{L}$과 $\sigma^2$에 대한 closed form expression을 계산할 수 있다. 그리고 sampling을 이용해 $\mathcal{L}$과 $\sigma^2$에 대한 bound를 할 수 있는데, sampling의 종류에 따라 이 bound가 달라진다.&lt;/p&gt;

&lt;h2 id=&quot;4-optimal-mini-batch-size&quot;&gt;[4] Optimal Mini-Batch Size&lt;/h2&gt;
&lt;p&gt;위에서 나온 결과에 $|S|=n$인 경우를 대입해 보면 우리가 원래 알고 있던 full-batch gradient descent결과와 같다. 따라서 full-batch gradient descent는 SGD의 특별한 케이스라고 볼 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;nontation&quot;&gt;[Nontation]&lt;/h3&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;L_i=\lambda_{\max}(M_i)&lt;/script&gt;&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;L_{\max}=\max_{i\in[n]}L_i&lt;/script&gt;&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;\bar{h} = \frac{1}{n}\sum_{i\in[n]}\|h_i\|^2&lt;/script&gt;&lt;/p&gt;

&lt;h3 id=&quot;41-nonzero-gradient-noise&quot;&gt;[4.1] Nonzero gradient noise&lt;/h3&gt;
&lt;p&gt;mini-batch 크기에 따라 복잡도가 어떻게 변하는지 알기 위해 $|S|=\tau$인 independent sampling과 $\tau$-nice sampling의 경우를 생각해보자.&lt;/p&gt;

&lt;p&gt;Independent sampling의 경우에는 bound된 결과를 Thm 3.1에 넣으면 최소 몇 번의 반복을 해야 하는지 알 수 있는데, 이것은 현재 나온 것 중 최적이다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;k\ge\frac{2}{\mu}\max\{L+\max_{i\in[n]}\frac{1-p_i}{np_i}L_i, \frac{2}{\mu\epsilon}\frac{1-p_i}{np_i}\bar{h}\}&lt;/script&gt;

&lt;p&gt;또한 최적의 step size $\gamma$도 알 수 있게 되고, $\tau$가 커질수록 stepsize는 줄어든다는 것을 알 수 있다. 따라서 전체 반복 수를 최소화하는 mini-batch 크기($\tau$)를 선택할 수 있고, 계산으로 이를 구할 수 있다. $\tau$-nice sampling에서도 independent sampling과 같은 방법으로 최적의 mini-batch 크기를 구할 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;42-zero-gradient-noise&quot;&gt;[4.2] Zero gradient noise&lt;/h3&gt;

&lt;p&gt;$\sigma=0$인 경우를 생각해 보자. Thm 3.1에 따르면 SGD의 반복 복잡도는 매우 간단해진다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;k\ge\frac{2L}{\mu}&lt;/script&gt;

&lt;p&gt;이 경우에 $f$는 weak growth condition을 만족하게 되고, 이전의 연구 결과와 직접적으로 비교할 수 있게 된다. $\tau$가 $n$까지 커질수록 복잡도가 개선되지만, 전체 복잡도는 $\tau$가 곱해지기 때문에 이것으로는 충분하지 않아서 $\tau=1$이 최적의 mini-batch 크기가 된다.&lt;em&gt;이부분이 잘 이해가 안됨..저걸로는 충분하지 않아서 batch-size를 1로 한다고..?&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;5-importance-sampling&quot;&gt;[5] Importance Sampling&lt;/h2&gt;

&lt;p&gt;이 논문에서는 single element sampling과 independent sampling에 대한 importance sampling을 각각 제안한다.&lt;/p&gt;

&lt;h3 id=&quot;51-single-element-sampling&quot;&gt;[5.1] Single element sampling&lt;/h3&gt;

&lt;p&gt;$\mathcal{L}$에 대한 bound와 $\sigma^2$을 Thm 3.1에 대입하면 최소 반복 수를 얻을 수 있는데, $p_i$에 대해 이 최소 반복 수를 최소화하는 문제는 매우 어렵기 때문에 이 논문에서는 &lt;script type=&quot;math/tex&quot;&gt;\mathcal{L}_{\max}&lt;/script&gt; 와 $\sigma^2$ 을 줄이는 것에 집중한다. single element sampling에 대해서는 이전 연구에 있었던 partially biased sampling을 떠올릴 수 있다. &lt;script type=&quot;math/tex&quot;&gt;\mathcal{L}_{\max}&lt;/script&gt;를 최소화시키는 $p_i$는 $p_i^\mathcal{L}=\frac{L_i}{\sum_{j\in [n]} L_j}$인데, 이 확률을 적용하면 이전 연구에 있었던 partially biased sampling과 똑같아진다. uniform sampling과 비교해보면 $L_{\max}=n\bar{L}$인 극한의 상황에서는 partially biased sampling이 절반의 복잡도를 가질 수 있다. &lt;em&gt;왜 갑자기 uniform sampling이랑 비교하지?&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;52-minibatches&quot;&gt;[5.2] Minibatches&lt;/h3&gt;
&lt;p&gt;이 논문에서는 mini-batch SGD를 위한 importance sampling을 최초로 제안한다. 이에 대한 결과는&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;k\ge\max\{(1-\frac{2}{\tau})\frac{2\bar{L}}{\alpha\mu}, (\frac{2}{\tau}-\frac{1}{n})\frac{8\bar{h}}{\epsilon\mu^2}\}&lt;/script&gt;

&lt;p&gt;인데, $L_{\max}$에 대한 의존성을 없앴을 뿐 아니라, $\tau$가 커지면 복잡도가 더 줄어든다.&lt;/p&gt;

&lt;h2 id=&quot;6-experiments&quot;&gt;[6] Experiments&lt;/h2&gt;

&lt;h3 id=&quot;61-constant-vs-decreasing-step-size&quot;&gt;[6.1] Constant vs. decreasing step size&lt;/h3&gt;
&lt;p&gt;이 실험에서는 ridge regression과 logistic regression 문제에 집중했는데, 여기서는 목적 함수가 strongly convex이다. Thm 3.2에서 예상했듯이, 특정 시점에 step size를 줄이면 성능이 더 좋다는 것을 보여준다.&lt;/p&gt;

&lt;h3 id=&quot;62-minibatches&quot;&gt;[6.2] Minibatches&lt;/h3&gt;
&lt;p&gt;이 실험에서는 [3.2]에 소개된 여러 다른 분포의 $\mathcal{D}$(single element sampling, $\tau$ independent sampling, $\tau$-nice sampling)를 선택하여 SGD의 수렴도를 비교하였는데, $\tau^* $를 이용한 방법이 가장 좋은 결과를 보였다. &lt;em&gt;여기 비교했다는건 6갠데 그래프에는 왜 5개만 있음?&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;63-sum-of-non-convex-functions&quot;&gt;[6.3] Sum-of-non-convex functions&lt;/h3&gt;
&lt;p&gt;이 실험에서는 PCA 문제에 집중했는데, 여기서는 $f$는 strongly convex지만 $f_i$는 그렇지 않다. non-convex한 $f_i$에서도 위와 마찬가지로, stepsize를 줄였을 때 더 좋은 성능을 보였고 $\tau^* $가 가장 좋은 성능을 보였다.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;아직 잘 모르겠는 개념 : $\mu$-strongly quasi-convx, condition number, uniform sampling&lt;/p&gt;
</description>
        <pubDate>Sat, 02 Nov 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/thesis/2019/11/02/SGDGENANAL.html</link>
        <guid isPermaLink="true">http://localhost:4000/thesis/2019/11/02/SGDGENANAL.html</guid>
        
        <category>Optimization</category>
        
        
        <category>Thesis</category>
        
      </item>
    
      <item>
        <title>ADAM - A Method for Stochastic Optimization</title>
        <description>&lt;hr /&gt;

&lt;h2 id=&quot;abstract&quot;&gt;[Abstract]&lt;/h2&gt;

&lt;p&gt;이 논문에서는 저차를 이용한 예측에 기반한 확률적 목적함수의 1차 기울기 기반의 최적화 알고리즘인 Adam을 소개한다. 이 방법은 구현하기에 직접적이고, 계산량이 효과적이고, 메모리가 적게 필요하고, 기울기를 diagonal rescaling하는데 변함이 없고, 큰 데이터나 파라미터에 적용하는 데에 적합하다. 이 방법은 또한 매우 노이즈가 많거나 기울기가 sparse한 등의 변화하는 목적이나 문제에 적합하다. 조정 변수는 직관적으로 해석될 수 있으며 약간의 전형적인 튜닝만을 필요로 한다. Adam을 만드는 데에 영감을 준 관련된 알고리즘과의 연결고리도 소개된다. 또한 알고리즘의 이론적인 수렴 성질도 분석했고, 알려진 최고의 알고리즘들과 비교하여 수렴도에 대한 regret bound도 제공한다. 실험 결과는 아담이 다른 최적화 방법들과 비교해도 잘 작동한다는 것을 보여준다. 마지막으로, AdaMax에 대해서도 논의할 것이다.&lt;/p&gt;

&lt;h2 id=&quot;1-introduction&quot;&gt;[1] Introduction&lt;/h2&gt;

&lt;p&gt;1차 기반 최적화는 널리 쓰였는데, 계산량에서 효율적이다. SGD도 많이 쓰이는데, 이때 목적함수는 data subsampling 말고 또다른 노이즈 소스(dropout같은)가 있을 수 있다. 따라서 noisy할 경우에 대해 더 효과적인 알고리즘이 필요하다. 이 논문은 고차원 파라미터 공간에서의 SGD 최적화와 1차 최적화 방법에 중점을 둔다.&lt;/p&gt;

&lt;p&gt;Adam은 1차 방법인데, 적은 메모리를 사용하여 효율적인 확률론적 최적화를 한다. 이 방법은 각각의 파라미터에 대해 gradient의 첫번째와 두번째 moment의 추정치를 이용하여 최적의 learning rate를 계산한다. 또한 AdaGrad(sparse gradient에 효과적)와 RMSProp(on-line, non-stationary에 효과적)의 장점을 합친 방법이다.&lt;/p&gt;

&lt;p&gt;Adam의 장점은 엄청난 수의 파라미터 업데이트가 기울기를 rescaling해도 변화가 없다는 것, step size가 제한되는 것, 불변 목적함수가 필수가 아닌 것, sparse한 기울기에서도 잘 작동하는 것, 그리고 step size annealing을 자동으로 하는 것이다.&lt;/p&gt;

&lt;h2 id=&quot;2-algorithm&quot;&gt;[2] Algorithm&lt;/h2&gt;

&lt;p&gt;$\alpha$ : step size&lt;br /&gt;
$\beta_1, \beta_2$ : moment 예측의 지수 decay rate&lt;br /&gt;
$f(\theta)$ : 목적 함수&lt;br /&gt;
$\theta_0$ : 초기 파라미터&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}\text{while }&amp;\theta_t\text{ not converged, do}\\
&amp; t\leftarrow t+1\\
&amp; g_t\leftarrow\triangledown_\theta f_t(\theta_{t-1})\\
&amp; m_t\leftarrow\beta_1\cdot m_{t-1}+(1-\beta_1)\cdot g_t\\
&amp; v_t\leftarrow\beta_2\cdot v_{t-1}+(1-\beta_2)\cdot g_2^t\\
&amp; \hat{m_t}\leftarrow\frac{m_t}{1-\beta_1^t}\\
&amp; \hat{v_t}\leftarrow\frac{v_t}{1-\beta_2^t}\\
&amp; \theta_t\leftarrow\theta_{t-1}-\alpha\cdot\frac{\hat{m_t}}{\sqrt{\hat{v_t}}+\epsilon}\\
\text{end while}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;알고리즘은 기울기와 기울기의 제곱의 지수평균을 업데이트하고, hyper-parameter $\beta_1, \beta_2$는 이 이동 평균의 지수적인 decay rate을 조절한다. 이 두 움직임의 평균은 기울기의 1차와 2차 모멘트로부터 예측된다. 0으로 초기화 되기 때문에 초기 시점에나 $\beta_1, \beta_2$가 작을 때 0으로 bias되는 경우가 생기는데, 뒤에서 상쇄가 가능하다. 위 알고리즘 1의 뒤쪽에 약간의 순서 변경을 하면 표현의 명확성이 떨어지지만, 효율성을 더 확보할 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;21-adams-update-rule&quot;&gt;[2.1] Adam’s update rule&lt;/h3&gt;
&lt;p&gt;Adam의 특징 중 하나는 업데이트할 때 stepsize를 신중하게 선택해야 한다는 점이다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\triangle t=\alpha \cdot \frac{\hat{m}_t}{\sqrt{\hat{v}_t}}&lt;/script&gt;

&lt;p&gt;이게 effective한 step 이다. 이 stepsize는 $(1-\beta_1)\gt\sqrt{1-\beta_2}$일 경우($|\triangle_t|\le\alpha\cdot\frac{(1-\beta_1)}{\sqrt{1-\beta_2}}$)와 이 경우가 아닐 경우($|\triangle_t|\le\alpha$)에 대해 두 개의 upper bound가 있다. 첫번째 경우는 현재 timestep 제외하고 대부분 gradient가 0일 경우 같은 매우 sparse한 경우이고, 두번째 경우는 덜 sparse한 경우인데, 효과적인 step size가 줄어든다.&lt;/p&gt;

&lt;p&gt;일반적인 경우는 후자인데, 결국 effective stepsize는 $\alpha$에 의해 bound된다. 현재 기울기 예측이 충분한 정보를 주지 않더라도 trust region을 구축하기 위한 것이라고 이해될 수 있다. 따라서 $\alpha$의 적절한 크기를 아는 것이 상대적으로 쉽다. (보통 머신러닝 모델에서 우리는 적절한 파라미터 값을 대략적으로 알고 있다.)&lt;/p&gt;

&lt;p&gt;약간 과장하면, $\frac{\hat{m}_t}{\sqrt{\hat{v}_t}}$ 를 signal-to-noise ratio(SNR)이라고 할 수 있다. SNR이 더 작으면 $\triangle_t$는 0에 더 가까워진다. SNR이 작다는 말은 $\hat{m_t}$가 진짜 기울기 방향으로 가고 있다는 데에 더 큰 불확실성을 갖는다는 말, 즉 현재 optima에 거의 다 왔으므로 어디로 갈지 모르는 상황이 되어 effective stepsize가 더 작아진다. 따라서 자동 annealing의 형태를 띤다. 또한 $\triangle_t$는 gradient의 scale이 변해도 변하지 않는다(분수이므로 scale된것이 사라진다).&lt;/p&gt;

&lt;h2 id=&quot;3-initialization-bias-correction&quot;&gt;[3] Initialization Bias Correction&lt;/h2&gt;

&lt;p&gt;우리는 제곱된 기울기의 지수 평균과 decay rate $\beta_2$를 이용하여 f의 2차 moment를 알아내려고 한다. 먼저 지수 이동 평균을 0으로 초기화한다. 위 알고리즘의 식에 대입하면, 다음 지수 이동 평균을 구할 때 이전의 gradient가 $\beta_2$의 지수만큼 기여하게 된다. $\mathbb{E}[v_t]$와 $\mathbb{E}[g_t^2]$와의 관계성을 알면 그 차이를 없앨 수 있다. 알고리즘의&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;v_t = (1-\beta_2)\sum_{i=1}^t\beta_2^{t-i}\cdot g_i^2&lt;/script&gt;

&lt;p&gt;식에 평균을 취하면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbb{E}[v_t] = \mathbb{E}[g^2_t]·(1−\beta_t^2) +\zeta&lt;/script&gt;

&lt;p&gt;이 식을 얻을 수 있다. 만약 $g_t$가 변하지 않는 값이면 $\zeta=0$이 되는데, 만약 $g_t$가 변하더라도 아주 이전의 gradient에는 작은 계수가 붙기 때문에, $\zeta$는 아주 작은 숫자가 된다. $1-\beta_2^t$는 running average를 0으로 초기화하면서 생긴 텀인데, 초기 bias를 없애주기 위해 알고리즘 1에서는 bias correction을 위해 이 식으로 나눠준다.&lt;/p&gt;

&lt;p&gt;gradient가 sparse할 경우에는, 믿을만한 2차 모멘트의 예측을 위해서는 작은 $\beta_2$를 선택하여 많은 기울기를 평균해야 한다. 그러나 작은 $\beta_2$를 선택하면 초기 bias의 correction이 없게 되므로(bias correction은 $1-\beta_2$로 나누는 것이므로, 작은 $\beta_2$를 선택할 경우 나누는 항이 1이 되어 bias correction이 줄어든다) sparse하지 않을 경우보다 초기 step이 더 커지게 된다. (앞의 [2.1] stepsize의 bound 식을 보면 이 경우의 bound가 더 크다)&lt;/p&gt;

&lt;h2 id=&quot;4-convergence-analysis&quot;&gt;[4] Convergence Analysis&lt;/h2&gt;

&lt;p&gt;각 시간 t에 대해, 우리의 목적은 $\theta_t$를 예측하고 그 예측을 unknown cost function $f_t$에 대해 평가하는 것이다. sequence가 이전에 알려져있지 않기 때문에 regret이라는 것을 사용해서 평가한다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;R(T) = \sum_{t=1}^T[f_t(\theta_t)-f_t(\theta^* )]&lt;/script&gt;

&lt;p&gt;이게 regret 식이다. Adam은 $O(\sqrt{T})$의 regret bound 갖고 있다. 이 결과는 현존하는 online learning중 최고이다.&lt;/p&gt;

&lt;h3 id=&quot;thm41&quot;&gt;[Thm4.1]&lt;/h3&gt;

&lt;p&gt;Thm 1은 $\alpha_t$가 $t^{-1/2}$로 decay하고, $\beta_1$이 $\lambda^t$ 로($\lambda$는 $1$에 엄청나게 가까운 수)로 decay할 때 성립한다. Thm1은 데이터가 sparse하고 제한된 기울기를 갖고 있을 때, $\sum$ 항이 그 상한보다 훨씬 작다는 것을 의미한다(특정한 함수와 데이터 특징에 대해). 이 결과에 기댓값($\mathbb{E}$)을 적용하면 Adam에도 적용될 수 있다. Adaptive method는 non-adaptive 모델보다 더 좋은 convergence를 갖는다. $\beta_1$의 decay가 중요한 역할을 하는데, 이전에 있었던 모멘텀 계수를 줄이는 것이 수렴을 향상시킨다는 연구결과와도 들어맞는다.&lt;/p&gt;

&lt;h3 id=&quot;cor42&quot;&gt;[Cor4.2]&lt;/h3&gt;

&lt;p&gt;Thm4.1의 따름정리인데, 결국은 $\frac{R(T)}{T}$ 가 0으로 수렴한다는 내용이다.&lt;/p&gt;

&lt;h2 id=&quot;5--related-work&quot;&gt;[5]  Related Work&lt;/h2&gt;

&lt;p&gt;Adam에 직접적으로 영향을 준 연구는 RMSProp과 AdaGrad이다. vSGD, AdaDelta, natural Newton Method같은 stochastic 방법들은 1차 미분 정보에서 곡률을 예측하여 stepsize를 정한다. SFO는 mini-batch 기반의 quasi-newton 방법인데, Adam과 달리 선형적인 메모리가 필요해 GPU에서는 사용할 수 없다. NGD와 같이 Adam은 데이터의 구조에 적응하는 preconditioner를 도입했다(Adam에서의 $\hat{v_t}$는 Fisher information matrix의 근사이다). 그러나 Adam의 preconditioner는 AdaGrad에서와 비슷한데, 그냥 NGD보다 더 적응이 느리다.&lt;/p&gt;

&lt;h3 id=&quot;기본지식&quot;&gt;[기본지식]&lt;/h3&gt;

&lt;p&gt;newton’s method : GD랑 비슷하지만 $x_n+1 = x_n - \frac{f’}{f’’}$ 으로 업데이트 하는 방법&lt;br /&gt;
quasi-newton method : newton’s method와 비슷하지만 계산량이 훨씬 적다.$f’’$ 대신 $f’‘$을 근사한 행렬을 사용한다.&lt;/p&gt;

&lt;h3 id=&quot;rmsprop&quot;&gt;[RMSProp]&lt;/h3&gt;

&lt;p&gt;모멘텀을 이용한 RMSProp에서는 rescale된 gradient에 momentum을 사용하여 파라미터 업데이트를 하는데, Adam은 현재 기울기의 1차, 2차 momentum의 평균을 이용하여 바로 예측한다(rescale이 없음). 또한 RMSProp은 bias-correction 항이 없다. 이러면 너무 큰 stepsize나 발산으로 이어질 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;adagrad&quot;&gt;[AdaGrad]&lt;/h3&gt;

&lt;p&gt;sparse한 gradient에 잘 작동하는 알고리즘이다. Adam에서 $\alpha$와 $\beta$를 적절하게 설정하면 AdaGrad와 똑같다. bias-correction 항이 없으면 비슷하지 않게 된다. 과거의 기울기의 변화량(momentum)을 참고하여 update하는 것이 특징이다.&lt;/p&gt;

&lt;h2 id=&quot;6-experiments&quot;&gt;[6] Experiments&lt;/h2&gt;

&lt;p&gt;큰 모델과 데이터셋을 사용하므로써, Adam이 현실에서도 효과적이라는 것을 증명한다. hyper-parameter의 경우는, dense grid로 찾고, 가장 좋은 결과를 내는 hyper-parameter를 비교에 사용한다.&lt;/p&gt;

&lt;h3 id=&quot;61-experiment--logistic-regression&quot;&gt;[6.1] Experiment : Logistic Regression&lt;/h3&gt;

&lt;p&gt;L2 정규화된 multi-class logistic regression을 MNIST를 이용하여 평가했다. logistic regression은 convex한 목적함수를 갖고 있다. Adam은 모멘텀을 사용한 SGD와 비슷하게 수렴했고 Adagrad보다는 빨랐다.&lt;/p&gt;

&lt;p&gt;AdaGrad는 feature와 gradient가 sparse할 때 유리하다. Adam이 $\frac{1}{\sqrt{t}}$의 stepsize를 사용한다면 이론적으로 AdaGrad의 성능과 같다. 이 부분에 대해서는 IMDB 영화 리뷰 데이터셋을 이용해 sparse feature 문제를 시험했다. 결과는 이론과 같았다.&lt;/p&gt;

&lt;h3 id=&quot;62-experiment--multi-layer-neural-networks&quot;&gt;[6.2] Experiment : Multi-Layer Neural Networks&lt;/h3&gt;

&lt;p&gt;여기서 non-convex한 함수에 대한 분석은 이루어지지 않았지만, 경험적으로 Adam이 이런 상황에서도 좋은 성능을 낸다는 것을 알았다. 우선 기본 deterministic cross-entropy 목적함수와 L2 weight decay를 사용하여 여러 다른 optimizer들을 연구했다. SFO 방법은 최근에 제안된 minibatch 기반의 quasi-Newton 방법인데,  multi-layer NN에서 좋은 성능을 보여준다. 그런데 Adam의 iteration이 덜 필요했고, 절대적인 시간도 빨랐다. (곡선 정보를 update하는 데에 SFO는 시간이 더 필요하고, linear memory가 필요하다.) 또한 dropout을 사용한 다른 확률론적 방법들과의 비교에서도 Adam이 월등한 성적을 보여주었다.&lt;/p&gt;

&lt;h3 id=&quot;63-experiment--convolutional-neural-networks&quot;&gt;[6.3] Experiment : Convolutional Neural Networks&lt;/h3&gt;

&lt;p&gt;CNN에서도 Adam이 효율적으로 작동할 수 있다. 처음에는 Adam과 AdaGrad가 빠른데, 나중에는 SGD와 Adam이 잘 수렴한다. 이 이유는 2차 moment 예측 $\hat{v_t}$가 몇 epoch뒤에는 사라지고 $\epsilon$이 성능을 좌우하기 때문이다. fully connected NN과 비교할 때, CNN에서는 2차 moment 예측은 cost function의 지형을 근사하는 데에 실패한 것이다. CNN에서는 1차 moment를 통해 minibatch의 분산을 줄이는 것이 더 중요하다. Adam은 SGD보다 약간 더 좋은데, 또한 SGD처럼 수동으로 learning rate를 조절해야 하는 것이 아니라 자동으로 서로 다른 layer들에 대한 learning rate를 조절해준다.&lt;/p&gt;

&lt;h3 id=&quot;64-experiment--bias-correction-term&quot;&gt;[6.4] Experiment : Bias-Correction Term&lt;/h3&gt;

&lt;p&gt;bias term은 Adam에서 매우 중요하며 bias correction term을 없애는 것은 momentum을 사용한 RMSProp과 같은 결과를 보여준다. 이 논문에서는 $\beta_1$과 $\beta_2$, $\alpha$를 다양하게 바꾸며 실험했다. $\beta_2$가 1에 가까우면 sparse한 gradient에 견고성이 더 요구되므로 더 큰 초기 bias가 생긴다. 따라서 이 논문에서는 느린 decay같은 상황에서 bias correction term이 중요하다고 예상했고, 실험 결과에서는 $\beta_2$가 1에 가깝고 bias term이 있는 것이 가장 성능이 좋았다(예상한 것이 맞았다). 결과적으로, Adam은 RMSProp보다 같거나 더 좋다.&lt;/p&gt;

&lt;h2 id=&quot;7-extensions-안읽었음&quot;&gt;[7] Extensions &lt;em&gt;안읽었음&lt;/em&gt;&lt;/h2&gt;

&lt;h2 id=&quot;8-conclusion&quot;&gt;[8] Conclusion&lt;/h2&gt;

&lt;p&gt;이 논문에서는 간단하고 계산효율적인 기울기 기반의 확률 목적 함수 최적화 알고리즘을 소개하였다. 이 방법은 큰 데이터셋과 고차 파라미터 공간을 위한 것이다. Adam은 두 유명한 최적화 방법인 AdaGrad 와 RMSProp의 장점을 결합하여 만들어졌는데, AdaGrad의 sparse한 기울기에 대한 장점과 RMSProp의 변동 목적함수에 대한 장점이다. 이 방법은 구현이 직관적이고 적은 메모리를 사용한다. 실험은 convex 문제에서의 수렴도 해석을 증명해주었다. 종합하면, Adam은 딥러닝에서의 다양한 non-convex 최적화 문제에서 강건하고 잘 맞는다.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;다음에 읽어볼 논문 : &lt;a href=&quot;http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf&quot;&gt;AdaGrad&lt;/a&gt;, RMSProp&lt;/p&gt;
</description>
        <pubDate>Wed, 30 Oct 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/thesis/2019/10/30/ADAM.html</link>
        <guid isPermaLink="true">http://localhost:4000/thesis/2019/10/30/ADAM.html</guid>
        
        <category>Optimization</category>
        
        
        <category>Thesis</category>
        
      </item>
    
      <item>
        <title>SGDR - Stochastic Gradient Descent with warm Restarts</title>
        <description>&lt;hr /&gt;

&lt;h2 id=&quot;abstract&quot;&gt;[Abstract]&lt;/h2&gt;
&lt;p&gt;재시작 방법은 gradient-free 최적화에서 멀티모달 함수에 적용할 때 자주 쓰인다. 부분적 재시작 또한 gradient기반 최적화에서 ill-conditioned 함수에서 수렴도를 개선하기 위해 자주 쓰이는 추세이다. 이 논문에서는 SGD를 위한 간단한 재시작 테크닉을 소개하는데, 딥네트워크를 학습시킬 때 항상(anytime) 결과를 향상시킬 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;1-introduction&quot;&gt;[1] Introduction&lt;/h2&gt;
&lt;p&gt;GD를 쓸때 hessian을 쓰면 더 좋은데 계산량이 많다. AdaDelta와 Adam은 hessian을 잘 줄여서 사용한 좋은 예이다. 그런데 sota 결과는 사실 특별한 방법을 쓴게 아니라 SGD에 momentum만 추가한 것이었다.&lt;/p&gt;

&lt;p&gt;보통의 learning rate schedule은 정해진 상수를 일정 간격의 상수로 나누는 것이었는데, 이 논문에서 제안하는 새로운 learning rate schedule은 주기적으로 SGD를 재시작하는 방법이다. 실험 결과에 의하면 재시작 방법은 원래 쓰이던 방법보다 2배에서 4배정도 적은 epoch만으로 비슷한 결과를 낼 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;2-related-work&quot;&gt;[2] Related Work&lt;/h2&gt;
&lt;p&gt;gradient-free optimization에서는 많은 local optima를 찾는 것이 목적이다. niching 방법 기반 방법들은 local optimizer를 전체 space에 다 적용시킬 수 있는데, 차원의 저주 때문에 확장시킬 수는 없다. 최근에는 다양한 재시작 매커니즘들을 사용하는데, 한 방법에서는 많은 후보를 쓰면 더 글로벌한 검색이 가능한데, 각 재시작 처음엔 적은 후보를 쓰고 각 재시작 후에는 키우는 방법을 사용하는 것이 일반적이다.&lt;/p&gt;

&lt;p&gt;gradient-based optimization에서는 gradient-free 에서보다 $n$배의 속도 향상이 있다. 이때 재시작 방법은 multimodality를 해결하기 위함보다는 수렴도를 개선하기 위해 사용된다.&lt;/p&gt;

&lt;h2 id=&quot;3-sgdr&quot;&gt;[3] SGDR&lt;/h2&gt;
&lt;p&gt;현존하는 재시작 방법은 SGD에도 적용될 수 있다. 데이터의 덩어리에 따라 loss value와 기울기가 다양할 수 있어서, 기울기나 loss의 평균을 내는 등의 노이즈의 제거가 필요하다.&lt;/p&gt;

&lt;p&gt;이 논문에서는 지정된 epoch까지 도달하면 다시 재시작을 하는 가장 간단한 재시작 방법을 사용한다. 그리고 제안된 cosine annealing이라고도 불리는 learning rate schedule은 다음과 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\eta_t=\eta_{min}^i+\frac{1}{2}(\eta_{max}^i−\eta_{min}^i)(1 + \cos(\frac{T_{cur}}{T_i}\pi))&lt;/script&gt;

&lt;p&gt;여기서 $\eta_t$는 learning rate, $\eta_{min}^i$와 $\eta_{max}^i$는 learning rate의 범위, $T_{cur}$는 현재 epoch, $T_i$는 지정된 epoch(이만큼 지나면 재시작)이다.&lt;/p&gt;

&lt;p&gt;재시작은 learning rate ($\eta_t$)을 증가시키므로써 수행되고, $x_t$는 초기 해로 사용된다. learning rate는 $\eta_{max}^i$부터 $\eta_{min}^i$까지 줄어들고, 정해진 epoch를 돌면 다시 처음부터 시작한다. $T_{mult}$라는 변수를 이용하여 재시작마다 줄어드는 간격을 점점 넓힐 수도 있다.&lt;/p&gt;

&lt;p&gt;처음 재시작 전에는 $x_t$를 초기 해로 사용하지만, 그 다음에는 이전의 최소 learning rate로부터 얻어진 $x$를 초기 해로 사용한다. (이 점이 중요한 부분임)
&lt;em&gt;그런데 계속 저렇게 사용하는거? 아님 재시작마다 저렇게 사용하는거?&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;4-experimental-results&quot;&gt;[4] Experimental Results&lt;/h2&gt;
&lt;h3 id=&quot;42-single-model-result&quot;&gt;[4.2] Single-Model Result&lt;/h3&gt;
&lt;p&gt;$T_0=200$의 결과가 가장 좋은데, 가장 마지막 몇 epoch에서 좋아진다. $T_{mult}=2$는 재시작 후 주기를 2배로 늘려주는데, 이렇게 하는 이유는 좋은 테스트 에러에 가장 빨리 도달하기 위함이다. SGDR이 좋은 성능에 빠르게 도달할 수 있기 때문에, 더 큰 신경망을 학습시킬 수 있다. 따라서 WRN을 2배 넓게 만들어서 학습시켰다.&lt;/p&gt;

&lt;p&gt;SGDR 자체 실험에서는 SGDR과 기본 스케줄을 비교했는데, 120 epoch까지는 더 빨리 training loss가 줄었다. 이 이후에 기본 스케줄은 overfit되었다. 결론적으로, SGDR은 overfit이 잘 되지 않는다.&lt;/p&gt;

&lt;h3 id=&quot;43-ensemble-results&quot;&gt;[4.3] Ensemble Results&lt;/h3&gt;
&lt;p&gt;SGDR은 WRN논문의 follow-up study에서 영감을 얻었다.  여기서는 재시작 전마다 snapshot을 찍고 그것으로 앙상블 모델을 만든다. 결과로는, 3번 돌려서 앙상블하는 것보다 한번 돌려서 3번 스냅샷 찍어서 앙상블하는게 낫다. SGDR에서 찍은 스냅샷은 앙상블을 할 때의 유용한 다양성을 제공해 준다. 이 결과는 WRN보다 더 좋은 모델에서 더 좋은 결과를 낼 것이다.&lt;/p&gt;

&lt;h3 id=&quot;45-preliminary-experiments-on-a-downsampled-imagenet-dataset&quot;&gt;[4.5] Preliminary Experiments on a Downsampled ImageNet Dataset&lt;/h3&gt;
&lt;p&gt;다운샘플된 이미지넷 데이터는 원래보다 더 어렵고 이미지의 대부분을 대상이 차지하는 CIFAR-10보다도 더 어렵다.&lt;/p&gt;

&lt;h2 id=&quot;5-discussion&quot;&gt;[5] Discussion&lt;/h2&gt;
&lt;p&gt;이 learnin rate schedule은 재시작 없이도 충분히 경쟁적이고, 단 두 개의 파라미터(초기 lr, epoch 수)만을 필요로 한다. 재시작 방법의 목적은 ‘항상(anytime)’ 좋은 성능을 내기 위함이다. 매 재시작마다 $\eta_{max}$와 $\eta_{min}$을 줄이는 방법도 가능하다. SGDR 중 얻어진 중간 모델은 앙상블에 사용될 수 있고, cost도 들지 않는다는 점을 이용했다.&lt;/p&gt;

&lt;h2 id=&quot;6-conclusion&quot;&gt;[6] Conclusion&lt;/h2&gt;
&lt;p&gt;WRN에서는 더 넓은 모델을 사용하고 스냅샷을 앙상블에 사용해 sota 결과를 만들어냈고, EEG에서는 더 재시작을 많이 하고 더 스냅샷을 많이 찍으면 더 좋은 결과를 낸다는 것을 알았다. Downsampled ImageNet 데이터에서는 SGDR이 scan을 통해 lr을 선택하는 문제를 줄여준다는 것을 알았다. 다음 연구는 AdaDelta나 Adam에 적용하는 것이 될 것이다.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;다음에 읽어볼 논문 : &lt;a href=&quot;https://arxiv.org/abs/1704.00109&quot;&gt;Snapshot&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1605.07146&quot;&gt;WRN&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Sun, 27 Oct 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/thesis/2019/10/27/SGDR.html</link>
        <guid isPermaLink="true">http://localhost:4000/thesis/2019/10/27/SGDR.html</guid>
        
        <category>Optimization</category>
        
        
        <category>Thesis</category>
        
      </item>
    
      <item>
        <title>Optimization Lecture 10</title>
        <description>&lt;h1 id=&quot;mirror-descent&quot;&gt;Mirror Descent&lt;/h1&gt;

&lt;p&gt;지금까지의 모든 결과들(특히 Lipschitz에 관해서)은 유클리드 공간에서 정의되었다. 그런데 Lipschitz는 norm에 따라서 크기가 달라지는데, 다른 norm에 관해서는 어떤 convergence speed를 가지게 될까 하는 궁금증이 생기게 된다.&lt;/p&gt;

&lt;h3 id=&quot;dual-space&quot;&gt;Dual Space&lt;/h3&gt;

&lt;p&gt;이 궁금증을 해결하기 위해, 먼저 Dual space를 정의한다. 모든 벡터공간 $V$는 $V$에서 정의된 모든 선형 함수에 대해서 항상 dual space $V^* $를 갖는다. 모든 Tangent Line ($y=f(x^* )+f’(x^* )(x-x^* )$)는 항상 선형이기 때문에, 모든 gradient에 대해서는 항상 dual space를 갖는다.&lt;/p&gt;

&lt;h3 id=&quot;dual-norm&quot;&gt;Dual Norm&lt;/h3&gt;

&lt;p&gt;$\mathbb{R}^n$에서 정의된 모든 norm &lt;script type=&quot;math/tex&quot;&gt;\|\cdot \|&lt;/script&gt;에 대해 dual space에서의 norm 또한 항상 존재하는데, dual norm &lt;script type=&quot;math/tex&quot;&gt;\|\cdot\|_ *&lt;/script&gt;은 다음과 같이 정의한다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\|g\|_ * =\sup_{x\in\mathbb{R}^n:\|x\|\le 1}g^T x&lt;/script&gt;

&lt;p&gt;말이 어려운데, $p$-norm에 대해 생각해 보면 다음과 같은 관계가 있다고 한다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{1}{p}+\frac{1}{q} = 1&lt;/script&gt;

&lt;p&gt;즉, 원래 공간에서 $p$-norm을 사용하였다면 dual space에서는 $q$-norm을 사용하면 된다. 이러한 새로운 dual의 정의에서, Lipschitz는 다음과 같이 쓸 수 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\|g\|_ * \le L, \forall x \in X, \forall g \in \partial f(x)&lt;/script&gt;

&lt;h3 id=&quot;bregman-divergence&quot;&gt;Bregman Divergence&lt;/h3&gt;

&lt;p&gt;Dual space에서의 convergence를 해석하기 위해 Bregman divergence라는 것을 정의한다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;D_f(x,y)=f(x)-f(y)-\triangledown f(y)^T(x-y)&lt;/script&gt;

&lt;p&gt;사실 첫 항을 제외한 항은 Tangent Line을 의미하는 식이다. 결국 Bregman divergence는 한 점에서의 접선에 대해 같은 $y$값에서 원래 함수와 Tangent Line의 차이를 의미한다. Bregman divergence에 관한 특성으로는 다음과 같은 것이 있다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;$(\triangledown f(x)-\triangledown f(y)^T(x-z)=D_f(x,y)+D_f(z, x)-D_f(z, y)$&lt;/li&gt;
  &lt;li&gt;$\lambda$-strongly convex인 함수 $h$에 대하여 &lt;script type=&quot;math/tex&quot;&gt;D_f(x, y)\ge \frac{\lambda}{2}\|x-y\|^2\ge 0&lt;/script&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;mirror-map&quot;&gt;Mirror Map&lt;/h3&gt;

&lt;p&gt;우선 $D\in\mathbb{R}^n$은 $X\subset\bar{D}$인 open set이라고 하자. Mirror Map $\Phi$는 $D$에서 $\mathbb{R}^n$으로의 mapping function인데, 다음과 같은 조건이 있다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;$\Phi$는 convex하고 미분가능한 함수이다.&lt;/li&gt;
  &lt;li&gt;$\Phi$의 gradient는 어떤 숫자든 가능하다.&lt;/li&gt;
  &lt;li&gt;$\Phi$의 gradient는 $D$의 가장자리에서 발산한다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;이렇게 놓고 보면, 이전에 우리가 썼던 gradient descent 식 $x_{t+1}=x_t-\gamma\triangledown f(x_t)$가 좀 이상해 보이기 시작한다. $x_t$는 원래 공간인데, $\triangledown f(x_t)$는 dual space에서 정의되는 것이기 때문이다. 사실 이전에는 유클리드 norm을 기준으로 진행했기 때문에 dual space의 norm도 유클리드 norm이 되어서 상관이 없었다. 그렇지만 이제는 다르므로 gradient descent를 새롭게 정의할 필요가 있다.&lt;/p&gt;

&lt;h3 id=&quot;mirror-descent-1&quot;&gt;Mirror Descent&lt;/h3&gt;

&lt;p&gt;다음은 Dual space 공간에서의 gradient descent 알고리즘이다. Mirror Descent라고도 한다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;$x_t$를 mirror map $\triangledown \Phi (x_t)$에 매핑시킨다. 이후는 모두 dual space이다.&lt;/li&gt;
  &lt;li&gt;$\triangledown \Phi (x_t)-\gamma \triangledown f(x_t)$&lt;/li&gt;
  &lt;li&gt;$\triangledown\Phi(y_{t+1})=\triangledown\Phi(x_t)-\gamma\triangledown f(x_t)$를 만족하는 $y_{t+1}$를 찾는다.&lt;/li&gt;
  &lt;li&gt;다시 원래 공간으로 가져오는데, constrained set $X$ 안에 $x_{t+1}$이 있어야 하기 때문에 projection을 한다. &lt;script type=&quot;math/tex&quot;&gt;x_{t+1}=\Pi_X^\Phi(y_{t+1})=\arg\min_{x\in X}D_\Phi(x, y_{t+1})&lt;/script&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Mirror Descent는 proximal gradient와도 연결된다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}x_{t+1}&amp;=\arg\min_{x\in X}D_\Phi(x, y_{t+1})\\
&amp;=\arg\min_{x\in X}\{\Phi(x)-\triangledown \Phi(y_{t+1})^Tx-\Phi(y_{t+1})+\triangledown\Phi(y_{t+1})^Ty_{t+1}\}\\
&amp;=\arg\min_{x\in X}\{\Phi(x)-\triangledown\Phi(y_{t+1})^Tx\}\\
&amp;=\arg\min_{x\in X}\{\Phi(x)-(\triangledown\Phi(x_t)-\gamma\triangledown f(x_t))^Tx\}\\
&amp;=\arg\min_{x\in X}\{\gamma\triangledown f(x_t)^Tx+\Phi(x)-\Phi(x_t)-\triangledown\Phi(x_t)^T(x-x_t)\}\\
&amp;=\arg\min_{x\in X}\{\gamma\triangledown f(x_t)^Tx+D_\Phi(x, x_t)\}\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;중간에 $y$만에 관한 항들은 $\arg\min$이므로 마음대로 넣어도 상관 없고, 마찬가지로 $x_t$만에 관한 항은 마음대로 넣어도 상관없다.&lt;/p&gt;

&lt;h3 id=&quot;mirror-descent--l-lipschitz-continuous&quot;&gt;Mirror Descent : L-Lipschitz continuous&lt;/h3&gt;

&lt;p&gt;우선 함수에 대해서, $\Phi$는 $\rho$-strongly convex이고, $f$는 convex이고 L-Lipschitz이다. 그리고 &lt;script type=&quot;math/tex&quot;&gt;R^2=\sup_{x\in X}\{\Phi(x)-\Phi(x_1)\}&lt;/script&gt;이다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}f(x_t)-f(x^* ) &amp; \le g_t^T(x_t-x^* )\\
&amp;= \frac{1}{\gamma}(\triangledown\Phi(x_t)-\triangledown\Phi(y_{t+1}))^T(x_t-x^* )\\
&amp;=\frac{1}{\gamma}(D_\Phi(x^* , x_t)+D_\Phi(x_t, y_{t+1})-D_\Phi(x^* , y_{t+1}))\\
&amp;\le\frac{1}{\gamma}(D_\Phi(x^* , x_t)+D_\Phi(x_t, y_{t+1})-D_\Phi(x^* , x_{t+1})-D_\Phi(x_{t+1}, y_{t+1}))\text{ (by } \triangledown\Phi(x^* , y_{t+1})\ge D_\Phi(x^* , x_{t+1})+D_\Phi(x_{t+1}, y_{t+1})\text{)}\\
&amp;= \frac{1}{\gamma}(D_\Phi(x^* , x_t)-D_\Phi(x^* , x_{t+1}))+\frac{1}{\gamma}(D_\Phi(x_t, y_{t+1})-D_\Phi(x_{t+1}, y_{t+1}))\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;모든 $T$에 대해서 다 더하면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{t=1}^T(f(x_t)-f(x^* ))=\frac{1}{\gamma}(D_\Phi(x^* , x_1)-D_\Phi(x^* , x_{T+1}))+\frac{1}{\gamma}\sum_{t=1}^T(D_\Phi(x_t, y_{t+1})-D_\Phi(x_{t+1}, y_{t+1}))&lt;/script&gt;

&lt;p&gt;인데, 마지막 $\sum$항만 bound할 수 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}D_\Phi(x_t, y_{t+1})-D_\Phi(x_{t+1}, y_{t+1})&amp;=\Phi(x_t)-\Phi(x_{t+1})-\triangledown\Phi(y_{t+1})^T(x_t-x_{t+1})\\
&amp;\le(\triangledown\Phi(x_t)-\triangledown\Phi(y_{t+1}))^T(x_t-x_{t+1})-\frac{\rho}{2}\|x_t-x_{t+1}\|^2\text{ (by }\rho\text{-strongly convex)}\\
&amp;=\gamma g_t^T(x_t-x_{t+1})-\frac{\rho}{2}\|x_t-x_{t+1}\|^2\\
&amp;\le \gamma L\|x_t-x_{t+1}\|-\frac{\rho}{2}\|x_t-x_{t+1}\|^2\text{ (by L-Lipschitz)}\\
&amp;\le\frac{\gamma^2L^2}{2\rho}\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;마지막 항은, 그 전 식이 &lt;script type=&quot;math/tex&quot;&gt;\|x_t-x_{t+1}\|^2&lt;/script&gt;에 관한 이차식이고, 위로 볼록한 함수이기 때문에 미분해서 $0$이 되는 점이 최대점이라는 점을 이용했다. 다시 $\sum$으로 돌아가면,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}\sum_{t=1}^T(f(x_t)-f(x^* ))&amp;\le\frac{1}{\gamma}(D_\Phi(x^* , x_1)-D_\Phi(x^* , x_{T+1}))+\frac{1}{\gamma}\cdot T\cdot\frac{\gamma^2L^2}{2\rho}\\
&amp;=\frac{1}{\gamma}(D_\Phi(x^* , x_1)-D_\Phi(x^* , x_{T+1}))+\frac{\gamma TL^2}{2\rho}\\
&amp;\le\frac{1}{\gamma}D_\Phi(x^* , x_1)+\frac{\gamma TL^2}{2\rho}\text{ (by Bregman Divergence property, }D_\Phi(x^* , x_{T+1})\ge 0\text{)}\\
&amp;\le \frac{R^2}{\gamma}+\frac{\gamma TL^2}{2\rho}\text{ (아직도 왜이런지 모르겠음)}\\
&amp;\le RL\sqrt{\frac{2T}{\rho}}\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;따라서, 다음과 같은 결론이 나온다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(\bar{x})-f(x^* )\le RL\sqrt{\frac{2}{\rho T}}&lt;/script&gt;

&lt;h3 id=&quot;proof-d_phix-y_t1ge-d_phix-x_t1d_phix_t1-y_t1&quot;&gt;(Proof) &lt;script type=&quot;math/tex&quot;&gt;D_\Phi(x, y_{t+1})\ge D_\Phi(x, x_{t+1})+D_\Phi(x_{t+1}, y_{t+1})&lt;/script&gt;&lt;/h3&gt;

&lt;p&gt;위 증명에서 그냥 넘어갔던 위 명제를 증명해보자.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}D_\Phi(x, x_{t+1})&amp;=D_\Phi(x)-\Phi(x_{t+1})-\triangledown\Phi(x_{t+1})^T(x-x_{t+1})\\
D_\Phi(x_{t+1}, y_{t+1})&amp;=\Phi(x_{t+1})-\Phi(y_{t+1})-\triangledown\Phi(x_{t+1})^T(x_{t+1}-y_{t+1})\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;이다. 두 식을 더하면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;D_\Phi(x, x_{t+1})+D_\Phi(x_{t+1}, y_{t+1})=\Phi(x)-\Phi(y_{t+1})-\triangledown\Phi(y_{t+1})^T(x-y_{t+1})-(\triangledown\Phi(x_{t+1})-\triangledown\Phi(y_{t+1}))^T(x-x_{t+1})&lt;/script&gt;

&lt;p&gt;을 얻게 되는데, 마지막 항을 제외한 식은 $D_\Phi(x, y_{t+1})$이다. 마지막 항은 $-\triangledown_xD_\Phi(x_{t+1}, y_{t+1})^T(x-x_{t+1})$ 라고도 쓸 수 있는데, $x_{t+1}$은 정의에 따라 Bregman Divergence에서의 최적값이므로, 이 항은 $0$보다 무조건 작다. 따라서&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;D_\Phi(x, y_{t+1})\ge D_\Phi(x, x_{t+1})+D_\Phi(x_{t+1}, y_{t+1})&lt;/script&gt;

&lt;p&gt;가 성립함을 알 수 있다.&lt;/p&gt;
</description>
        <pubDate>Tue, 01 Oct 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/lecture/optimization/2019/10/01/OptLecture10.html</link>
        <guid isPermaLink="true">http://localhost:4000/lecture/optimization/2019/10/01/OptLecture10.html</guid>
        
        
        <category>Lecture</category>
        
        <category>Optimization</category>
        
      </item>
    
      <item>
        <title>Optimization Lecture 9</title>
        <description>&lt;h1 id=&quot;subgradient&quot;&gt;Subgradient&lt;/h1&gt;

&lt;p&gt;만약 함수 $f(x)$가 미분불가능하다면, 우리는 임의의 gradient를 정해야 한다. subgradient를 정의할 수 있는데,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(y) \ge f(x) + g^T(y-x), \forall y\in X&lt;/script&gt;

&lt;p&gt;인 모든 $g$를 subgradient라고 한다. $\partial f(x)$가 subgradient의 집합을 의미한다. 원래함수 $f(x)$가 convex라면 subgradient에서 gradient descent를 써서 같은 결과를 낼 수 있다. subgradient는 저 조건만 충족하면 되기 때문에 한 점에서 여러 개의 subgradient가 발생할 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;subgradient-descent--l-lipschitz-continuous&quot;&gt;Subgradient Descent : L-Lipschitz continuous&lt;/h3&gt;

&lt;p&gt;여태까지 우리가 $\gamma = \frac{1}{\beta}$를 쓸 수 있었던 것은 $\beta$-smooth를 가정했기 때문인데, 실제 미분값 대신 subgradient를 사용해야 하는 함수라면 smooth한 함수가 아닐 가능성이 크다. 따라서 모든 subgradient에서는 Lipschitz continuous를 가정한다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}f(x_t) -f(x^* ) &amp;\le g_t^T(x_t-x^* )\\
&amp;=\frac{1}{\gamma}(x_t-x_{t+1})^T(x_t-x^* )\\
&amp;= \frac{1}{2\gamma}(\|x_t-x_{t+1}\|^2+\|x_t-x^* \|^2-\|x_{t+1}-x^* \|^2)\\
&amp;\le \|g_t\|^2+\frac{1}{2\gamma}(\|x_t-x^* \|^2-\|x_{t+1}-x^* \|^2)\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;라고 할 수 있는데, 모든 $t$에 대하여 다 더하면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{t=0}^{T-1}(f(x_t)-f(x^* ))\le\frac{1}{2\gamma}(\|x_0-x^* \|^2-\|x_T-x^* \|^2)+\sum_{t=0}^{T-1}\frac{\gamma}{2}\|g_t\|^2&lt;/script&gt;

&lt;p&gt;이라고 할 수 있다. 다시 말하면,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}f(\bar{x})-f(x^* ) &amp;\le \frac{1}{T}\sum_{t=0}^{T-1}(f(x_t)-f(x^* ))\\
&amp;\le \frac{1}{T}\{\frac{1}{2\gamma}(\|x_0-x^* \|^2-\|x_T-x^* \|^2)+\sum_{t=0}^{T-1}\frac{\gamma}{2}\|g_t\|^2\}\\
&amp;\le \frac{1}{T}(\frac{1}{2\gamma}\|x_0-x^* \|^2+\sum_{t=0}^{T-1}\frac{\gamma}{2}\|g_t\|^2) \\
&amp;\le(\frac{1}{2\gamma}R^2+\frac{\gamma}{2}B^2T)\\
&amp;= \frac{1}{T}(\frac{B\sqrt{T}}{2R}R^2+\frac{R}{2R\sqrt{T}}B^2T)\\
&amp;\le \frac{BR}{\sqrt{T}}\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;라고 할 수 있다.&lt;/p&gt;
</description>
        <pubDate>Mon, 30 Sep 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/lecture/optimization/2019/09/30/OptLecture9.html</link>
        <guid isPermaLink="true">http://localhost:4000/lecture/optimization/2019/09/30/OptLecture9.html</guid>
        
        
        <category>Lecture</category>
        
        <category>Optimization</category>
        
      </item>
    
      <item>
        <title>Optimization Lecture 8</title>
        <description>&lt;h1 id=&quot;proximal-gradient&quot;&gt;Proximal Gradient&lt;/h1&gt;

&lt;h3 id=&quot;proximal-gradient-descent&quot;&gt;Proximal Gradient Descent&lt;/h3&gt;

&lt;p&gt;일반적으로 우리는 $f(x)$가 미분가능한 함수라고 생각하고 문제를 풀었지만, 사실 그렇지 않은 경우가 더 많다. 이런 경우를 잘 해결하기 위해 $f(x)$를 $g(x)$와 $h(x)$로 쪼갤 수 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(x) = g(x) + h(x)&lt;/script&gt;

&lt;p&gt;여기서 $g(x)$는 미분가능한 nice function이고, $h(x)$는 미분불가능할 수도 있지만 해석하기 쉬운 additional function 이다. $g(x)$와 $h(x)$는 둘다 convex이다.&lt;/p&gt;

&lt;p&gt;사실 gradient descent 방법의 함수 다음 $x$를 구하는 방법의 식은 다음과 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}x_{t+1} &amp;= x_t - \gamma\triangledown f(x)\\
&amp;= \arg\min_y\{f(x_t)+\triangledown f(x_t)^T(y-x_t)+\frac{1}{2\gamma}\|y-x_t\|^2\}\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;테일러 전개를 이용한 것인데, 마지막 항은 $\triangledown^2f(x)=\frac{1}{\gamma}I$로 대체한 것이다. &lt;em&gt;왜 이렇게 대체를 할 수 있는지는 모르겠다&lt;/em&gt; 다시 써보면,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}x_{t+1}&amp;=\arg\min_y\{g(y)+h(y)\}\\
&amp;=\arg\min_y\{g(x_t) + \triangledown g(x_t)^T(y-x_t)+\frac{1}{2\gamma}\|y-x_t\|^2 + h(y)\}\\
&amp;=\arg\min_y\{\triangledown g(x_t)^T(y-x_t)+\frac{1}{2\gamma}\|y-x_t\|^2 \frac{\gamma}{2}\|\triangledown g(x_t)\|^2+ h(y)\}\\
&amp;=\arg\min_y\{\frac{1}{2\gamma}(\|y-x_t\|^2 + 2\gamma\triangledown g(x_t)^T(y-x_t)+\gamma^2\|\triangledown g(x_t)\|^2)+h(y)\}\\
&amp;=\arg\min_y\{\frac{1}{2\gamma}\|y-(x_t-\gamma\triangledown g(x_t))\|^2+h(y)\}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;이다. 중간에 뜬금없이 추가되거나 삭제된 항은 $y$와 관계없는 항이기 때문에 추가가 가능하다. 여기서 &lt;script type=&quot;math/tex&quot;&gt;\text{prox}_{h, \gamma}(z) = \arg\min_y\{\frac{1}{2\gamma}\|y-z\|^2+h(y)\}&lt;/script&gt;라고 정의하면,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x_{t+1} = \text{prox}_{h, \gamma}(x_t-\gamma\triangledown g(x_t))&lt;/script&gt;

&lt;p&gt;라고 쓸 수 있다. 항상 이 방법이 좋은 것은 아니며, $f(x)$를 어떤 $g(x)$와 $h(x)$로 나누는지에 따라 효과가 달라진다.&lt;/p&gt;

&lt;h3 id=&quot;generalized-gradient&quot;&gt;Generalized Gradient&lt;/h3&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;G_{h, \gamma}(x) = \frac{1}{\gamma}(x-\text{prox}_{h, \gamma}(x-\gamma\triangledown g(x)))&lt;/script&gt;

&lt;p&gt;라고 하면, $x_{t+1}= x_t-\gamma G_{h, \gamma}(x)$라고 할 수 있다. 이 식으로 그냥 gradient descent와 projected gradient descent도 포함시킬 수 있는데, 만약 $h(x)=0$이면 그냥 gradient descent이고, &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
h(x)=\begin{cases}0 &amp; \text{if }x\in X \\ \infty &amp; \text{otherwise}\end{cases} %]]&gt;&lt;/script&gt; 라고 정의한다면 projected gradient descent이다.&lt;/p&gt;

&lt;h3 id=&quot;convergence-analysis-beta-smooth&quot;&gt;Convergence Analysis ($\beta$-smooth)&lt;/h3&gt;

&lt;p&gt;$\beta$-smooth function이므로 $\gamma=\frac{1}{\beta}$ 라고 하자. $G(x)=G_{h, \gamma}(x)$라고 하자.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}g(x-\gamma G(x))&amp;\le g(x) - \gamma\triangledown g(x)^TG(x)+\frac{\gamma^2\beta}{2}\|G(x)\|^2 \text{ (by }\beta\text{-smooth)}\\
&amp;\le g(x)-\frac{1}{\beta}\triangledown g(x)^TG(x)+\frac{1}{2\beta}\|G(x)\|^2 \text{ (by }\gamma=\frac{1}{\beta}\text{)}\\
f(x-\frac{1}{\beta}G(x))&amp;\le g(x)-\frac{1}{\beta}\triangledown g(x)^TG(x)+\frac{1}{2\beta}\|G(x)\|^2 + h(x-\frac{1}{\beta}G(x))\\
&amp;\le g(z)+\triangledown g(x)^T(x-z)-\frac{1}{\beta}\triangledown g(x)^TG(x)+\frac{1}{2\beta}\|G(x)\|^2 + h(z)+(G(x)-\triangledown g(x))^T(x-z-\frac{1}{\beta}G(x))\text{ (by Convexity)}\\
&amp; \le g(z)+h(z)+G(x)^T(x-z)-\frac{1}{2\beta}\|G(x)\|^2 \text{ (by }G(x)-\triangledown g(x)=\triangledown h(x-\frac{1}{\beta}G(x)\text{)})\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;$x=x_t, z=x^* $를 대입하면,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(x_t-\frac{1}{\beta}G(x_t))\le g(x^* )+h(x^* )+G(x_t)^T(x_t-x^* )-\frac{1}{2\beta}\|G(x_t)\|^2&lt;/script&gt;

&lt;p&gt;이고, $f(x_t-\frac{1}{\beta}G(x))=f(x_{t+1})$, $g(x^* )+h(x^* )=f(x^* )$이므로&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(x_{t+1})-f(x^* )\le G(x_t)^T(x_t-x^* )-\frac{1}{2\beta}\|G(x_t)\|^2&lt;/script&gt;

&lt;p&gt;이 된다.&lt;/p&gt;

&lt;h3 id=&quot;proof-gx-triangledown-gxtriangledown-hx-gamma-gx&quot;&gt;(Proof) &lt;script type=&quot;math/tex&quot;&gt;G(x)-\triangledown g(x)=\triangledown h(x-\gamma G(x))&lt;/script&gt;&lt;/h3&gt;

&lt;p&gt;위에서 그냥 넘어간 이 명제를 증명해보자.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}G(x) &amp;= \frac{1}{\gamma}(x-\text{prox}_{h, \gamma}(x-\gamma\triangledown g(x)))\\
&amp;=\frac{1}{\gamma}(x-\arg\min_y\{\frac{1}{2\gamma}\|y-(x-\gamma\triangledown g(x))\|^2 + h(y)\})\\
\Rightarrow x-\gamma G(x)&amp;=\arg\min_y\{\frac{1}{2\gamma}\|y-(x-\gamma\triangledown g(x))\|^2 + h(y)\}\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;$\arg\min$이므로 마지막 식의 우항을 미분해서 좌항을 넣으면 $0$이 되어야 한다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}&amp;\Rightarrow \frac{1}{\gamma}(x-\gamma G(x)-x+\gamma \triangledown g(x))+\triangledown h(x-\gamma G(x))=0\\
&amp;\Rightarrow G(x)-\triangledown g(x) = \triangledown h(x-\gamma G(x))\end{align} %]]&gt;&lt;/script&gt;
</description>
        <pubDate>Wed, 25 Sep 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/lecture/optimization/2019/09/25/OptLecture8.html</link>
        <guid isPermaLink="true">http://localhost:4000/lecture/optimization/2019/09/25/OptLecture8.html</guid>
        
        
        <category>Lecture</category>
        
        <category>Optimization</category>
        
      </item>
    
      <item>
        <title>Optimization Lecture 7</title>
        <description>&lt;h1 id=&quot;barrier-method&quot;&gt;Barrier Method&lt;/h1&gt;

&lt;h3 id=&quot;inequality-constrained-problems&quot;&gt;Inequality Constrained Problems&lt;/h3&gt;

&lt;p&gt;최적화해야 하는 함수 $f(x)$가 제한된 집합 $X$에서 정의될 때 projected gradient descent 외의 다른 방법을 소개한다. 이런 constrained optimization 문제를 다르게 말하면, ‘어떤 함수 $h, g$에 대하여 모든 $1\le i \le m$인 $i$들에 대해 $h_i(x)=0$을 만족하고, 모든 $1\le j \le r$인 $j$들에 대해 $g_j(x)\le 0$을 만족하면서 $f(x)$를 최소화시켜라’라고 표현할 수 있다. 이것을 Inequality Constrained Problem이라고 한다.&lt;/p&gt;

&lt;h3 id=&quot;barrier-method-1&quot;&gt;Barrier Method&lt;/h3&gt;

&lt;p&gt;Barrier Method란, constrained 문제를 unconstrained 문제로 바꾸는 방법을 말한다. 이 때 원래의 최적화해야 하는 함수 뒤에 penalty function을 더하게 된다. 그런데 여기서, 과연 이렇게 찾은 해가 우리가 실제로 원하던 해일까? 라는 의문을 갖게 된다.&lt;/p&gt;

&lt;h3 id=&quot;lagrange-multipliers&quot;&gt;Lagrange Multipliers&lt;/h3&gt;

&lt;p&gt;라그랑지 계수로 이를 해결할 수 있는데, 위에서 소개한 Inequality Constrained Problem을 이용하자. 먼저 Lagrange function을 다음과 같이 정의할 수 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Lambda(x, \mu, \lambda):=f(x) + \sum_{i=1}^m{\mu_ih_i(x)} + \sum_{j=1}^r{\lambda_jg_j(x)}&lt;/script&gt;

&lt;p&gt;여기서 $\mu_1, \cdots, \mu_m, \lambda_1, \cdots, \lambda_r$을 Lagrange multiplier라고 한다.&lt;/p&gt;

&lt;h3 id=&quot;lagrange-dual&quot;&gt;Lagrange Dual&lt;/h3&gt;

&lt;p&gt;다음은 Lagrange function을 최적화하는 방법이다. 먼저 Lagrange dual function과 Lagrange dual problem을 정의한다.&lt;/p&gt;

&lt;p&gt;Lagrange dual function :&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L(\mu, \lambda) := \min_x\Lambda(x, \mu, \lambda)&lt;/script&gt;

&lt;p&gt;Lagrange dual problem :&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\max_{\mu, \lambda}L(\mu, \lambda) \text{ s.t. }\lambda_j\ge, \forall 1 \le j\le r&lt;/script&gt;

&lt;p&gt;먼저 Lagrange dual function 값을 구한 뒤, Lagrange dual problem을 푼다. 그 후 $L(\mu, \lambda)$를 최대화시키는 $\lambda$와 $\mu$를 찾아서 $\Lambda(x, \mu, \lambda)$에 넣고 다시 Lagrange dual function을 풀고 $\cdots$를 반복한다. 최종 나오는 $x$값이 우리가 원하던 $x^* $값이다.&lt;/p&gt;

&lt;p&gt;Lagrange dual problem을 푸는 과정에서는, $L(\mu, \lambda)$가 concave function이라는 점을 활용하면 gradient ascent를 이용해서 maximization을 할 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;proof-lmu-lambda-is-convex&quot;&gt;(Proof) $L(\mu, \lambda)$ is convex&lt;/h3&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}&amp;L(\alpha \mu^{(1)}+(1-\alpha)\mu^{(2)}, \alpha\lambda^{(1)}+(1-\alpha)\lambda^{(2)})\\
&amp;=\min_x(f(x)+\sum(\alpha\mu^{(1)}+(1-\alpha)\mu^{(2)})h_i(x) + \sum(\alpha\lambda^{(1)}+(1-\alpha)\lambda^{(2)})g_j(x))\\
&amp;\ge \alpha(\min(f(x)+\sum\mu^{(1)}h_i(x))+\sum\lambda^{(1)}g_j(x))+(1-\alpha)(\min(f(x)+\sum\mu^{(2)}h_i(x))+\sum\lambda^{(2)}g_j(x))\\
&amp;=\alpha L(\mu^{(1)}, \lambda^{(1)})+(1-\alpha)L(\mu^{(2)}, \lambda^{(2)})\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;따라서 $L(\mu, \lambda)$는 concave 함수이다.&lt;/p&gt;

&lt;h3 id=&quot;lagrange-dual-and-barrier&quot;&gt;Lagrange Dual and Barrier&lt;/h3&gt;

&lt;p&gt;다시 Barrier로 돌아오면, Lagrange dual에서 penalty function은 $\sum\mu_ih_i(x)+\sum\lambda_jg_j(x)$이다. Lagrange dual problem에서 우리는 $L(\mu, \lambda)$를 최대화시켜야 하므로,&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;$g_j(x)\lt 0$이면 $\lambda_j^* =0$이어야 하고,&lt;/li&gt;
  &lt;li&gt;$g_j(x) \gt 0$이면 $\lambda_j^* = \infty$,&lt;/li&gt;
  &lt;li&gt;$g_j(x)=0$이면 $\lambda_j^* $는 아무 숫자나 되어도 상관없으므로 $\lambda_j^* \gt 0$이게 된다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;barrier의 관점에서 봐도 말이 된다. barrier를 만족하지 못하면 penalty function이 무한대로 가게 되므로, 해가 constrained set 안으로 무조건 들어가도록 하게 된다. 따라서, Lagrange dual로 구한 $x^* $는 우리가 원하던 최적 해가 맞다.&lt;/p&gt;

&lt;p&gt;이렇게 구한 해인 $x^* (\mu^* , \lambda^* )$에는 몇 가지의 특성이 있다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;$\triangledown f(x^* (\mu^* , \lambda^* ))+\sum\mu_i^* \triangledown h_i(x^* (\mu^* , \lambda^* ))+\sum\lambda_j^* \triangledown g_j(x^* (\mu^* , \lambda^* ))=0$&lt;/li&gt;
  &lt;li&gt;$h_i(x^* (\mu^* , \lambda^* ))=0, \forall i$&lt;/li&gt;
  &lt;li&gt;$\lambda_j^* g_j(x^* (\mu^* , \lambda^* ))=0 \text{ when }\lambda_j^* =0, g_j(x^* (\mu^* , \lambda^* ))\lt 0$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;$f(x)$나 constrained set이 convex가 아니면 진짜 해와 우리가 구한 해가 차이가 나게 되는데, 이것을 Lagrange Gap이라고 한다.&lt;/p&gt;
</description>
        <pubDate>Mon, 23 Sep 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/lecture/optimization/2019/09/23/OptLecture7.html</link>
        <guid isPermaLink="true">http://localhost:4000/lecture/optimization/2019/09/23/OptLecture7.html</guid>
        
        
        <category>Lecture</category>
        
        <category>Optimization</category>
        
      </item>
    
      <item>
        <title>Optimization Lecture 6</title>
        <description>&lt;h1 id=&quot;projected-gradient-descent&quot;&gt;Projected Gradient Descent&lt;/h1&gt;

&lt;h2 id=&quot;projected-gradient--alpha-strongly-convex--beta-smooth&quot;&gt;Projected Gradient : $\alpha$-strongly convex &amp;amp; $\beta$-smooth&lt;/h2&gt;

&lt;h3 id=&quot;recall-unconstrained-vanilla-analysis&quot;&gt;(Recall) Unconstrained Vanilla Analysis&lt;/h3&gt;

&lt;p&gt;vanilla analysis에서는&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}\|x_{t+1}-x^* \|^2&amp;\le\frac{2}{\beta}(f(x^* )-f(x_t))+\frac{1}{\beta}\|\triangledown f(x_t)\|^2+(1-\frac{\alpha}{beta})\|x_t-x^* \|^2\\
&amp;\le(1-\frac{\alpha}{\beta})\|x_t-x^* \|^2\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;을 얻을 수 있었다. 이 때 마지막 부등식은&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(x^* )-f(x_t)\le f(x_{t+1})-f(x_t)\le-\frac{1}{2\beta}\|\triangledown f(x_t)\|^2\cdots(* 1)&lt;/script&gt;

&lt;p&gt;라는 성질에 의해&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{2}{\beta}(f(x^* )-f(x_t))\le -\frac{1}{\beta^2}\|\triangledown f(x_t)\|^2&lt;/script&gt;

&lt;p&gt;를 얻어서 성립한 것이다.&lt;/p&gt;

&lt;h3 id=&quot;constrained-vanilla-analyasis&quot;&gt;Constrained Vanilla Analyasis&lt;/h3&gt;

&lt;p&gt;constrained에서는 $(* 1)$가 아닌&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(x^* )-f(x_t)\le f(x_{t+1})- f(x_t)\le-\frac{1}{2\beta}\|\triangledown f(x_t)\|^2+\frac{\beta}{2}\|y_{t+1}-x_{t+1}\|^2\cdots(* 2)&lt;/script&gt;

&lt;p&gt;가 성립한다. 따라서,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}\|x_{t+1}-x^* \|^2&amp;\le\frac{2}{\beta}(f(x^* )-f(x_t))+\frac{1}{\beta^2}\|\triangledown f(x_t)\|^2-\|y_{t+1}-x_{t+1}\|^2+(1-\frac{\alpha}{\beta})\|x_t-x^* \|^2\\
&amp;\le(1-\frac{\alpha}{\beta})\|x_t-x^* \|^2(\text{by }(* 2))\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;가 성립하므로,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}\|x_T-x^* \|^2&amp;\le(1-\frac{\alpha}{\beta})^T\|x_0-x^* \|^2\\
\|x_T-x^* \|&amp;\le(1-\frac{\alpha}{\beta})^{\frac{T}{2}}\|x_0-x^* \|\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;이다. 따라서&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}f(x_T)-f(x^* )&amp;\le\triangledown f(x^* )^T(x_T-x^* )+\frac{\beta}{2}\|x_T-x^* \|^2\\
&amp;\le\|\triangledown f(x^* )\|\|x_T-x^* \|+\frac{\beta}{2}\|x_T-x^* \|^2\\
&amp;\le\|\triangledown f(x^* )\|(1-\frac{\alpha}{\beta})^{\frac{T}{2}}\|x_0-x^* \|+\frac{\beta}{2}(1-\frac{\alpha}{\beta})^T\|x_0-x^* \|^2\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;결론적으로, Projected Gradient Descent를 사용하면 Unconstrained에서와 거의 비슷하게 수렴하고 bound하게 된다. 하지만 Projection에 많은 계산량이 사용되기 때문에 이 방법은 거의 쓰지 않는다.&lt;/p&gt;
</description>
        <pubDate>Wed, 18 Sep 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/lecture/optimization/2019/09/18/OptLecture6.html</link>
        <guid isPermaLink="true">http://localhost:4000/lecture/optimization/2019/09/18/OptLecture6.html</guid>
        
        
        <category>Lecture</category>
        
        <category>Optimization</category>
        
      </item>
    
  </channel>
</rss>
