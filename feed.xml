<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>YeonjeeJung's Blog</title>
    <description></description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Tue, 03 Dec 2019 22:59:53 +0900</pubDate>
    <lastBuildDate>Tue, 03 Dec 2019 22:59:53 +0900</lastBuildDate>
    <generator>Jekyll v3.8.6</generator>
    
      <item>
        <title>A Closer Look at Deep Learning Heuristics - Learning Rate Restarts, Warmup and Distillation</title>
        <description>&lt;h2 id=&quot;abstract&quot;&gt;[Abstract]&lt;/h2&gt;

&lt;p&gt;현재의 딥러닝 모델들은 heuristic들로부터 좋은 결과를 냈다. 현재 이 경험적인 방법들은 linear interpolation이나 차원 감소를 통한 시각화들로 분석되는데, 이들은 각자의 단점을 가지고 있다. 이 논문에서는 mode connectivity, Canonical correlation analysis(CCA)를 가지고 knowledge distillation과 cosine restart, warmup을 재분석한다. 분석으로 얻은 결과는 다음과 같다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;cosine annealing이 잘 작동되는 이유로 설명된 것들이 실험적으로 아님을 보였다.&lt;/li&gt;
  &lt;li&gt;warmup은 깊은 층의 가중치 변화를 하지 않게 만들며, 그냥 그 가중치들을 고정시켜도 warmup과 같은 효과를 얻는다.&lt;/li&gt;
  &lt;li&gt;knowledge distillation에서 teacher가 공유한 latent knowledge는 더 깊은 층으로 분배된다.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;1-introduction&quot;&gt;[1] Introduction&lt;/h2&gt;

&lt;p&gt;knowledge distillation은 teacher model을 먼저 학습하고, 더 작은 데이터셋을 가지고 student model을 학습하는데, loss function을 사용하는 것이 아니고 teacher를 따라하는 방식으로 학습한다.&lt;/p&gt;

&lt;p&gt;Mode connectivity(MC)를 이용하면 네트워크의 local minima들을 piecewise-linear curve로 연결할 수 있는데, 이는 서로 다른 방법으로 얻은 local minima와 특성들이 연결되어 있음을 알 수 있다.&lt;/p&gt;

&lt;p&gt;CCA를 전처리 단계에서 사용하여 네트워크의 활성화를 분석할 수 있다는 연구도 있었다.&lt;/p&gt;

&lt;h2 id=&quot;2-empirical-tools&quot;&gt;[2] Empirical Tools&lt;/h2&gt;

&lt;h3 id=&quot;21-mode-connectivity&quot;&gt;[2.1] Mode Connectivity&lt;/h3&gt;

&lt;p&gt;local minima들을 연결하면, 그들을 연결한 사이에도 비교적 작은 loss의 점들이 있다.&lt;/p&gt;

&lt;h4 id=&quot;211-resilience-of-mode-connectivity&quot;&gt;[2.1.1] Resilience of Mode Connectivity&lt;/h4&gt;

&lt;p&gt;보통은 서로 다른 훈련 방법(batch size가 다르거나, optimizer가 다르거나, 초기화가 다르거나, 정규화 텀이 다르거나)을 사용하면 서로 다른 점으로 수렴한다고 알고 있다. 이 논문에서는 기준 mode $G$를포함해, 서로 다른 훈련 방법을 사용하여 ${A, B, C, D, E, F, G}$의 mode를 얻었다. 그리고 mode connectivity 알고리즘을 사용하여 $A$부터 $F$까지를 $G$와 짝지어 각각의 curve를 만들어 내었다. 그리고 그 curve의 점마다의 validation accuracy를 봤는데, 마지막 epoch에서는 다들 비슷한 정확도를 보였다. 따라서 결국 각 minima를 잇는 선들도 정확도가 비슷하다.&lt;/p&gt;

&lt;h3 id=&quot;22-cca-for-measuring-representational-similarity&quot;&gt;[2.2] CCA for Measuring Representational Similarity&lt;/h3&gt;

&lt;p&gt;CCA는 다변량 통계에서의 두 RV 집합간의 관계를 연구하는 전통적인 툴이다. 이전 연구에서는 CCA를 SVD나 DFT와 함께 이용해서 두 layer를 비교하는 방법이 제안되었다. 각 layer의 activation에서 나오는 값을 행렬로 만들고, 그들과 SVD를 이용해 layer간의 correlation을 계산한다. convolution layer인 경우에는 DFT를 사용한다. 이 방법은 서로 다른 네트워크를 비교할 뿐만 아니라 한 네트워크가 훈련 중 어떻게 변화하는지를 보고싶을 때 사용하기도 한다.&lt;/p&gt;

&lt;h2 id=&quot;3-stochastic-gradient-descent-with-restarts-sgdr&quot;&gt;[3] Stochastic Gradient Descent with Restarts (SGDR)&lt;/h2&gt;

&lt;p&gt;cosine annealing이 잘 되는 이유는 잘 모르는데, multi-modal 함수에서는 restart가 다른 지역을 더 탐색할 수 있게 해준다는 것이 하나의 설명이다. 실험 결과로는, restart가 일어나기 직전 minima들을 line segment로 연결한 점들은 두 점보다 높은 loss를 보였지만, learning rate를 줄인 minima들은 line segment로 연결해도 loss가 증가하지 않았다. 결과를 보니, 사실 SGDR이 local minima를 탈출하게 해준다는 결과는 보이지 않았다. 그리고 training loss와 validation loss가 달라서, train loss 입장에서는 loss가 높아져도, validation loss의 입장에서는 낮아져서 일반화가 잘 될 때도 있다.&lt;/p&gt;

&lt;h2 id=&quot;4-warmup-learning-rate-scheme&quot;&gt;[4] Warmup Learning rate Scheme&lt;/h2&gt;

&lt;p&gt;이론적으로, SGD의 learning dynamics는 batch size와 learning rate의 비율에 의존한다. warmup은 학습 불안정성을 야기하지 않고 큰 learning rate를 사용할 수 있는 방법으로 제안되었다. 이 논문에서 연구하고자 한 것은 “어떻게 learning rate warmup이 서로 다른 layer에 영향을 미칠까?”이다. 이것을 연구하기 위해 CCA를 사용하였고, 세 가지의 scheme을 사용한 네트워크들의 차이점과 유사점을 조사하였다. LB+warmup과 SB에서 유사점이 나타났는데, 이것은 큰 batch size와 큰 learning rate을 사용할 때, warmup이 불안정한 큰 변화를 없애준다는 것을 시사한다. 이것을 더 알아보기 위해 warmup을 사용하지 않고 대신 처음 20 epoch동안 fully connected layer들을 고정시켜 봤는데, 또한 유사도가 높았고 validation 정확도가 warmup을 사용한 것보다 더 높았다.&lt;/p&gt;

&lt;h2 id=&quot;5-knowledge-distillation&quot;&gt;[5] Knowledge Distillation&lt;/h2&gt;

&lt;p&gt;knowledge distillation 또한 CCA를 사용하여 연구하였다. 이전에는 dark knowledge의 전이가, 최근에는 중요도의 해석이 knowledge distillation을 잘 되게 한다고 설명되어 왔다. 이 논문에서는 knowledge transfer가 네트워크의 특정 부분에 국한되는지, student and teacher 모델의 layer사이의 유사도와 student가 이 질문에 대한 해답을 줄 수 있는지에 대해 조사하였다. student model을 학습하고, 같은 구조를 가진 모델을 hard label을 가지고 학습하였다. 그리고 그들의 layer간의 유사도를 teacher model과 비교하였다. 결과는, 깊은 층에 비해 얕은 층에 대해서는 base model과 teacher간의 유사도와 student와 teacher간의 유사도가 비슷했다. 따라서 깊은 층에 대해서만 knowledge transfer가 일어난다고 할 수 있다. &lt;em&gt;얕은 층에서도 knowledge transfer가 일어나지만, 그게 알아내기 쉬운 knowledge라서 transfer가 되지 않은 것처럼 보이는 것은 아닐까?&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;6-discussion-and-conclusion&quot;&gt;[6] Discussion and Conclusion&lt;/h2&gt;

&lt;p&gt;이 연구는 learning heuristic을 설명했고, 또한 우연히 새로운 heuristic을 발견하였다.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;처음부터 끝까지 실험, 실험, 실험…. 논문은 이렇게 쓰는거구나&lt;/p&gt;
</description>
        <pubDate>Sun, 01 Dec 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/thesis/2019/12/01/HEUR.html</link>
        <guid isPermaLink="true">http://localhost:4000/thesis/2019/12/01/HEUR.html</guid>
        
        <category>Optimization</category>
        
        
        <category>Thesis</category>
        
      </item>
    
      <item>
        <title>Don't Decay the Learning Rate, Increase the Batch Size</title>
        <description>&lt;h2 id=&quot;abstract&quot;&gt;[Abstract]&lt;/h2&gt;

&lt;p&gt;이 논문에서는 learning rate를 줄이는 것 대신 batch size를 훈련 동안 증가시키면서 원래와 비슷한 learning curve를 얻는 방법에 대해 소개한다. 이 방법은 SGD, SGD with momentum, Nesterov momentum, and Adam. 같은 epoch 수를 사용하면 같은 테스트 정확도가 나오지만, 파라미터 업데이트를 덜 하면 더 병렬화가 가능하고 학습 시간이 덜 걸린다. 따라서 learning rate $\epsilon$을 크게 하고, 그에 비례하게 batch size $B$도 늘일 수 있다. 결론적으로 momentum coefficient $m$을 늘리고 $B\propto\frac{1}{1-m}$로 batch size를 증가시킬 수 있다. 그러나 이것은 약간의 테스트 정확도 손실을 초래한다. 이 논문의 방법은 30분 안에 ImageNet을 ResNet-50을 이용해 76.1%의 검증 정확도가 나오게 학습했다.&lt;/p&gt;

&lt;h2 id=&quot;1-introduction&quot;&gt;[1] Introduction&lt;/h2&gt;

&lt;h2 id=&quot;2-stochastic-gradient-descent-and-convex-optimization&quot;&gt;[2] Stochastic Gradient Descent and Convex Optimization&lt;/h2&gt;

&lt;p&gt;SGD에는 noise가 들어가기 때문에 $\frac{dw}{dt}=\frac{dC}{dw}+\eta(t)$로 모델링된다. $\eta{(t)}$가 Gaussian random noise이다. noise scale $g=\epsilon(\frac{N}{B}-1)$로 정의되는데, 이는 학습동안의 noise를 제어한다. learning rate $\epsilon$을 줄이면 이 noise scale을 줄일 수 있는데, 줄이지 않고 $B$를 늘려도 같은 효과를 얻을 수 있다. 이 논문의 방법은 $B$를 $\frac{N}{10}$까지 증가시킨 이후에 $\epsilon$을 줄이는 방법을 사용한다.&lt;/p&gt;

&lt;h2 id=&quot;3-simulated-annealing-and-the-generalization-gap&quot;&gt;[3] Simulated Annealing and the Generalization GAP&lt;/h2&gt;

&lt;p&gt;simulated annealing은 처음 noise는 넓은 범위를 탐색할 수 있게 해주고, 좋은 지점에 다다르면 더 깊숙히 들어갈 수 있게 한다. 아마 이것이 SGD의 learning rate decay가 잘 되는 이유일 것이다.&lt;/p&gt;

&lt;h2 id=&quot;4-the-effective-learning-rate-and-the-accumulation-variable&quot;&gt;[4] The Effective Learning rate and the Accumulation Variable&lt;/h2&gt;

&lt;p&gt;최근 연구자들은 그냥 SGD대신 momentum을 사용한 SGD를 많이 사용한다. momentum 계수 $m$이 $0$으로 가면 noise scale이 작아진다. 한 연구에서 제안된 learning rate를 늘리고 동시에 $B\propto \frac{1}{1-m}$를 늘이는 것은 효과가 있었지만, momentum coefficient를 늘리는 것은 테스트 정확도의 손실을 가져왔다. 사실 더 큰 $m$에서 학습하려면 epoch를 더 추가해야 한다. 또한 $m$이 커지면 이전 gradient를 잊어버리는 시간이 더 늘어나는데, 이것이 noise scale이 줄어드는 데에 문제를 가져올 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;5-experiments&quot;&gt;[5] Experiments&lt;/h2&gt;

&lt;h3 id=&quot;51-simulated-annealing-in-a-wide-resnet&quot;&gt;[5.1] Simulated Annealing in a Wide ResNet&lt;/h3&gt;

&lt;p&gt;CIFAR-10과 “16-4” wide ResNet을 사용하였다. learning rate를 줄이는 것과 batch size를 늘리는 방법이 같다는 것을 보이기 위해, learning rate을 줄이는 방법, batch size를 늘리는 방법, hybrid 방법을 비교하였다. 결과는 세 learning curve가 거의 똑같았다. 따라서 learning rate 자체가 줄어들어야 하는 대상이 아니라, noise scale이 줄어들어야 한다는 것을 알았다. 게다가, batch size를 늘리는 방법이 파라미터 업데이트를 훨씬 덜 하고 같은 정확도를 얻을 수 있었다. 또한 다른 optimizer에도 실험하였는데 같은 결과를 얻을 수 있었다.&lt;/p&gt;

&lt;h3 id=&quot;52-increasing-the-effective-learning-rate&quot;&gt;[5.2] Increasing the Effective Learning rate&lt;/h3&gt;

&lt;p&gt;이 논문의 두번째 목표는 파라미터 업데이트를 줄이는 것이다. momentum coefficient $m$을 크게 하는 방법이 파라미터 업데이트 수가 가장 적었고, 초반 learning rate를 크게 하는 방법이 그 다음으로 파라미터 업데이트 수가 적었다. &lt;em&gt;하지만 [4]에서 $m$을 키우면 epoch가 더 추가되어야 한다고 했는데, 그럼 시간은 더 걸리는게 아닌가?&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;53-training-imagenet-in-2500-parameter-updates&quot;&gt;[5.3] Training ImageNet in 2500 Parameter Updates&lt;/h3&gt;

&lt;p&gt;이 실험은 Training ImageNet in 1 Hour에서의 세팅을 따라했지만, warm-up phase는 적용하지 않았다. 또한 ResNet 대신 더 강력한 Inception-ResNet-V2를 사용했다. 이전 논문에서 이미 learning rate를 maximum까지 높였기 때문에 이 논문에서는 파라미터 업데이트를 줄이기 위해 $m$을 크게 했다. 결과는 예상대로 더 큰 $m$을 사용할 때의 파라미터 업데이트가 더 줄었다.&lt;/p&gt;

&lt;h3 id=&quot;54-training-imagenet-in-30-minutes&quot;&gt;[5.4] Training ImageNet in 30 Minutes&lt;/h3&gt;

&lt;p&gt;후반 epoch로 갈수록 파라미터 업데이트가 적기 때문에 시간이 더 적게 걸렸다. 초반 30 epoch와 후반 60 epoch가 각각 15분 이내로 걸렸다.&lt;/p&gt;

&lt;h2 id=&quot;7-conclusions&quot;&gt;[7] Conclusions&lt;/h2&gt;

&lt;p&gt;learning rate를 줄이는 방식과 같은 효과를 batch size를 늘리면서도 낼 수 있다.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;실험을 굉장히 많이 하고 체계적으로 잘 설계해서 한 논문인듯. 이 논문의 related works 부분에 지금까지 읽었던 모든 논문들이 다 등장.. 새로운 것을 만들어 내는 것 뿐만 아니라 원래 있던 것을 개선하는 것, 더 효율적인 방향으로 가게 하는 것도 논문이 될 수 있다.&lt;/p&gt;
</description>
        <pubDate>Thu, 28 Nov 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/thesis/2019/11/28/INCSIZE.html</link>
        <guid isPermaLink="true">http://localhost:4000/thesis/2019/11/28/INCSIZE.html</guid>
        
        <category>Optimization</category>
        
        
        <category>Thesis</category>
        
      </item>
    
      <item>
        <title>Accurate, Large Minibatch SGD - Training ImageNet in 1 Hour</title>
        <description>&lt;h2 id=&quot;abstract&quot;&gt;[Abstract]&lt;/h2&gt;

&lt;p&gt;딥러닝에서 네트워크와 데이터셋이 커지면 학습시간이 늘어나게 되어서 분산병렬 SGD를 사용한다. 이 논문에서는 미니배치 크기가 크면 최적화하는 데에 어려움을 야기하지만, 이렇게 훈련된 네트워크는 일반화를 잘 한다는 것을 보여준다. 또한 한번에 미니배치 사이즈를 8192개까지 늘려도 정확도에는 변함이 없다는 것을 보여준다. 이 결과를 얻기 위해 learning rate를 정하는 규칙을 적용했고, 새로운 warmup 방법을 만들었다.&lt;/p&gt;

&lt;h2 id=&quot;2-large-minibatch-sgd&quot;&gt;[2] Large Minibatch SGD&lt;/h2&gt;

&lt;h3 id=&quot;21-learning-rates-for-large-minibatches&quot;&gt;[2.1] Learning Rates for Large Minibatches&lt;/h3&gt;

&lt;p&gt;이 논문의 목적은 정확도는 유지하면서 큰 미니배치 사이즈를 사용하는 것이다. 이들은 Linear Scaling Rule이라고 불리는, 미니배치 크기가 $k$배 커지면 learning rate도 $k$배 크게 해야 한다는 규칙을 발견했다. 작은 미니배치를 사용했을 때의 $w_{t+k}$와, $k$배의 미니배치를 사용했을 때의 $\hat{w}_{t+1}$이 비슷해지려면 $\hat{\eta}=k\eta$가 되어야 한다. 이렇게 학습시켰을 때, 둘의 정확도와 학습곡선이 비슷하다. 그러나 학습 초기에는 비슷하지 않을 수 있고, 미니배치 사이즈는 계속해서 커질 수 없다는 성질이 있다. 이 논문은 전례 없는 미니배치 사이즈를 가지고 실험적으로 Discussion에 있는 이론들을 테스트해본다.&lt;/p&gt;

&lt;h3 id=&quot;22-warmup&quot;&gt;[2.2] Warmup&lt;/h3&gt;

&lt;p&gt;constant warmup은 학습 초기 epoch에서 constant learning rate를 사용한다. 그러나 이 논문에서는 충분하지 않아서 gradual warmup을 사용했는데, 훈련 초반에 잘 수렴하도록 해준다.&lt;/p&gt;

&lt;h3 id=&quot;23-batch-normalization-with-large-minibatches&quot;&gt;[2.3] Batch Normalization with Large Minibatches&lt;/h3&gt;

&lt;p&gt;BN을 사용하면 원래 집합 $X$안에 있는 크기가 $n$인 모든 부분집합들을 모두 포함하고 있는 것처럼 생각할 수 있다. 분산 컴퓨팅 환경에서 BN을 사용한다면 per-worker sample size가 $n$이고 total minibatch size가 $kn$일 때, $X^n$에서 골라낸 독립된 $k$개의 $\cal{B}_ j$로 볼 수 있다. &lt;em&gt;이게 결국 각 minibatch의 분산을 비슷하게 만들어준다는 수업때 하려던 내용인듯&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;3-subtleties-and-pitfalls-of-distributed-sgd&quot;&gt;[3] Subtleties and Pitfalls of Distributed SGD&lt;/h2&gt;

&lt;h3 id=&quot;weight-decay&quot;&gt;Weight decay&lt;/h3&gt;

&lt;p&gt;L2 regularization때문에 weight decay를 사용하는데, 이 항이 원래의 loss에 더해진다. 따라서 원래 loss를 scaling 하는 것과 learning rate를 scaling하는 것은 다르다.&lt;/p&gt;

&lt;h3 id=&quot;momentum-correction&quot;&gt;Momentum correction&lt;/h3&gt;

&lt;p&gt;momentum을 사용한 SGD가 최근 많이 이용되고 있는데, learning rate가 바뀌면 momentum correction term을 적용해야 한다. &lt;em&gt;이게 왜 문제점인지 잘 모르겠음&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;gradient-aggregation&quot;&gt;Gradient aggregation&lt;/h3&gt;

&lt;p&gt;전체 loss를 계산할 때 평균을 사용하는데, 각 worker의 loss를 $n$으로 나누면 다시 한번 전체를 $k$로 나누어야 하기 때문에, 그냥 처음부터 각 worker의 loss를 $kn$으로 나눠서 최종적으로는 다 더하기만 한다.&lt;/p&gt;

&lt;h3 id=&quot;data-shuffling&quot;&gt;Data shuffling&lt;/h3&gt;

&lt;p&gt;한 epoch당 데이터 전체를 $k$ worker에게 분배하는데, 모든 데이터들은 반드시 한번씩만 사용되도록 분배한다.&lt;/p&gt;

&lt;h2 id=&quot;4-communication&quot;&gt;[4] Communication&lt;/h2&gt;

&lt;p&gt;gradient aggregation은 backprop과 병렬로 처리되어야 한다. 한 layer의 gradient가 계산되면, 모든 worker에게서 정보를 받아서 합치고, 그와 동시에 다음 layer의 gradient가 계산될 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;41-gradient-aggregation&quot;&gt;[4.1] Gradient Aggregation&lt;/h3&gt;

&lt;p&gt;gradient aggregation에는 allreduce 연산이 사용된다. 이 논문에서의 구현은 세 부분으로 되어 있다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;8개의 GPU에 있는 버퍼들을 각 서버에서 하나로 합친다.&lt;/li&gt;
  &lt;li&gt;결과 버퍼가 공유되고 합쳐진다.&lt;/li&gt;
  &lt;li&gt;결과가 각 GPU로 퍼진다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;1번과 3번은 NCCL을 이용하였다. 2번에 대해서는 recursive halving and doubling 알고리즘과 bucket 알고리즘을 구현했는데, 큰 사이즈의 요소에 대해서는 recursive halving and doubling 알고리즘이 더 좋았다.&lt;/p&gt;

&lt;p&gt;having/doubling 알고리즘은 reduce scatter collective와 allgather 단계로 구성된다. reduce scatter에서는 서버가 2개씩 짝을 지어, 1번 서버의 경우 자신의 버퍼의 뒤쪽 반을 보내고 2번 서버의 앞쪽 반을 받는다. 따라서 2의 승수개의 서버가 필요하다. 수신 및 전송하는 데이터가 reduction 되는 동안 목적지까지의 거리는 2배가 된다. 이 단계 이후에는 각 서버에는 최종 reduction된 데이터의 일부가 있다. allgather 단계에서는 통신을 역추적해서 각 서버에 있는 벡터들을 모은다. 각 서버에서 처음에 보내는 부분이 allgather에서 수신되고 있고, 그 이후에는 받았던 부분을 보낸다.&lt;/p&gt;

&lt;h3 id=&quot;42-software&quot;&gt;[4.2] Software&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;이부분이 사실 메인인 것 같은데.. 이해가 안감ㅠㅠ&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;43-hardware&quot;&gt;[4.3] Hardware&lt;/h3&gt;

&lt;p&gt;이 논문에서는 50Gbit 네트워크가 ResNet-50을 학습시키는 데에 충분하다고 한다. 게다가 이 최대 bandwidth는 backprop에만 사용되고, forward pass에는 그만큼 쓰이지 않는다.&lt;/p&gt;

&lt;h2 id=&quot;5-main-results-and-analysis&quot;&gt;[5] Main Results and Analysis&lt;/h2&gt;

&lt;p&gt;이 논문의 메인 결과는 ImageNet에 ResNet-50과 256worker를 이용해 정확도 손실 없이 한시간만에 학습시켰다는 것이다. Linear scaling rule과 warmup 전략을 사용해서 이가 가능했다.&lt;/p&gt;

&lt;h3 id=&quot;52-optimization-or-generalization-issues&quot;&gt;[5.2] Optimization or Generalization Issues?&lt;/h3&gt;

&lt;p&gt;이 논문에서는 no warmup, constant warmup, gradual warmup을 실험했는데, training error와 validation error 면에서 모두 gradual warmup이 가장 성능이 좋았다. 또한 baseline의 learning curve도 초반 빼고는 잘 따라갔다.&lt;/p&gt;

&lt;h3 id=&quot;53-analysis-experiments&quot;&gt;[5.3] Analysis Experiments&lt;/h3&gt;

&lt;p&gt;minibatch size가 8k보다 작거나 같을 때에는 learning curve가 거의 비슷했는데, 그 이상에서는 비슷하지도 않았다. 여기서 알 수 있는 것은, learning curve를 비교해 보면 training이 끝나기 전에 결과가 잘 나올지, 아닐지 알 수 있다는 점이다.&lt;/p&gt;

&lt;p&gt;minibatch size가 작았을 때는 $\eta=0.1$언저리에서는 비슷한 성능이 나왔지만, size가 커질수록 $\eta$를 더 섬세하게 바꿔야 한다.&lt;/p&gt;

&lt;p&gt;BN에서의 일반적인 초기화는 $\gamma=1$인데, 이 논문에서는 각 residual block에 대해서만 $\gamma=0$으로 초기화했더니 더 좋은 결과가 나왔다.&lt;/p&gt;

&lt;p&gt;ImageNet-5k dataset에 대해서도 실험해봤는데, 정확도가 작은 minibatch size와 정확도가 비슷한 minibatch size는 8k까지로, 데이터셋의 크기와 minibatch size의 관계는 없는 것으로 관찰되었다.&lt;/p&gt;

&lt;h3 id=&quot;54-generalization-to-detection-and-segmentation&quot;&gt;[5.4] Generalization to Detection and Segmentation&lt;/h3&gt;

&lt;p&gt;ImageNet을 학습시킨 ResNet-50을 R-CNN의 초기화에 이용해보았는데, $8k$까지는 성능이 나빠지지 않았다. linear scaling rule과 large batchsize는 다른 모델에서도 잘 적용된다는 것을 보여준다.&lt;/p&gt;

&lt;h3 id=&quot;55-run-time&quot;&gt;[5.5] Run Time&lt;/h3&gt;

&lt;p&gt;batch size를 늘리면 GPU간의 소통에도 불구하고 iteration 당 시간은 비슷하지만, epoch당 걸리는 시간이 선형적으로 줄어든다. 또한 GPU가 k배 늘어나면 초당 처리되는 이미지 수도 k배 늘어나는 것이 이상적인데, 이 논문의 방법은 GPU간의 소통에도 불구하고 이상의 $90%$정도를 달성했다.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;allreduce operation&lt;/p&gt;
</description>
        <pubDate>Tue, 26 Nov 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/thesis/2019/11/26/IMG1HOUR.html</link>
        <guid isPermaLink="true">http://localhost:4000/thesis/2019/11/26/IMG1HOUR.html</guid>
        
        <category>Optimization</category>
        
        
        <category>Thesis</category>
        
      </item>
    
      <item>
        <title>SAGA - A Fast Incremental Gradient Method With Support for Non-Strongly Convex Composite Objectives</title>
        <description>&lt;hr /&gt;

&lt;h2 id=&quot;abstract&quot;&gt;[Abstract]&lt;/h2&gt;

&lt;p&gt;SAGA는 SAG와 SVRG의 뒤를 잇는데, 더 좋은 수렴도를 갖는다. SDCA와는 다르게 strongly convex가 아닌 문제도 바로 풀 수 있고, 문제의 본질적인 strong convexity에 적응할 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;1-introduction&quot;&gt;[1] Introduction&lt;/h2&gt;

&lt;p&gt;함수 $f(x)$를 최소화하고 싶은데, $f(x)$는&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(x)=\frac{1}{n}\sum_{i=1}^nf_i(x)&lt;/script&gt;

&lt;p&gt;이렇게 생겼다. 각 $f_i$는 convex하고 gradient가 $L$-Lipschitz continuous하다. 이 논문에서는 $f_i$들이 $\mu$-strongly convex한 경우와 $F(x)=f(x)+h(x)$인 경우(proximal gradient descent에서 봤던 모양)도 다룰 것이다.&lt;/p&gt;

&lt;h2 id=&quot;2-saga-algorithm&quot;&gt;[2] SAGA Algorithm&lt;/h2&gt;

&lt;p&gt;SAGA 알고리즘은 다음과 같다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;$j$를 랜덤으로 뽑는다.&lt;/li&gt;
  &lt;li&gt;$\phi_j^{k+1}=x^k$라고 하고, $f’_ j(\phi_j^{k+1})$를 테이블에 저장한다. 즉, 모든 $f$중 하나의 $f_j$의 gradient만을 구한다.&lt;/li&gt;
  &lt;li&gt;$x$를 $f’_ j(\phi_j^{k+1}), f’_ j(\phi_j^k)$과 테이블에 있는 평균을 이용해 업데이트한다. &lt;script type=&quot;math/tex&quot;&gt;w^{k+1}=x^k-\gamma\left[f'_ j(\phi_j^{k+1})-f'_ j(\phi_j^k)+\frac{1}{n}\sum_{i=1}^nf'_ i(\phi_i^k)\right]&lt;/script&gt;이고, &lt;script type=&quot;math/tex&quot;&gt;x^{k+1}=\text{prox}_\gamma^h(w^{k+1})&lt;/script&gt;이다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;strongly convex일 때와 아닐 때의 수렴도는 각각 증명되어 있다. strongly convex가 아닌 경우에 $\gamma=\frac{1}{3L}$를 사용할 경우, SAGA는 자동적으로 strong convexity $\mu\gt 0$에 적응한다. strongly convex가 아닌 문제에서는 정규화 항($\lambda w^Tw$)을 통해 incremental gradient method들이 적용될 수 있는데, SAGA에서는 $\lambda$의 조정을 피할 수 있다. &lt;em&gt;아마 $h$함수로 뺄 수 있다는 뜻인듯?&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;3-related-work&quot;&gt;[3] Related Work&lt;/h2&gt;

&lt;h3 id=&quot;saga--midpoint-between-sag-and-svrgs2gd&quot;&gt;SAGA : midpoint between SAG and SVRG/S2GD&lt;/h3&gt;

&lt;p&gt;SVRG에서는 SGD의 분산을 관찰해서 step size가 0으로 수렴해야만 전체도 수렴한다는 것을 알고, 상수 step size를 사용하기 위해 SGD에 분산 감소 접근법을 사용하여 선형 수렴도를 얻었다. SVRG 논문에서는 SAG도 분산 감소를 하지만, 어떻게 그런 맥락이 나왔는지는 설명하지 않았는데 이 논문에서 그 연결고리를 설명할 것이다.&lt;/p&gt;

&lt;p&gt;$\mathbb{E}(X)=\theta_\alpha=\alpha(X-Y)+\mathbb{E}(Y)$으로 추정하고 $\theta_\alpha$의 분산을 줄이고 싶을 때, $\text{Var}(\theta_\alpha)=\alpha^2\left[\text{Var}(X)+\text{Var}(Y)-2\text{Cov}(X,Y)\right]$ 이므로 $X, Y$가 서로 높은 상관관계가 있다면 $X$의 분산에 비해 $\theta_\alpha$의 분산이 더 작다. $\alpha$가 $0$부터 $1$까지 증가할 때 분산은 증가하지만 bias는 감소한다.&lt;/p&gt;

&lt;p&gt;$X$가 현재 선택된 gradient $f’_ j(x^k)$이고, $Y$가 과거에 저장된 gradient $f’_ j(\phi_j^k)$라고 하면, SAG는 $\alpha=\frac{1}{n}$이고, SAGA는 $\alpha=1$이다. SAGA는 bias가 없기 때문에 proximal operator를 사용할 수 있다. SVRG는 $Y=f’_ i(\tilde(x))$이다. SAG는 이전의 모든 gradient를 저장해야 하고, SVRG는 반복이 시작될 때 연산이 크다는 단점이 있다. SVRG는 안쪽 루프의 반복수 $m$을 파라미터로 지정해 줘야 한다.&lt;/p&gt;

&lt;h3 id=&quot;finitomisomu&quot;&gt;Finito/MISO$\mu$&lt;/h3&gt;

&lt;p&gt;SAGA에서 $u^0=x^0+\gamma\sum_{i=1}^nf’_ i(x^0)$으로 놓으면 SAGA 알고리즘을 $u$에 대해 업데이트하는 것으로 바꿀 수 있다. Finito와 MISO$\mu$는 $x^{k+1}=\frac{1}{n}\sum_i\phi_i^k-\gamma\sum_{i=1}^nf’_ i(\phi_i^k)$로 업데이트 한다. step length는 $\gamma$이며, step size는 $\frac{1}{\mu n}$이다.&lt;/p&gt;

&lt;p&gt;Finito에서의 $\bar{\phi}$에 대한 식은, $\bar{\phi}$를 $u$로 바꾸면 SAGA의 식과 같다. SAGA는 Finito와 MISO$\mu$과 비교했을 때 strongly convex를 요구하지 않고, 따라서 proximal operator를 사용할 수 있다. 또한 강력한 $\phi_i$값을 요구하지 않는다. Finito/MISO$\mu$는 $f_i$가 연산량이 많을 때 유용하게 쓰일 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;sdca&quot;&gt;SDCA&lt;/h3&gt;

&lt;p&gt;SDCA는 원래 $f_i$의 convex conjugate를 사용하는데, 여기서는 SAGA와의 연결고리를 설명하기 위해 primal값만을 사용하는 방법을 소개한다. 이 방법은 MISO$\mu$방법과도 비슷하다. 이 방법은 dual variable을 사용하는 방법과 결과가 똑같다. &lt;em&gt;아마도 이런 말.. 이부분 내용 이해는 하지 못했음&lt;/em&gt; 이 방법은 각각의 $f_i$가 그냥 convex이기만 할 때, strongly convex한 정규화 텀으로 인해 전체 $f$의 strongly convex함이 유도된다. 그러나 이 정규화 텀을 각 $f_i$에 고르게 넣으면 각 $f_i$가 strongly convex 하도록 바꿀 수 있으며, 그렇게 나온 방법은 Finito와 SDCA의 중간에 있다.&lt;/p&gt;

&lt;h2 id=&quot;5-theory&quot;&gt;[5] Theory&lt;/h2&gt;

&lt;h3 id=&quot;thm-1&quot;&gt;[Thm 1]&lt;/h3&gt;

&lt;p&gt;$x^* $가 optimal solution이고,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
T^k=T(x^k, \{\phi_i^k\}_{i=1}^n)=\frac{1}{n}\sum_if_i(\phi_i^k)-f(x^* )-\frac{1}{n}\sum_i\left&lt;f'_ i(x^* ), \phi_i^k-x^* \right&gt;+c\|x^k-x^* \|^2 %]]&gt;&lt;/script&gt;

&lt;p&gt;($T$는 Lyapunov function) 으로 정의하자. $\gamma=\frac{1}{2(\mu n+L)}, c=\frac{1}{2\gamma(1-\gamma\mu)n}, \kappa=\frac{1}{\gamma\mu}$라고 하면,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbb{E}[T^{k+1}]=(1-\frac{1}{\kappa})T^k&lt;/script&gt;

&lt;p&gt;를 얻을 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;cor-1&quot;&gt;[Cor 1]&lt;/h3&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;c\|x^k-x^* \|^2\le T^k&lt;/script&gt;이므로, $\mu(n-0.5)\le\mu n$을 사용하면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\mathbb{E}\left[\|x^k-x^* \|^2\right]\le\left(\frac{\mu}{2(\mu n+L)}\right)^k\left[\|x^0-x^* \|^2+\frac{n}{\mu n+L}\left[f(x^0)-\left&lt;f'(x^* ), x^0-x^* \right&gt;-f(x^* )\right]\right] %]]&gt;&lt;/script&gt;

&lt;p&gt;를 얻을 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;6-experiments&quot;&gt;[6] Experiments&lt;/h2&gt;

&lt;p&gt;MNIST, COVTYPE, IJCNN1, MILLIONSONG 데이터셋에 실험을 했는데, Finito가 성능은 가장 좋지만 expensive하다. SVRG는 epoch 단위에서는 빠르게 수렴하지만, epoch당 gradient evaluation이 다른 방법에 비해 2배이기 때문에, 가장 좋다고 말할 수 없다. SAGA는 non-permuted Finito와 SDCA와 성능이 비슷하다. 결론은, 수렴도 보다는 각 문제에 대한 성질과 잘 맞는 optimizer를 사용해야 한다.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;incremental gradient methods는 SGD와 같은 것임&lt;br /&gt;
&lt;em&gt;step lenght는 뭐지?&lt;/em&gt;&lt;br /&gt;
&lt;em&gt;proximal operator?&lt;/em&gt;&lt;/p&gt;
</description>
        <pubDate>Sat, 23 Nov 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/thesis/2019/11/23/SAGA.html</link>
        <guid isPermaLink="true">http://localhost:4000/thesis/2019/11/23/SAGA.html</guid>
        
        <category>Optimization</category>
        
        
        <category>Thesis</category>
        
      </item>
    
      <item>
        <title>Accelerating Stochastic Gradient Descent using Predictive Variance Reduction</title>
        <description>&lt;hr /&gt;

&lt;h2 id=&quot;abstract&quot;&gt;[Abstract]&lt;/h2&gt;

&lt;p&gt;SGD의 본질적인 분산 때문에 느리고 점진적으로 수렴하는 단점을 해결하고자 이 논문에서는 SVRG라고 불리는 explicit한 분산 감소 방법을 제안한다. smooth하고 strongly convex한 함수에 대해서는 SDCA, SAG와 같은 수렴속도를 증명했다. SDCA, SAG와의 차이점은 gradient를 저장할 필요가 없다.&lt;/p&gt;

&lt;h2 id=&quot;1-introduction&quot;&gt;[1] Introduction&lt;/h2&gt;

&lt;p&gt;머신러닝에서 푸는 문제는 주로&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\min P(w), P(w):=\frac{1}{n}\sum_{i=1}^n\psi_i(w)&lt;/script&gt;

&lt;p&gt;의 형태이다. (주로 $\psi$는 loss function이다) 주로 이를 풀기 위해 SGD를 사용하는데, 일반적인 형태는&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;w^{(t)}=w^{(t-1)}-\eta_tg_t(w^{(t-1)}, \xi_t)&lt;/script&gt;

&lt;p&gt;이다. SGD는 분산 때문에 computation과 convergence 사이에 trade-off가 있는데, 이를 개선하기 위해 여러 연구가 있었다. SDCA와 SAG가 대표적인데, 이들은 이전의 gradient를 저장하기 때문에 딥러닝같은 복잡한 문제에는 적합하지 않다. 이 논문에서 제안하는 방법은&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;gradient를 저장하지 않아도 되고, 따라서 SDCA나 SAG가 적용하지 못하는 복잡한 문제에 적용할 수 있다.&lt;/li&gt;
  &lt;li&gt;SGD의 분산 감소와 직접적으로 연결될 수 있는 증명을 제시한다.&lt;/li&gt;
  &lt;li&gt;nonconvex 최적화에도 적용될 수 있다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;는 장점을 갖고 있다.&lt;/p&gt;

&lt;h2 id=&quot;2-stochastic-variance-reduced-gradient&quot;&gt;[2] Stochastic Variance Reduced Gradient&lt;/h2&gt;

&lt;p&gt;이 논문에서 제안하는 방법에서는 지정된 시간마다 $\tilde{w}$를 저장한다. 그리고 평균 gradient $\tilde{\mu}$ 또한 저장한다. 이 논문에서 제안하는 update rule은 다음과 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;w^{(t)}=w^{(t-1)}-\eta_t(\triangledown\psi_i(w^{(t-1)})-\triangledown\psi_{i_t}(\tilde{w})+\tilde{\mu})&lt;/script&gt;

&lt;p&gt;SGD와는 다르게 이 방법에 있는 learning rate $\eta_t$는 decay 할 필요가 없고, 따라서 수렴 속도를 높여준다. gradient 계산이 끝나고 나서 $\tilde{w_s}$를 정하는 옵션에는 두 가지가 있는데,&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;$\tilde{w_s}=w_m$으로 정하는 방법&lt;/li&gt;
  &lt;li&gt;$t\in {0, \cdots , m-1}$에서 랜덤으로 선택된 $t$에 대해 $\tilde{w_s}=w_t$으로 선택하는 방법&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;이 있다. practical하게는 첫번째 방법을 쓰지만, 이 논문에서는 분석에서 두번째 방법을 사용했다.&lt;/p&gt;

&lt;h2 id=&quot;3-analysis&quot;&gt;[3] Analysis&lt;/h2&gt;

&lt;p&gt;우선 모든 $\psi_i(w)$는 convex하고 $P(w)$는 strongly convex하다고 한다.&lt;/p&gt;

&lt;h3 id=&quot;thm-1&quot;&gt;[Thm 1]&lt;/h3&gt;

&lt;p&gt;$w_* =\arg\min_w P(w)$라고 하자. $m$이 충분히 커서&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\alpha = \frac{1}{\gamma\eta(1-2L\eta)m}+\frac{2L\eta}{1-2L\eta}\lt 1&lt;/script&gt;

&lt;p&gt;이면, 평균에서 다음과 같은 기하 수렴을 갖게 된다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbb{E}P(\tilde{w_s})\le \mathbb{E}P(w_* )+\alpha^s[P(\tilde{w_* })-P(w_* )]&lt;/script&gt;

&lt;p&gt;이 수렴도를 SAG나 SDCA와 비교해보자. condition number가 $\frac{L}{\gamma}=n$인 사례를 생각해 보면 batch gradient descent에서는 정확도 $\epsilon$을 얻기 위해 반복 당 $n\ln(\frac{1}{\epsilon})$의 복잡도가 나온다. 그러나 SVRG는 $n\ln(\frac{1}{\epsilon})$의 복잡도가 나온다. 이 복잡도들은 SAG나 SDCA와 비슷한 수준이며, SVRG가 더 직관적이므로 더 낫다.&lt;/p&gt;

&lt;p&gt;SVRG는 smooth하지만 strongly convex 하지 않은 경우에도 적용될 수 있는데, 수렴도는 $O(\frac{1}{T})$로, SGD의 수렴도인 $O(\frac{1}{\sqrt{T}})$보다 개선되었다. 인공신경망같은 nonconvex 문제에 SVRG를 이용하려면 local minimum에 가까운 $\tilde{w_0}$에서부터 시작하는 것이 좋다. 그러면 그 이후에 빠르게 수렴할 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;4-sdca-as-variance-reduction&quot;&gt;[4] SDCA as Variance Reduction&lt;/h2&gt;

&lt;p&gt;SDCA와 SAG는 분산 감소라는 측면에서 SVRG와 연결되어 있다. SDCA에서는 dual variable $\alpha_i^* =-\frac{1}{\lambda n}\triangledown\phi_i(w_* )$, $w^{(t)}=\sum_{i=1}^n\alpha_i^{(t)}$를 이용해서 analysis 한다. SDCA도 SVRG와 비슷하게 $\eta_t$가 $0$으로 가지 않아도 분산이 수렴한다.&lt;/p&gt;

&lt;h2 id=&quot;5-experiments&quot;&gt;[5] Experiments&lt;/h2&gt;

&lt;p&gt;Experiments에서는 SVRG를 SGD와 SDCA와 convex한 환경, nonconvex인 환경에서 비교하였다. SVRG의 weight는 SGD를 1번(convex), 10번(nonconvex) 돌려서 초기화 하였다. convex 문제에서 SGD보다는 확실히 좋았고, SDCA와는 비슷했지만 분산 감소 면에서는 SVRG가 더 좋았다. nonconvex에서는 SGD보다는 확실히 좋았고, SDCA와의 비교에서는 좋은 것도 있었고 나쁜 것도 있었다. 그런데 SVRG가 더 안정적으로 수렴하는 것처럼 보인다. SDCA와 SAG는 neural network에는 적용이 불가능하다.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;SAGA 논문에서는 이 논문이 SAG가 분산 감소를 해준다고 하지만, 자세한 설명은 없다고 나와있음.&lt;/p&gt;
</description>
        <pubDate>Fri, 22 Nov 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/thesis/2019/11/22/SVRG.html</link>
        <guid isPermaLink="true">http://localhost:4000/thesis/2019/11/22/SVRG.html</guid>
        
        <category>Optimization</category>
        
        
        <category>Thesis</category>
        
      </item>
    
      <item>
        <title>Histograms of the Normalized Inverse Depth andLine Scanning for Urban Road Detection</title>
        <description>&lt;h2 id=&quot;abstract&quot;&gt;[Abstract]&lt;/h2&gt;

&lt;p&gt;이 논문에서는 3D LiDAR와 하나의 카메라의 기하 정보를 조합해 자동차 앞에 있는 도시의 도로 검출하는 방법을 제안한다. 이 방법의 장점은 3D LiDAR의 정확성과 도로의 연속성이다. 처음에는 LiDAR 데이터의 효과적인 표현과 2D 역 depth map을 얻는다. (3D LiDAR 포인트를 카메라 이미지 평면에 사영) 새로운 표현 방법을 이용하면 도로의 중간 표현을 얻을 수 있다. (정규화된 역 depth map의 vertical, horizontal histogram 추출) 근사된 도로 구역을 히스토그램 기반 방법으로 빠르게 찾을 수 있다. 근사된 도로 구역으로부터 더 정확하게 구역을 찾기 위해서 이 논문에서는 row and column scanning 방법을 제안한다.&lt;/p&gt;

&lt;h2 id=&quot;1-introduction&quot;&gt;[1] Introduction&lt;/h2&gt;

&lt;p&gt;이미지 기반의 검출 방법은 빛에 따라 결과가 달라진다. 특히 학습되지 않은 경우에는 LiDAR가 꼭 필요하다. 그러나 LiDAR만 사용하는 경우에는 point cloud가 매우 sparse하고 체계가 없다는 단점이 있다. 따라서 이 점들을 reorganize 하는 과정이 필요하다.&lt;/p&gt;

&lt;p&gt;원래는 3D LiDAR 점들을 이미지 평면에 사영시키는데, 그 사영은 sparse하므로 interpolation을 이용하여 dense하게 만든다. 그러나 이 논문에서는 Delaunay Triangulation 기반의 선형 평면 interpolation을 이용하여 dense한 역 depth map을 만든다. 그리고 이를 이용하여 horizontal histogram과 vertical histogram을 만드는데, horizontal histogram에서 도로 평면은 직선으로 사영된다. 반면 vertical histogram에서는 각 행마다 normalized inverse depth가 다르기 때문에 도로 픽셀의 누적 점수가 작다. 이 논문에서는 coarse to fine road detection 전략을 따랐는데, histogram은 이 coarse한 정보를 얻는 데에만 사용되었다. (기존의 다른 histogram을 사용한 방법에서는 이를 histogram에서 바로 예측하기 때문에 parameter setting에 노력이 많이 든다.)&lt;/p&gt;

&lt;p&gt;보행로와 도로의 구별이 쉽지 않은데, 이미지 평면의 기하적 특성을 사용하여 위 문제를 해결하기 위해 row and column scanning 전략을 사용하였다. 그리고 높이 차이 임계값을 설정하여 둘을 구분할 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;2-related-works&quot;&gt;[2] Related Works&lt;/h2&gt;

&lt;p&gt;한 &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.233.1475&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;서베이 논문&lt;/a&gt;에서 말하는 최근 트렌드는, 하나의 센서(only 카메라, 스테레오 카메라, LiDAR)를 이용하는 방법과 여러 센서를 이용하는 방법이다.&lt;/p&gt;

&lt;p&gt;하나의 카메라를 이용하는 방법은 보통 분류기를 학습하여 사용한다. 그러나 이런 방법들은 노이즈가 낄 경우 정확하지 않을 수 있다. 스테레오 카메라를 사용하는 방법은 disparity map을 사용한다. 그러나 이런 방법은 dense한 stereo matching을 사용하기 때문에 매우 시간이 오래 걸린다. 또한 마찬가지로 노이즈가 낄 경우 정확한 거리 예측이 어렵고, 연석을 구별하기도 어렵다. 반대로 3D LiDAR의 경우 더 정확한 거리 측정이 가능하다.&lt;/p&gt;

&lt;p&gt;하나의 센서를 이용하는 방법의 단점을 극복하기 위해 여러 센서를 동시에 사용하는 방법들도 있다. 이 논문에서도 카메라와 LiDAR를 이용하는데, 여기서 기하적 특성만 뽑아낸다. 여기서 색 정보와 질감 정보 등이 더 추가될 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;3-the-approximate-road-region-detection-by-histograms-of-the-normalized-inverse-depth&quot;&gt;[3] The Approximate Road Region Detection by Histograms of the Normalized Inverse Depth&lt;/h2&gt;

&lt;p&gt;이 방법은 세 가지의 단계로 이루어져 있는데, [A]와 [B], [4]가 그 단계이다.&lt;/p&gt;

&lt;h3 id=&quot;a-the-fusion-of-lidar-and-camera&quot;&gt;[A] The Fusion of LiDAR and Camera&lt;/h3&gt;

&lt;p&gt;LiDAR와 카메라 정보를 합칠 때 dense한 3D 정보를 얻기 위해서 &lt;a href=&quot;https://www.learnopencv.com/delaunay-triangulation-and-voronoi-diagram-using-opencv-c-python/&quot;&gt;Delaunay Triangulation&lt;/a&gt; 방법이 사용되었는데, normalized inverse depth map, height map($z$ axis), depth map($x$ axis), width map($y$ axis)가 만들어진다.&lt;/p&gt;

&lt;h3 id=&quot;b-the-maps-of-vertical-and-horizontal-histograms-for-rough-road-detection&quot;&gt;[B] The Maps of Vertical and Horizontal Histograms for Rough Road Detection&lt;/h3&gt;

&lt;p&gt;normalized inverse depth map은 stereo vision에서의 disparity map과 같도록 만들었다. horizontal과 vertical histogram은 normalized inverse depth map에서 각 열과 행에 값이 $t$인 값이 몇개가 있는지 세어 만든다.&lt;/p&gt;

&lt;p&gt;vertical histogram에서의 픽셀 수가 threshold보다 작으면 그 픽셀들은 도로에 해당한다고 예측한다. horizontal histogram에서는 도로 구역이 선형적인 구조로 표현되기 때문에, 선형 구조가 나타난다면 도로 평면으로 구분할 수 있다. 이 선은 가장 밑 행에서의 가장 많은 픽셀이 있는 점에서부터 시작해서 한 줄씩 올라가며 그보다 왼쪽에 있는 최대 개수의 픽셀이 있는 점으로 점점 올라가면 된다.&lt;/p&gt;

&lt;p&gt;이론상으로는 horizontal histogram에서 검출된 선을 가지고 바로 3D point cloud를 만들 수 있지만, 도로는 완벽한 평면이 아니기 때문에 margin을 지정해서 도로인 픽셀의 disparity 범위를 정한다. 만약 이 범위보다 $d(m, n)$이 작으면 더 멀리 있는 것이므로 negative obstacle이고, 이 범위보다 $d(m, n)$이 크면 가까이 있는 것이므로 positive obstacle이다. 이 범위에 $d(m, n)$이 있다면 도로로 판정한다.&lt;/p&gt;

&lt;p&gt;이렇게 범위를 정해서 판별하면 잘못 판별될 확률이 높으므로, 이 논문에서는 horizontal histogram 결과를 먼저 구하고, 그렇게 검출된 픽셀들에 한해서 vertical histogram 방법을 적용해서 최종 결과를 만든다.&lt;/p&gt;

&lt;h2 id=&quot;4-the-row-and-column-line-scanning-for-road-region-refinement&quot;&gt;[4] The Row and Column Line Scanning for Road Region Refinement&lt;/h2&gt;

&lt;p&gt;더 정확하게 도로를 검출하기 위해 line scanning을 사용한다. 이 방법은 도로 구역으로 검출된 영역에서만 수행하는데, 도로 영역에서만 reference point를 선택하고, 장애물의 영향을 줄이기 위해서이다.&lt;/p&gt;

&lt;h3 id=&quot;a-the-height-difference&quot;&gt;[A] The Height Difference&lt;/h3&gt;

&lt;p&gt;reference point와의 높이 차이가 threshold보다 크면 도로가 아니고, 그보다 작으면 도로로 검출한다.&lt;/p&gt;

&lt;h3 id=&quot;b-the-reference-point-selection&quot;&gt;[B] The Reference Point Selection&lt;/h3&gt;

&lt;p&gt;reference point의 선택은 결과에 중요한 영향을 미친다. 이 reference point들은 도로가 확실한 영역에서 선택되어야 한다. 이 reference point selection을 위해 row and column scanning이라는 방법이 사용된다.&lt;/p&gt;

&lt;h3 id=&quot;c-the-row-scanning&quot;&gt;[C] The Row Scanning&lt;/h3&gt;

&lt;p&gt;row scanning은 가운데에서 시작해서 왼쪽과 오른쪽으로 진행되고, 도로가 아닌 점을 만날 때 종료된다. 각 row에서의 맨처음 reference point의 선택이 매우 중요한데, 따라서 도로임이 확실한 point여야 한다. 맨 처음 row의 첫 reference point는 그 row에서 가장 가운데에 있는 점으로 정하고, 그 이후 row에서는&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{Middle}_{i-1}=\frac{\text{Left}_i+\text{Right}_i+\text{Middle}_i}{3}&lt;/script&gt;

&lt;p&gt;으로 정한다. 만약 이렇게 예측된 점이 도로 영역에 포함되지 않는다면(앞에 차가 있는 경우 등) 같은 row의 가장 가까운 점을 첫 reference point로 정한다.&lt;/p&gt;

&lt;p&gt;그 후 좌우로 탐색하는데, 노이즈의 영향을 줄이기 위해 도로가 아닌 점이 연속적으로 나오면 탐색을 중지한다.&lt;/p&gt;

&lt;h3 id=&quot;d-the-column-scanning&quot;&gt;[D] The Column Scanning&lt;/h3&gt;

&lt;p&gt;column scanning의 경우에는 row scanning에서의 결과를 가지고 해당 column의 가장 밑 픽셀부터 scanning을 시작한다.&lt;/p&gt;

&lt;h3 id=&quot;e-the-row-and-column-scanning&quot;&gt;[E] The Row and Column Scanning&lt;/h3&gt;

&lt;p&gt;row scanning은 장애물이 있을 때 더 먼 곳까지 발견하지 못하고, column scanning은 첫 reference point가 정확하지 않으면 도로 검출에 실패할 수도 있다. 따라서 이 둘의 방법을 섞는데, row scanning 결과를 기반으로 하고, column scanning 결과로 보충한다. row 와 column scanning에서의 boundary를 계산한 후, threshold를 넘는 개수의 연속된 세로 점들을 찾아서 row scanning 결과를 보충한다. 이후 median blur를 사용한 후 가장 큰 connection을 찾아 최종 결과로 사용한다.&lt;/p&gt;
</description>
        <pubDate>Wed, 13 Nov 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/thesis/2019/11/13/HID.html</link>
        <guid isPermaLink="true">http://localhost:4000/thesis/2019/11/13/HID.html</guid>
        
        <category>RobotNavigation</category>
        
        
        <category>Thesis</category>
        
      </item>
    
      <item>
        <title>Scaling SGD Batch Size to 32K for ImageNet Training</title>
        <description>&lt;h2 id=&quot;abastract&quot;&gt;[Abastract]&lt;/h2&gt;

&lt;p&gt;큰 네트워크의 학습 속도를 높이는 자연스러운 방법은 여러개의 GPU를 사용하는 것이다. 확률 기반 방법을 더 많은 프로세서로 확장하려면 각 GPU의 컴퓨팅 파워를 최대로 사용하기 위해 batch size를 늘려야 한다. 그러나 batch size를 계속해서 올리면서 네트워크의 정확도를 유지하는 것은 불가능하다. 현재 sota방법은 batch size에 반비례하게 LR을 늘리고, 초기 최적화의 어려움을 극복하기 위해 LR에 ‘warm-up’이라는 방법을 사용한다.&lt;/p&gt;

&lt;p&gt;LR을 학습 중에 조절하므로써 ImagNet 학습에 큰 batch size를 효과적으로 사용할 수 있다. 그러나 ImageNet-1k 학습을 위해서는 현재 AlexNet이나 ResNet은 batch size를 크게 늘릴 수 없다. 이유는 큰 LR을 사용할 수 없기 때문이다. 큰 batch size를 일반적인 네트워크나 데이터에 대해 사용 가능하게 하기 위해, 이 논문에서는 Layer-wise Adaptive Rate Scaling (LARS)를 제안한다. LARS에서는 weight의 norm과 gradient의 norm을 기반으로 층마다 다른 LR을 사용한다. LARS를 사용하면 ResNet50과 AlexNet에 대해서 큰 batch size를 사용할 수 있다. 큰 batch size는 시스템의 컴퓨팅 파워를 최대로 사용할 수 있다. 이것은 속도의 향상으로 이어진다.&lt;/p&gt;

&lt;h2 id=&quot;1-introduction&quot;&gt;[1] Introduction&lt;/h2&gt;

&lt;p&gt;깊은 네트워크의 확장과 속도 향상은 딥러닝의 응용에서 매우 중요하다. 속도 향상을 위해서는 더 많은 프로세서로 확장시켜야 한다. 그러기 위해서는 batch size를 더 크게 해야 한다.  그러나 이전 연구에 따르면 batch size를 늘이는 것은 test accuracy에서 좋지 않은 결과가 나온다. 학습 중에 LR을 조절하면 큰 batch size에서도 좋은 결과를 유지할 수 있다. 하지만 현재까지의 연구들은 1024 이상의 크기의 batch size에서는 사용할 수 없다. 이 논문의 저자들은 batch normallization을 사용해 큰 batch size에서도 좋은 성능을 얻었지만 여전히 정확도를 잃었다. &lt;em&gt;어디에서 잃었다는건지 모르겠음… 이전에 작은 batch size로 했을 때랑 결과가 똑같은데..?&lt;/em&gt; 이를 더 개선하기 위해 LARS를 제안하는데, weight의 norm &lt;script type=&quot;math/tex&quot;&gt;\|w\|&lt;/script&gt;과 gradient의 norm &lt;script type=&quot;math/tex&quot;&gt;\triangledown w\|&lt;/script&gt;에 따라 층마다 다른 LR을 사용한다. 만약 같은 LR을 쓴다면 &lt;script type=&quot;math/tex&quot;&gt;\frac{\|w\|}{\|\triangledown w\|}&lt;/script&gt;이 큰 층은 수렴해도 작은 층은 발산할 수도 있다. LARS를 사용하면 더 큰 batch size를 사용해도 같은 정확도를 얻을 수 있다. 또한, GPU의 컴퓨팅 파워를 최대로 사용할 수 있기 때문에 속도 향상 또한 가능하다.&lt;/p&gt;

&lt;h2 id=&quot;2-background-and-related-work&quot;&gt;[2] Background and Related Work&lt;/h2&gt;

&lt;h3 id=&quot;21-data-parallelism-mini-batch-sgd&quot;&gt;[2.1] Data-Parallelism Mini-Batch SGD&lt;/h3&gt;

&lt;p&gt;보통의 mini-batch SGD는 update 식을&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;w_{t+1}=w_t-\frac{\eta}{b}\sum_{b\in B_t}\triangledown l(x, y, w)&lt;/script&gt;

&lt;p&gt;라고 쓰는데, 복잡하므로 이 논문에서는&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;w_{t+1}=w_t-\eta\triangledown w_t&lt;/script&gt;

&lt;p&gt;라고 쓴다.&lt;/p&gt;

&lt;h3 id=&quot;22-large-batch-training-difficulty&quot;&gt;[2.2] Large-Batch Training Difficulty&lt;/h3&gt;

&lt;p&gt;GPU를 사용하면 여러 개의 프로세서를 동시에 돌릴 수 있지만, 이를 최대로 사용하기 위해서는 batch size가 커야 하는데, 특정 크기보다 크게 되면 테스트 정확도가 학습 정확도보다 현저하게 낮다. 이전 연구에서는 큰 batch size에 대해 학습 loss가 작아도 test loss는 그것보다 훨씬 크고, 반면 작은 batch size에 대해서는 train loss와 test loss가 비슷하다고 결론지었다. 다른 연구에서는 더 오래 학습하는 것이 일반화에 더 도움이 된다고 했는데, 반면 또다른 연구에서는 LR을 잘 조절하는 것이 정확도를 유지하는데 더 도움이 된다고 했다.&lt;/p&gt;

&lt;h3 id=&quot;23-learning-rate-lr&quot;&gt;[2.3] Learning Rate (LR)&lt;/h3&gt;

&lt;p&gt;batch size를 크게 하면 기본 LR도 그에 따라 올려야 하는데, 그러면서 정확도는 내려가지 않아야 한다. 기존 연구에서는 sqrt scaling rule과 linear scaling rule이 있었다. 또한 warmup scheme도 많이 사용되고 있는데, 처음에 작은 LR을 설정해 놓고 몇 epoch가 지나면 점점 크게 하는 방법이다. 이것이 gradual warmup scheme이고, constant warmup scheme은 초기 몇 번의 epoch에서 constant한 LR을 사용하는 방법이다. &lt;em&gt;그뒤에 어떻게 한다는 건지 안나와있네…왜 이게 warmup이지&lt;/em&gt; constant warmup은 object detection이나 segmentation에 효과적이고, gradual warmup은 ResNet-50을 학습시키는 데에 좋다. batch size에 관계없이 LR에 상한이 있다는 또다른 연구도 있는데, 이 논문의 실험 결과는 위 논문의 결과와 같다.&lt;/p&gt;

&lt;h2 id=&quot;3-imagenet-1k-training&quot;&gt;[3] ImageNet-1k Training&lt;/h2&gt;

&lt;h3 id=&quot;31-reproduce-and-extend-facebooks-result&quot;&gt;[3.1] Reproduce and Extend Facebook’s result&lt;/h3&gt;

&lt;p&gt;이 논문에서의 첫 단계는 Facebook에서 한 연구의 결과를 따라하는 것이었는데, 그 연구에서는 multistep LR과 warmup, linear scaling LR을 사용했다. 이 연구에서도 그 연구와 비슷하게 warmup과 linear scaling을 사용했는데, Facebook의 연구와 다른 점은&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;LR을 더 높였다.&lt;/li&gt;
  &lt;li&gt;multistep rule 대신 poly rule &lt;em&gt;설명이 없네&lt;/em&gt; 을 사용하였다.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;32-train-imagenet-by-alexnet&quot;&gt;[3.2] Train ImageNet by AlexNet&lt;/h3&gt;

&lt;h4 id=&quot;321-linear-scaling-and-warmup-schemes-for-lr&quot;&gt;[3.2.1] Linear Scaling and Warmup schemes for LR&lt;/h4&gt;

&lt;p&gt;이 논문에서의 baseline은 Batch-512 AlexNet이고, 100 epoch동안 0.58의 정확도를 얻는 것이다. poly rule을 사용했고, base LR은 0.01이며 poly power는 2이다. 목표는 Batch-4096, Batch-8192이고 정확도 0.58을 100 epoch 안에 얻는 것이다.&lt;/p&gt;

&lt;p&gt;첫번째로 Batch-4096 AlexNet을 사용했고 linear scaling을 사용했으며 baseLR을 0.08로 사용했으나, Batch-4096은 0.01의 LR에도 수렴하지 않았다. 이후에는 같은 모델에 linear scaling과 warmup을 사용하였다. 하이퍼파라미터 튜닝을 거쳐 나온 가장 좋은 결과는 0.531에 그쳤다. 또한 Batch-8192의 정확도는 그것보다도 낮았다. 따라서 linear scaling과 warmup은 큰 batch size의 AlexNet을 학습시키는데 충분하지 않다는 것을 알았다.&lt;/p&gt;

&lt;h4 id=&quot;322-batch-normalization-for-large-batch-training&quot;&gt;[3.2.2] Batch Normalization for Large-Batch Training&lt;/h4&gt;

&lt;p&gt;여러 방법을 사용해본 결과, Batch Normalization만 성능을 개선시킨다는 것을 관찰했다. BN을 적용하고 0.01부터 결과가 발산할 때까지 LR을 늘렸는데, Batch-4096은 0.3에서 멈추었고, Batch-8192는 0.41에서 멈추었다. 그러나 이렇게 해도 Batch-512의 정확도에 도달하기 위해서는 정확도가 부족했다. momentum과 weight decay 파라미터를 조정해 보았으나 개선은 없었다.&lt;/p&gt;

&lt;p&gt;이후에는 성능이 개선되지 않는 문제가 일반화 문제 때문이 아님을 관찰하였다. 이유는 최적화 어려움 때문이었다.&lt;/p&gt;

&lt;h4 id=&quot;323-layer-wise-adaptive-rate-scaling-lars-for-large-batch-training&quot;&gt;[3.2.3] Layer-wise Adaptive Rate Scaling (LARS) for Large-Batch Training&lt;/h4&gt;

&lt;p&gt;따라서 새로운 LR updating rule을 디자인했다. 실험 결과에서 층마다 다른 LR이 필요함을 느꼈는데, 그 이유는 &lt;script type=&quot;math/tex&quot;&gt;\|w\|_ 2&lt;/script&gt;와 &lt;script type=&quot;math/tex&quot;&gt;\|\triangledown w\|_ 2&lt;/script&gt;의 비율이 층마다 매우 다르기 때문이었다. 이전에 이런 문제를 해결하기 위한 ResNet50을 위한 해결법 연구가 있었는데, AlexNet에는 효과가 없었다.&lt;/p&gt;

&lt;p&gt;이 논문에서 제안한 LARS의 업데이트 룰은 다음과 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\eta=l\times\gamma\times\frac{\|w\|_ 2}{\|\triangledown w\|_ 2}&lt;/script&gt;

&lt;p&gt;여기서 $l$은 scaling factor ($0.001$)이고, $\gamma$는 input LR ($1$~$50$)이다. $\mu$를 momentum이라고 하고 $\beta$를 weight decay 라고 하고 LARS 알고리즘을 살펴보면&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;각 층마다 local LR을 구한다. &lt;script type=&quot;math/tex&quot;&gt;\alpha = l\times\frac{\|w\|_ 2}{\|\triangledown w\|_ 2+\beta\|\triangledown w\|_ 2}&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;각 층마다 true LR을 구한다. &lt;script type=&quot;math/tex&quot;&gt;\eta = \gamma\times\alpha&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;gradient를 업데이트 한다. &lt;script type=&quot;math/tex&quot;&gt;\triangledown w=\triangledown w+\beta w&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;가속 항 $a$를 &lt;script type=&quot;math/tex&quot;&gt;a=\mu a+\eta\triangledown w&lt;/script&gt; 를 이용해 업데이트 한다.&lt;/li&gt;
  &lt;li&gt;weight를 업데이트 한다. $w=w-a$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;이다. LARS를 적용해서 baseline과 같은 결과를 얻을 수 있었다. 그 뒤에는 BN을 빼고 기본 AlexNet 모델을 사용했는데, Batch-4096에는 13 epoch, Batch-8192에는 8 epoch의 warmup range를 사용하였다. 큰 batch size를 사용해서 작은 batch size에서의 결과와 비슷한 결과를 얻으려면 BN만으로는 부족하고 LARS까지 사용해야 한다는 것을 알았다. 결과에서 보면 LARS만 사용한 결과도 둘 다를 사용한 결과보다 조금 더 낮다.&lt;/p&gt;

&lt;h2 id=&quot;4-experimental-results&quot;&gt;[4] Experimental Results&lt;/h2&gt;

&lt;h3 id=&quot;42-implementation-details&quot;&gt;[4.2] Implementation Details&lt;/h3&gt;

&lt;p&gt;8192정도의 Batch size를 한번에 저장할 수 있는 GPU가 없었기 때문에 하나의 Batch를 32개로 나누어 순차적으로 gradient를 계산했다. GPU의 메모리가 Batch 여러 개를 넣기에 충분한 경우에는 multi solver를 넣어 속도를 향상시켰다.&lt;/p&gt;

&lt;h3 id=&quot;43-state-of-the-art-results-for-resnet50-training&quot;&gt;[4.3] State-of-the-art Results for ResNet50 training&lt;/h3&gt;

&lt;p&gt;이 논문의 결과는 data augmentation을 하지 않았기 때문에 sota 결과보다는 정확도가 낮다. data augmentation을 추가하면 sota 결과가 나온다. 이 논문에서는 32768의 batch size를 사용했다.&lt;/p&gt;

&lt;h3 id=&quot;44-benefits-of-using-large-batch&quot;&gt;[4.4] Benefits of Using Large Batch&lt;/h3&gt;

&lt;p&gt;큰 batch size를 사용할 수 있게 되고 baseline과 같은 결과를 얻게 되자, 이 논문에서는 속도 비교에 중점을 두었다. AlexNet-BN의 batch size가 512에서 4096으로 커지자, 4개의 GPU에서는 속도가 비슷했지만, 8개의 GPU에서는 속도가 3배 빨라졌다.&lt;/p&gt;

&lt;h2 id=&quot;5-conclusion&quot;&gt;[5] Conclusion&lt;/h2&gt;

&lt;p&gt;최적화 어려움이 큰 batch size에 대해 학습을 어렵게 만들었다. linear scaling이나 warmup같은 방법들은 AlexNet등의 복잡한 모델에는 충분하지 않다. BN같은 모델 구조를 변형하는 방법들도 사용할 수 있겠지만 충분하지 않으므로, 이 논문에서 제안하는 LARS를 사용하면 충분히 baseline 정확도를 얻을 수 있다. LARS는 층마다 weight와 weight의 gradient의 norm에 따라 다른 LR을 사용하는데, 매우 높은 효율성을 보여준다. ImageNet을 학습시키는 AlexNet에서 Batch size를 128에서 8192로 키워도 정확도 손실이 없다. 또한 ResNet50에서는 batch size를 32768까지 키울 수 있었다.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;수학적인 증명은 별로 없는 논문이었다. 실험적인 결과로만 가설을 세우고 쓴 논문..&lt;/p&gt;

&lt;p&gt;설명이 더 필요한 개념 : LR scheduler에서의 poly rule, constant warmup&lt;/p&gt;
</description>
        <pubDate>Sun, 03 Nov 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/thesis/2019/11/03/SCALEBATCHSIZE.html</link>
        <guid isPermaLink="true">http://localhost:4000/thesis/2019/11/03/SCALEBATCHSIZE.html</guid>
        
        <category>Optimization</category>
        
        
        <category>Thesis</category>
        
      </item>
    
      <item>
        <title>SGD - General Analysis and Improved Rates</title>
        <description>&lt;h2 id=&quot;abstract&quot;&gt;[Abstract]&lt;/h2&gt;

&lt;p&gt;이 논문에서는 임의 샘플링에 대한 SGD의 수렴을 설명하는 이론을 제안한다. 이 이론은 SGD의 여러 종류의 수렴을 설명하는데, 각각은 mini-batch를 형성하는 데에 특정한 확률 분포에 연관되어 있다. 이런 분석을 한 것은 처음이며, 분석에 사용된 SGD는 대부분 이전에 논문에서는 명시적으로 고려되지 않았던 것들이다. 이 분석은 최근에 소개된 expcted smoothness에 의존하며, stochastic gradient들의 bound에는 의존하지 않는다. replacement sampling과 independent sampling 등과 같은 여러 다른 minibatch 전략에 대해 연구하므로써, 우리는 stepsize를 mini-batch의 크기에 대한 하나의 함수로써 표현해냈다. 이렇게 하면 전체 복잡도를 최소화하는 mini-batch의 크기를 정할 수 있으며, 최소로 evaluate된 stochastic gradient의 분산이 커지면 최적의 mini-batch 크기도 증가한다는 것을 명시적으로 보여주었다. 분산이 0이면 최적의 mini-batch 크기는 $1$이다. 더해서, 상수였던 stepsize를 언제 감소시켜야 하는지를 결정하는 stepsize-switching 규칙을 증명한다.&lt;/p&gt;

&lt;h2 id=&quot;1-introduction&quot;&gt;[1] Introduction&lt;/h2&gt;

&lt;p&gt;우선 이런 최적화 문제를 풀려고 한다. (이것은 감독학습 문제의 일반적인 형태이다)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x^* = \arg\min_{x\in \mathbb{R}^d}\left[f(x) =\frac{1}{n}\sum_{i=1}^n f_i(x)\right]&lt;/script&gt;

&lt;p&gt;모든 $f_i : \mathbb{R}^d\rightarrow\mathbb{R}$은 smooth이다. 그리고 $f$는 global minimizer $x^* $를 가지며, $\mu$-strongly quasi-convex이다. 따라서&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
f(x^* )\ge f(x) +\left&lt;\triangledown f(x),x^* −x\right&gt;+\frac{\mu}{2}\|x^* −x\|^2 %]]&gt;&lt;/script&gt;

&lt;p&gt;를 만족한다.&lt;/p&gt;

&lt;h3 id=&quot;11-background-and-contributions&quot;&gt;[1.1] Background and contributions&lt;/h3&gt;

&lt;h4 id=&quot;linear-convergence-of-sgd&quot;&gt;Linear convergence of SGD&lt;/h4&gt;

&lt;p&gt;2011년에는 strongly convex한 $f$를 특정 noise level까지 수렴하게 하는 비점근 분석의 연구가 있었다. 2016년에는 condition number에 대한 2차 의존성을 없애서 위 결과를 개선하고, importance sampling을 고려했다. 위 연구의 결과는 이후에 mini-batch 다양성에 대한 연구로 확장되었다. 이 논문에서는 위 결과를 대부분의 샘플링에 대해 적용할 수 있도록 했다. 분산 감소 방법의 관점에서 expected smoothness assumption을 도입했는데, 이 assumption은 SGD에서 사용하는 $f$와 $\mathcal{D}$의 연관 특징으로, 이 논문에서 일반적인 복잡도 결과(Thm 3.1)를 증명할 수 있게 해주었다. strong convexity 없이(사실은 quasi-convexity를 가정하였다) 선형 수렴도를 얻을 수 있었다. 또한, 함수 $f_i$들은 convex가 아니어도 된다.&lt;/p&gt;

&lt;h4 id=&quot;gradient-noise-assumptions&quot;&gt;Gradient noise assumptions&lt;/h4&gt;

&lt;p&gt;이전의 많은 SGD의 선형 수렴에 관한 연구들은 확률론적 gradient의 분산이 bound됨을 가정하고 증명했다. 이후에는 점점 가정을 약하게 하는 방향으로 가고 있다. 이 논문에서는 직접적으로 gradient의 bound는 가정하지 않고, 더 약한 가정인 expected smoothness assumption을 사용한다.&lt;/p&gt;

&lt;h4 id=&quot;optimal-mini-batch-size&quot;&gt;Optimal mini-batch size&lt;/h4&gt;

&lt;p&gt;이전의 연구 결과로 큰 mini-batch 크기를 사용하는 것이 넓은 범위의 non-convex문제를 푸는 것에 효과적이라는 것이 밝혀졌는데, 저자는 최적의 stepsize가 mini-batch 크기에 따라 선형적으로 늘어난다고 추측했다. 이 논문에서는 이 추측이 최적의 mini-batch 크기까지 커질 때에만 적용되며, mini-batch 크기에 따른 최적의 stepsize의 정확한 공식이 있음을 증명하고 그 공식을 제공한다.&lt;/p&gt;

&lt;h4 id=&quot;learning-schedules&quot;&gt;Learning schedules&lt;/h4&gt;

&lt;p&gt;이전 연구에서는 해 주위에서 SGD의 수렴을 감지하는 방법이 제안되었다. 이 논문에서는 stepsize를 언제 줄여야 하는지 알려주는 식을 제공한다. 또한, 샘플링 방법에 따라, 그리고 mini-batch 크기에 따라 stepsize와 반복 복잡도가 어떻게 증가하고 감소하는지 보여준다.&lt;/p&gt;

&lt;h4 id=&quot;over-parameterized-models&quot;&gt;Over-parameterized models&lt;/h4&gt;

&lt;p&gt;이전 연구에서 데이터보다 파라미터 수가 많을 때의 SGD의 수렴 속도를 분석한 연구가 있었다. 이 논문에서는 over-parametrized model의 경우 최적의 mini-batch 크기가 $1$임을 보여준다.&lt;/p&gt;

&lt;h3 id=&quot;12-stochastic-reformulation&quot;&gt;[1.2] Stochastic reformulation&lt;/h3&gt;

&lt;p&gt;sampling vector를 이용해서 각 함수와 gradient의 weight를 다르게 만든다. 평균을 취하면 결국 원래의 함수 또는 gradient와 같게 된다. 이 방법은 이전에도 분산을 줄이는 목적으로 사용되었다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x^{k+1}=x^k-\gamma^k\triangledown f_{c^k}(x^k)&lt;/script&gt;

&lt;p&gt;를 이용해서 gradient descent를 할 수 있다. 이 논문에서는 여러 다른 분포 $\mathcal{D}$에 대한 위 gradient descent의 수렴을 분석할 것이다. 또한 수렴을 얻기 위해 SGD를 변형한다.&lt;/p&gt;

&lt;h2 id=&quot;2-expected-smoothness-and-gradient-noise&quot;&gt;[2] Expected Smoothness and Gradient Noise&lt;/h2&gt;

&lt;h3 id=&quot;21-expected-smoothness&quot;&gt;[2.1] Expected smoothness&lt;/h3&gt;

&lt;p&gt;expected smoothness는 $\mathcal{D}$ 분포의 특성과 $f$의 smoothness 특성을 결합한 것이다.&lt;/p&gt;

&lt;h4 id=&quot;assumption-21-expected-smoothness&quot;&gt;Assumption 2.1 (Expected Smoothness)&lt;/h4&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbb{E}_\mathcal{D}[\|\triangledown f_v(x)-\triangledown f_v(x^* )\|^2]\le 2\mathcal{L}(f(x)-f(x^* )), \forall x\in\mathbb{R}^d&lt;/script&gt;

&lt;p&gt;인 $\mathcal{L}=\mathcal{L}(f, \mathcal{D})\gt0$이 존재할 때, $f$가 분포 $\mathcal{D}$에 대해 $\mathcal{L}$-smooth in expectation이라고 한다. 간단히, $(f, \mathcal{D})\sim ES(\mathcal{L})$이라고 표현한다. 이것이 expected smoothness의 정의이다. 이 expected smoothness는 $f_i$나 $f$가 convex가 아니어도 성립할 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;22-gradient-noise&quot;&gt;[2.2] Gradient noise&lt;/h3&gt;

&lt;p&gt;이 논문에서 쓰이는 중요 가정 중 두번째는 gradient noise의 유한성이다. 이 가정은 매우 약한 가정이고, $f$보다는 $\mathcal{D}$에 관한 가정이다.&lt;/p&gt;

&lt;h3 id=&quot;23-key-lemma-and-connection-to-the-weak-growth-condition&quot;&gt;[2.3] Key lemma and connection to the weak growth condition&lt;/h3&gt;

&lt;p&gt;보통 SGD의 수렴을 증명할 때는&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbb{E}\|\triangledown f_v(x)\|^2\le c&lt;/script&gt;

&lt;p&gt;라는 가정이 들어가는데, $f$가 strongly convex일 때는 보통 성립하지 않는다. 이 논문에서는 이 가정 대신에 확률론적 gradient의 기댓값을 bound하기 위해 expected smoothness를 사용한다.&lt;/p&gt;

&lt;h4 id=&quot;lem-24&quot;&gt;[Lem 2.4]&lt;/h4&gt;

&lt;p&gt;만약 $(f, \mathcal{D})\sim ES(\mathcal{L})$이면,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbb{E}_\mathcal{D}[\|\triangledown f_v(x)\|^2]\le 4\mathcal{L}(f(x)-f(x^* ))+2\sigma^2&lt;/script&gt;

&lt;p&gt;이다.&lt;/p&gt;

&lt;p&gt;$\sigma=0$인 zero-noise setting에서는 이 식이 weak growth condition이라고 알려져 있다. 이전에도 비슷한 가정이 있었는데, 이 논문에서 제시된 bound가 더 강력하다.&lt;/p&gt;

&lt;h2 id=&quot;3-convergence-analysis&quot;&gt;[3] Convergence Analysis&lt;/h2&gt;

&lt;h3 id=&quot;31-main-rules&quot;&gt;[3.1] Main rules&lt;/h3&gt;

&lt;h4 id=&quot;thm-31&quot;&gt;[Thm 3.1]&lt;/h4&gt;

&lt;p&gt;$f$가 $\mu$-quasi-strongly convex이고 $(f, \mathcal{D})\sim ES(\mathcal{L})$이라고 하자. 모든 $k$에 대하여 $\gamma^k=\gamma\in(0, \frac{1}{2\mathcal{L}}]$을 고른다. 그러면 SGD는 다음을 만족한다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbb{E}\|x^k-x^* \|^2\le(1-\gamma\mu)^k\|x^0-x^* \|^2+\frac{2\gamma\sigma^2}{\mu}&lt;/script&gt;

&lt;p&gt;따라서 모든 $\epsilon \gt 0$에 대해 $\gamma=\min{\frac{1}{2\mathcal{L}}, \frac{\epsilon\mu}{4\sigma^2}}$와 &lt;script type=&quot;math/tex&quot;&gt;k\ge\max\{\frac{2\mathcal{L}}{\mu}, \frac{4\sigma^2}{\epsilon\mu^2}\}\log\left(\frac{2\|x^0-x^* \|^2}{\epsilon}\right)&lt;/script&gt;을 고르면,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbb{E}\|x^k-x^* \|^2\le\epsilon&lt;/script&gt;

&lt;p&gt;을 만족한다.&lt;/p&gt;

&lt;p&gt;이 Thm이 말하는 바는 SGD가 gradient noise $\sigma^2$과 stepsize $\gamma$에 의존하는 $\frac{2\gamma\sigma^2}{\mu}$까지 선형적으로 수렴한다는 것이다. 더 작은 stepsize를 사용하면 더 수렴할 수 있지만, 수렴도가 떨어진다. $\mathcal{D}$를 조절할 수 있으므로, $\sigma^2$와 $\mathcal{L}$도 조절이 가능하다. stepsize 를 계속 감소시키면 위 항을 더 작게 만들 수도 있다.&lt;/p&gt;

&lt;h4 id=&quot;thm-32&quot;&gt;[Thm 3.2]&lt;/h4&gt;

&lt;p&gt;$f$가 $\mu$-quasi-strongly convex이고 $(f, \mathcal{D})\sim ES(\mathcal{L})$이라고 하자. $\mathcal{K}=\frac{\mathcal{L}}{\mu}$라고 하고,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\gamma^k=\begin{cases}\frac{1}{2\mathcal{L}} &amp; \text{for }k\le 4\lceil\mathcal{K}\rceil\\ \frac{2k+1}{(k+1)^2\mu} &amp; \text{for }k\gt 4\lceil\mathcal{\mathcal{K}}\rceil\end{cases} %]]&gt;&lt;/script&gt;

&lt;p&gt;라고 하자. 만약 $k\ge 4\lceil\mathcal{K}\rceil$이면 다음을 만족하며 수렴한다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbb{E}\|x^k-x^* \|^2\le\frac{\sigma^2}{\mu^2}\frac{8}{k}+\frac{16\lceil{\mathcal{K}\rceil^2}}{e^2k^2}\|x^0-x^* \|^2&lt;/script&gt;

&lt;h3 id=&quot;32-choosing-mathcald&quot;&gt;[3.2] Choosing $\mathcal{D}$&lt;/h3&gt;

&lt;p&gt;위 gradient descent가 효과적이려면, sampling vector $v$가 sparse 해야 한다.&lt;em&gt;왜?&lt;/em&gt; 이 논문에서는 proper sampling 만을 고려한다. sampling $S$가 proper 하다는 말은 모든 $i$에 대해, $i$가 $S$안에 존재할 확률 $p_i$, 즉 $i \in C$인 모든 $C$의 확률을 다 더한 것이 양수라는 말이다. ($\mathbb{P}[i\in S] = \sum_{C:i\in C}p_C \ge 0, \forall i$) 이 proper sampling은 sampling $S$에 의존하는 sampling vector를 특수화 한, 이 논문에서는 특수한 케이스이다. proper sampling의 정의를 기반으로 Independent sampling, Partition sampling, Single element sampling, $\tau$-nice sampling을 정의할 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;33-bounding-mathcall-and-sigma2&quot;&gt;[3.3] Bounding $\mathcal{L}$ and $\sigma^2$&lt;/h3&gt;

&lt;p&gt;$f$가 convex이고 smooth하다고 가정하면 expected smoothness $\mathcal{L}$과 $\sigma^2$에 대한 closed form expression을 계산할 수 있다. 그리고 sampling을 이용해 $\mathcal{L}$과 $\sigma^2$에 대한 bound를 할 수 있는데, sampling의 종류에 따라 이 bound가 달라진다.&lt;/p&gt;

&lt;h2 id=&quot;4-optimal-mini-batch-size&quot;&gt;[4] Optimal Mini-Batch Size&lt;/h2&gt;
&lt;p&gt;위에서 나온 결과에 $|S|=n$인 경우를 대입해 보면 우리가 원래 알고 있던 full-batch gradient descent결과와 같다. 따라서 full-batch gradient descent는 SGD의 특별한 케이스라고 볼 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;nontation&quot;&gt;[Nontation]&lt;/h3&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;L_i=\lambda_{\max}(M_i)&lt;/script&gt;&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;L_{\max}=\max_{i\in[n]}L_i&lt;/script&gt;&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;\bar{h} = \frac{1}{n}\sum_{i\in[n]}\|h_i\|^2&lt;/script&gt;&lt;/p&gt;

&lt;h3 id=&quot;41-nonzero-gradient-noise&quot;&gt;[4.1] Nonzero gradient noise&lt;/h3&gt;
&lt;p&gt;mini-batch 크기에 따라 복잡도가 어떻게 변하는지 알기 위해 $|S|=\tau$인 independent sampling과 $\tau$-nice sampling의 경우를 생각해보자.&lt;/p&gt;

&lt;p&gt;Independent sampling의 경우에는 bound된 결과를 Thm 3.1에 넣으면 최소 몇 번의 반복을 해야 하는지 알 수 있는데, 이것은 현재 나온 것 중 최적이다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;k\ge\frac{2}{\mu}\max\{L+\max_{i\in[n]}\frac{1-p_i}{np_i}L_i, \frac{2}{\mu\epsilon}\frac{1-p_i}{np_i}\bar{h}\}&lt;/script&gt;

&lt;p&gt;또한 최적의 step size $\gamma$도 알 수 있게 되고, $\tau$가 커질수록 stepsize는 줄어든다는 것을 알 수 있다. 따라서 전체 반복 수를 최소화하는 mini-batch 크기($\tau$)를 선택할 수 있고, 계산으로 이를 구할 수 있다. $\tau$-nice sampling에서도 independent sampling과 같은 방법으로 최적의 mini-batch 크기를 구할 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;42-zero-gradient-noise&quot;&gt;[4.2] Zero gradient noise&lt;/h3&gt;

&lt;p&gt;$\sigma=0$인 경우를 생각해 보자. Thm 3.1에 따르면 SGD의 반복 복잡도는 매우 간단해진다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;k\ge\frac{2L}{\mu}&lt;/script&gt;

&lt;p&gt;이 경우에 $f$는 weak growth condition을 만족하게 되고, 이전의 연구 결과와 직접적으로 비교할 수 있게 된다. $\tau$가 $n$까지 커질수록 복잡도가 개선되지만, 전체 복잡도는 $\tau$가 곱해지기 때문에 이것으로는 충분하지 않아서 $\tau=1$이 최적의 mini-batch 크기가 된다.&lt;em&gt;이부분이 잘 이해가 안됨..저걸로는 충분하지 않아서 batch-size를 1로 한다고..?&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;5-importance-sampling&quot;&gt;[5] Importance Sampling&lt;/h2&gt;

&lt;p&gt;이 논문에서는 single element sampling과 independent sampling에 대한 importance sampling을 각각 제안한다.&lt;/p&gt;

&lt;h3 id=&quot;51-single-element-sampling&quot;&gt;[5.1] Single element sampling&lt;/h3&gt;

&lt;p&gt;$\mathcal{L}$에 대한 bound와 $\sigma^2$을 Thm 3.1에 대입하면 최소 반복 수를 얻을 수 있는데, $p_i$에 대해 이 최소 반복 수를 최소화하는 문제는 매우 어렵기 때문에 이 논문에서는 &lt;script type=&quot;math/tex&quot;&gt;\mathcal{L}_{\max}&lt;/script&gt; 와 $\sigma^2$ 을 줄이는 것에 집중한다. single element sampling에 대해서는 이전 연구에 있었던 partially biased sampling을 떠올릴 수 있다. &lt;script type=&quot;math/tex&quot;&gt;\mathcal{L}_{\max}&lt;/script&gt;를 최소화시키는 $p_i$는 $p_i^\mathcal{L}=\frac{L_i}{\sum_{j\in [n]} L_j}$인데, 이 확률을 적용하면 이전 연구에 있었던 partially biased sampling과 똑같아진다. uniform sampling과 비교해보면 $L_{\max}=n\bar{L}$인 극한의 상황에서는 partially biased sampling이 절반의 복잡도를 가질 수 있다. &lt;em&gt;왜 갑자기 uniform sampling이랑 비교하지?&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;52-minibatches&quot;&gt;[5.2] Minibatches&lt;/h3&gt;
&lt;p&gt;이 논문에서는 mini-batch SGD를 위한 importance sampling을 최초로 제안한다. 이에 대한 결과는&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;k\ge\max\{(1-\frac{2}{\tau})\frac{2\bar{L}}{\alpha\mu}, (\frac{2}{\tau}-\frac{1}{n})\frac{8\bar{h}}{\epsilon\mu^2}\}&lt;/script&gt;

&lt;p&gt;인데, $L_{\max}$에 대한 의존성을 없앴을 뿐 아니라, $\tau$가 커지면 복잡도가 더 줄어든다.&lt;/p&gt;

&lt;h2 id=&quot;6-experiments&quot;&gt;[6] Experiments&lt;/h2&gt;

&lt;h3 id=&quot;61-constant-vs-decreasing-step-size&quot;&gt;[6.1] Constant vs. decreasing step size&lt;/h3&gt;
&lt;p&gt;이 실험에서는 ridge regression과 logistic regression 문제에 집중했는데, 여기서는 목적 함수가 strongly convex이다. Thm 3.2에서 예상했듯이, 특정 시점에 step size를 줄이면 성능이 더 좋다는 것을 보여준다.&lt;/p&gt;

&lt;h3 id=&quot;62-minibatches&quot;&gt;[6.2] Minibatches&lt;/h3&gt;
&lt;p&gt;이 실험에서는 [3.2]에 소개된 여러 다른 분포의 $\mathcal{D}$(single element sampling, $\tau$ independent sampling, $\tau$-nice sampling)를 선택하여 SGD의 수렴도를 비교하였는데, $\tau^* $를 이용한 방법이 가장 좋은 결과를 보였다. &lt;em&gt;여기 비교했다는건 6갠데 그래프에는 왜 5개만 있음?&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;63-sum-of-non-convex-functions&quot;&gt;[6.3] Sum-of-non-convex functions&lt;/h3&gt;
&lt;p&gt;이 실험에서는 PCA 문제에 집중했는데, 여기서는 $f$는 strongly convex지만 $f_i$는 그렇지 않다. non-convex한 $f_i$에서도 위와 마찬가지로, stepsize를 줄였을 때 더 좋은 성능을 보였고 $\tau^* $가 가장 좋은 성능을 보였다.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;아직 잘 모르겠는 개념 : $\mu$-strongly quasi-convx, condition number, uniform sampling&lt;/p&gt;
</description>
        <pubDate>Sat, 02 Nov 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/thesis/2019/11/02/SGDGENANAL.html</link>
        <guid isPermaLink="true">http://localhost:4000/thesis/2019/11/02/SGDGENANAL.html</guid>
        
        <category>Optimization</category>
        
        
        <category>Thesis</category>
        
      </item>
    
      <item>
        <title>ADAM - A Method for Stochastic Optimization</title>
        <description>&lt;hr /&gt;

&lt;h2 id=&quot;abstract&quot;&gt;[Abstract]&lt;/h2&gt;

&lt;p&gt;이 논문에서는 저차를 이용한 예측에 기반한 확률적 목적함수의 1차 기울기 기반의 최적화 알고리즘인 Adam을 소개한다. 이 방법은 구현하기에 직접적이고, 계산량이 효과적이고, 메모리가 적게 필요하고, 기울기를 diagonal rescaling하는데 변함이 없고, 큰 데이터나 파라미터에 적용하는 데에 적합하다. 이 방법은 또한 매우 노이즈가 많거나 기울기가 sparse한 등의 변화하는 목적이나 문제에 적합하다. 조정 변수는 직관적으로 해석될 수 있으며 약간의 전형적인 튜닝만을 필요로 한다. Adam을 만드는 데에 영감을 준 관련된 알고리즘과의 연결고리도 소개된다. 또한 알고리즘의 이론적인 수렴 성질도 분석했고, 알려진 최고의 알고리즘들과 비교하여 수렴도에 대한 regret bound도 제공한다. 실험 결과는 아담이 다른 최적화 방법들과 비교해도 잘 작동한다는 것을 보여준다. 마지막으로, AdaMax에 대해서도 논의할 것이다.&lt;/p&gt;

&lt;h2 id=&quot;1-introduction&quot;&gt;[1] Introduction&lt;/h2&gt;

&lt;p&gt;1차 기반 최적화는 널리 쓰였는데, 계산량에서 효율적이다. SGD도 많이 쓰이는데, 이때 목적함수는 data subsampling 말고 또다른 노이즈 소스(dropout같은)가 있을 수 있다. 따라서 noisy할 경우에 대해 더 효과적인 알고리즘이 필요하다. 이 논문은 고차원 파라미터 공간에서의 SGD 최적화와 1차 최적화 방법에 중점을 둔다.&lt;/p&gt;

&lt;p&gt;Adam은 1차 방법인데, 적은 메모리를 사용하여 효율적인 확률론적 최적화를 한다. 이 방법은 각각의 파라미터에 대해 gradient의 첫번째와 두번째 moment의 추정치를 이용하여 최적의 learning rate를 계산한다. 또한 AdaGrad(sparse gradient에 효과적)와 RMSProp(on-line, non-stationary에 효과적)의 장점을 합친 방법이다.&lt;/p&gt;

&lt;p&gt;Adam의 장점은 엄청난 수의 파라미터 업데이트가 기울기를 rescaling해도 변화가 없다는 것, step size가 제한되는 것, 불변 목적함수가 필수가 아닌 것, sparse한 기울기에서도 잘 작동하는 것, 그리고 step size annealing을 자동으로 하는 것이다.&lt;/p&gt;

&lt;h2 id=&quot;2-algorithm&quot;&gt;[2] Algorithm&lt;/h2&gt;

&lt;p&gt;$\alpha$ : step size&lt;br /&gt;
$\beta_1, \beta_2$ : moment 예측의 지수 decay rate&lt;br /&gt;
$f(\theta)$ : 목적 함수&lt;br /&gt;
$\theta_0$ : 초기 파라미터&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}\text{while }&amp;\theta_t\text{ not converged, do}\\
&amp; t\leftarrow t+1\\
&amp; g_t\leftarrow\triangledown_\theta f_t(\theta_{t-1})\\
&amp; m_t\leftarrow\beta_1\cdot m_{t-1}+(1-\beta_1)\cdot g_t\\
&amp; v_t\leftarrow\beta_2\cdot v_{t-1}+(1-\beta_2)\cdot g_2^t\\
&amp; \hat{m_t}\leftarrow\frac{m_t}{1-\beta_1^t}\\
&amp; \hat{v_t}\leftarrow\frac{v_t}{1-\beta_2^t}\\
&amp; \theta_t\leftarrow\theta_{t-1}-\alpha\cdot\frac{\hat{m_t}}{\sqrt{\hat{v_t}}+\epsilon}\\
\text{end while}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;알고리즘은 기울기와 기울기의 제곱의 지수평균을 업데이트하고, hyper-parameter $\beta_1, \beta_2$는 이 이동 평균의 지수적인 decay rate을 조절한다. 이 두 움직임의 평균은 기울기의 1차와 2차 모멘트로부터 예측된다. 0으로 초기화 되기 때문에 초기 시점에나 $\beta_1, \beta_2$가 작을 때 0으로 bias되는 경우가 생기는데, 뒤에서 상쇄가 가능하다. 위 알고리즘 1의 뒤쪽에 약간의 순서 변경을 하면 표현의 명확성이 떨어지지만, 효율성을 더 확보할 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;21-adams-update-rule&quot;&gt;[2.1] Adam’s update rule&lt;/h3&gt;
&lt;p&gt;Adam의 특징 중 하나는 업데이트할 때 stepsize를 신중하게 선택해야 한다는 점이다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\triangle t=\alpha \cdot \frac{\hat{m}_t}{\sqrt{\hat{v}_t}}&lt;/script&gt;

&lt;p&gt;이게 effective한 step 이다. 이 stepsize는 $(1-\beta_1)\gt\sqrt{1-\beta_2}$일 경우($|\triangle_t|\le\alpha\cdot\frac{(1-\beta_1)}{\sqrt{1-\beta_2}}$)와 이 경우가 아닐 경우($|\triangle_t|\le\alpha$)에 대해 두 개의 upper bound가 있다. 첫번째 경우는 현재 timestep 제외하고 대부분 gradient가 0일 경우 같은 매우 sparse한 경우이고, 두번째 경우는 덜 sparse한 경우인데, 효과적인 step size가 줄어든다.&lt;/p&gt;

&lt;p&gt;일반적인 경우는 후자인데, 결국 effective stepsize는 $\alpha$에 의해 bound된다. 현재 기울기 예측이 충분한 정보를 주지 않더라도 trust region을 구축하기 위한 것이라고 이해될 수 있다. 따라서 $\alpha$의 적절한 크기를 아는 것이 상대적으로 쉽다. (보통 머신러닝 모델에서 우리는 적절한 파라미터 값을 대략적으로 알고 있다.)&lt;/p&gt;

&lt;p&gt;약간 과장하면, $\frac{\hat{m}_t}{\sqrt{\hat{v}_t}}$ 를 signal-to-noise ratio(SNR)이라고 할 수 있다. SNR이 더 작으면 $\triangle_t$는 0에 더 가까워진다. SNR이 작다는 말은 $\hat{m_t}$가 진짜 기울기 방향으로 가고 있다는 데에 더 큰 불확실성을 갖는다는 말, 즉 현재 optima에 거의 다 왔으므로 어디로 갈지 모르는 상황이 되어 effective stepsize가 더 작아진다. 따라서 자동 annealing의 형태를 띤다. 또한 $\triangle_t$는 gradient의 scale이 변해도 변하지 않는다(분수이므로 scale된것이 사라진다).&lt;/p&gt;

&lt;h2 id=&quot;3-initialization-bias-correction&quot;&gt;[3] Initialization Bias Correction&lt;/h2&gt;

&lt;p&gt;우리는 제곱된 기울기의 지수 평균과 decay rate $\beta_2$를 이용하여 f의 2차 moment를 알아내려고 한다. 먼저 지수 이동 평균을 0으로 초기화한다. 위 알고리즘의 식에 대입하면, 다음 지수 이동 평균을 구할 때 이전의 gradient가 $\beta_2$의 지수만큼 기여하게 된다. $\mathbb{E}[v_t]$와 $\mathbb{E}[g_t^2]$와의 관계성을 알면 그 차이를 없앨 수 있다. 알고리즘의&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;v_t = (1-\beta_2)\sum_{i=1}^t\beta_2^{t-i}\cdot g_i^2&lt;/script&gt;

&lt;p&gt;식에 평균을 취하면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbb{E}[v_t] = \mathbb{E}[g^2_t]·(1−\beta_t^2) +\zeta&lt;/script&gt;

&lt;p&gt;이 식을 얻을 수 있다. 만약 $g_t$가 변하지 않는 값이면 $\zeta=0$이 되는데, 만약 $g_t$가 변하더라도 아주 이전의 gradient에는 작은 계수가 붙기 때문에, $\zeta$는 아주 작은 숫자가 된다. $1-\beta_2^t$는 running average를 0으로 초기화하면서 생긴 텀인데, 초기 bias를 없애주기 위해 알고리즘 1에서는 bias correction을 위해 이 식으로 나눠준다.&lt;/p&gt;

&lt;p&gt;gradient가 sparse할 경우에는, 믿을만한 2차 모멘트의 예측을 위해서는 작은 $\beta_2$를 선택하여 많은 기울기를 평균해야 한다. 그러나 작은 $\beta_2$를 선택하면 초기 bias의 correction이 없게 되므로(bias correction은 $1-\beta_2$로 나누는 것이므로, 작은 $\beta_2$를 선택할 경우 나누는 항이 1이 되어 bias correction이 줄어든다) sparse하지 않을 경우보다 초기 step이 더 커지게 된다. (앞의 [2.1] stepsize의 bound 식을 보면 이 경우의 bound가 더 크다)&lt;/p&gt;

&lt;h2 id=&quot;4-convergence-analysis&quot;&gt;[4] Convergence Analysis&lt;/h2&gt;

&lt;p&gt;각 시간 t에 대해, 우리의 목적은 $\theta_t$를 예측하고 그 예측을 unknown cost function $f_t$에 대해 평가하는 것이다. sequence가 이전에 알려져있지 않기 때문에 regret이라는 것을 사용해서 평가한다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;R(T) = \sum_{t=1}^T[f_t(\theta_t)-f_t(\theta^* )]&lt;/script&gt;

&lt;p&gt;이게 regret 식이다. Adam은 $O(\sqrt{T})$의 regret bound 갖고 있다. 이 결과는 현존하는 online learning중 최고이다.&lt;/p&gt;

&lt;h3 id=&quot;thm41&quot;&gt;[Thm4.1]&lt;/h3&gt;

&lt;p&gt;Thm 1은 $\alpha_t$가 $t^{-1/2}$로 decay하고, $\beta_1$이 $\lambda^t$ 로($\lambda$는 $1$에 엄청나게 가까운 수)로 decay할 때 성립한다. Thm1은 데이터가 sparse하고 제한된 기울기를 갖고 있을 때, $\sum$ 항이 그 상한보다 훨씬 작다는 것을 의미한다(특정한 함수와 데이터 특징에 대해). 이 결과에 기댓값($\mathbb{E}$)을 적용하면 Adam에도 적용될 수 있다. Adaptive method는 non-adaptive 모델보다 더 좋은 convergence를 갖는다. $\beta_1$의 decay가 중요한 역할을 하는데, 이전에 있었던 모멘텀 계수를 줄이는 것이 수렴을 향상시킨다는 연구결과와도 들어맞는다.&lt;/p&gt;

&lt;h3 id=&quot;cor42&quot;&gt;[Cor4.2]&lt;/h3&gt;

&lt;p&gt;Thm4.1의 따름정리인데, 결국은 $\frac{R(T)}{T}$ 가 0으로 수렴한다는 내용이다.&lt;/p&gt;

&lt;h2 id=&quot;5--related-work&quot;&gt;[5]  Related Work&lt;/h2&gt;

&lt;p&gt;Adam에 직접적으로 영향을 준 연구는 RMSProp과 AdaGrad이다. vSGD, AdaDelta, natural Newton Method같은 stochastic 방법들은 1차 미분 정보에서 곡률을 예측하여 stepsize를 정한다. SFO는 mini-batch 기반의 quasi-newton 방법인데, Adam과 달리 선형적인 메모리가 필요해 GPU에서는 사용할 수 없다. NGD와 같이 Adam은 데이터의 구조에 적응하는 preconditioner를 도입했다(Adam에서의 $\hat{v_t}$는 Fisher information matrix의 근사이다). 그러나 Adam의 preconditioner는 AdaGrad에서와 비슷한데, 그냥 NGD보다 더 적응이 느리다.&lt;/p&gt;

&lt;h3 id=&quot;기본지식&quot;&gt;[기본지식]&lt;/h3&gt;

&lt;p&gt;newton’s method : GD랑 비슷하지만 $x_n+1 = x_n - \frac{f’}{f’’}$ 으로 업데이트 하는 방법&lt;br /&gt;
quasi-newton method : newton’s method와 비슷하지만 계산량이 훨씬 적다.$f’’$ 대신 $f’‘$을 근사한 행렬을 사용한다.&lt;/p&gt;

&lt;h3 id=&quot;rmsprop&quot;&gt;[RMSProp]&lt;/h3&gt;

&lt;p&gt;모멘텀을 이용한 RMSProp에서는 rescale된 gradient에 momentum을 사용하여 파라미터 업데이트를 하는데, Adam은 현재 기울기의 1차, 2차 momentum의 평균을 이용하여 바로 예측한다(rescale이 없음). 또한 RMSProp은 bias-correction 항이 없다. 이러면 너무 큰 stepsize나 발산으로 이어질 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;adagrad&quot;&gt;[AdaGrad]&lt;/h3&gt;

&lt;p&gt;sparse한 gradient에 잘 작동하는 알고리즘이다. Adam에서 $\alpha$와 $\beta$를 적절하게 설정하면 AdaGrad와 똑같다. bias-correction 항이 없으면 비슷하지 않게 된다. 과거의 기울기의 변화량(momentum)을 참고하여 update하는 것이 특징이다.&lt;/p&gt;

&lt;h2 id=&quot;6-experiments&quot;&gt;[6] Experiments&lt;/h2&gt;

&lt;p&gt;큰 모델과 데이터셋을 사용하므로써, Adam이 현실에서도 효과적이라는 것을 증명한다. hyper-parameter의 경우는, dense grid로 찾고, 가장 좋은 결과를 내는 hyper-parameter를 비교에 사용한다.&lt;/p&gt;

&lt;h3 id=&quot;61-experiment--logistic-regression&quot;&gt;[6.1] Experiment : Logistic Regression&lt;/h3&gt;

&lt;p&gt;L2 정규화된 multi-class logistic regression을 MNIST를 이용하여 평가했다. logistic regression은 convex한 목적함수를 갖고 있다. Adam은 모멘텀을 사용한 SGD와 비슷하게 수렴했고 Adagrad보다는 빨랐다.&lt;/p&gt;

&lt;p&gt;AdaGrad는 feature와 gradient가 sparse할 때 유리하다. Adam이 $\frac{1}{\sqrt{t}}$의 stepsize를 사용한다면 이론적으로 AdaGrad의 성능과 같다. 이 부분에 대해서는 IMDB 영화 리뷰 데이터셋을 이용해 sparse feature 문제를 시험했다. 결과는 이론과 같았다.&lt;/p&gt;

&lt;h3 id=&quot;62-experiment--multi-layer-neural-networks&quot;&gt;[6.2] Experiment : Multi-Layer Neural Networks&lt;/h3&gt;

&lt;p&gt;여기서 non-convex한 함수에 대한 분석은 이루어지지 않았지만, 경험적으로 Adam이 이런 상황에서도 좋은 성능을 낸다는 것을 알았다. 우선 기본 deterministic cross-entropy 목적함수와 L2 weight decay를 사용하여 여러 다른 optimizer들을 연구했다. SFO 방법은 최근에 제안된 minibatch 기반의 quasi-Newton 방법인데,  multi-layer NN에서 좋은 성능을 보여준다. 그런데 Adam의 iteration이 덜 필요했고, 절대적인 시간도 빨랐다. (곡선 정보를 update하는 데에 SFO는 시간이 더 필요하고, linear memory가 필요하다.) 또한 dropout을 사용한 다른 확률론적 방법들과의 비교에서도 Adam이 월등한 성적을 보여주었다.&lt;/p&gt;

&lt;h3 id=&quot;63-experiment--convolutional-neural-networks&quot;&gt;[6.3] Experiment : Convolutional Neural Networks&lt;/h3&gt;

&lt;p&gt;CNN에서도 Adam이 효율적으로 작동할 수 있다. 처음에는 Adam과 AdaGrad가 빠른데, 나중에는 SGD와 Adam이 잘 수렴한다. 이 이유는 2차 moment 예측 $\hat{v_t}$가 몇 epoch뒤에는 사라지고 $\epsilon$이 성능을 좌우하기 때문이다. fully connected NN과 비교할 때, CNN에서는 2차 moment 예측은 cost function의 지형을 근사하는 데에 실패한 것이다. CNN에서는 1차 moment를 통해 minibatch의 분산을 줄이는 것이 더 중요하다. Adam은 SGD보다 약간 더 좋은데, 또한 SGD처럼 수동으로 learning rate를 조절해야 하는 것이 아니라 자동으로 서로 다른 layer들에 대한 learning rate를 조절해준다.&lt;/p&gt;

&lt;h3 id=&quot;64-experiment--bias-correction-term&quot;&gt;[6.4] Experiment : Bias-Correction Term&lt;/h3&gt;

&lt;p&gt;bias term은 Adam에서 매우 중요하며 bias correction term을 없애는 것은 momentum을 사용한 RMSProp과 같은 결과를 보여준다. 이 논문에서는 $\beta_1$과 $\beta_2$, $\alpha$를 다양하게 바꾸며 실험했다. $\beta_2$가 1에 가까우면 sparse한 gradient에 견고성이 더 요구되므로 더 큰 초기 bias가 생긴다. 따라서 이 논문에서는 느린 decay같은 상황에서 bias correction term이 중요하다고 예상했고, 실험 결과에서는 $\beta_2$가 1에 가깝고 bias term이 있는 것이 가장 성능이 좋았다(예상한 것이 맞았다). 결과적으로, Adam은 RMSProp보다 같거나 더 좋다.&lt;/p&gt;

&lt;h2 id=&quot;7-extensions-안읽었음&quot;&gt;[7] Extensions &lt;em&gt;안읽었음&lt;/em&gt;&lt;/h2&gt;

&lt;h2 id=&quot;8-conclusion&quot;&gt;[8] Conclusion&lt;/h2&gt;

&lt;p&gt;이 논문에서는 간단하고 계산효율적인 기울기 기반의 확률 목적 함수 최적화 알고리즘을 소개하였다. 이 방법은 큰 데이터셋과 고차 파라미터 공간을 위한 것이다. Adam은 두 유명한 최적화 방법인 AdaGrad 와 RMSProp의 장점을 결합하여 만들어졌는데, AdaGrad의 sparse한 기울기에 대한 장점과 RMSProp의 변동 목적함수에 대한 장점이다. 이 방법은 구현이 직관적이고 적은 메모리를 사용한다. 실험은 convex 문제에서의 수렴도 해석을 증명해주었다. 종합하면, Adam은 딥러닝에서의 다양한 non-convex 최적화 문제에서 강건하고 잘 맞는다.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;다음에 읽어볼 논문 : &lt;a href=&quot;http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf&quot;&gt;AdaGrad&lt;/a&gt;, RMSProp&lt;/p&gt;
</description>
        <pubDate>Wed, 30 Oct 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/thesis/2019/10/30/ADAM.html</link>
        <guid isPermaLink="true">http://localhost:4000/thesis/2019/10/30/ADAM.html</guid>
        
        <category>Optimization</category>
        
        
        <category>Thesis</category>
        
      </item>
    
      <item>
        <title>SGDR - Stochastic Gradient Descent with warm Restarts</title>
        <description>&lt;hr /&gt;

&lt;h2 id=&quot;abstract&quot;&gt;[Abstract]&lt;/h2&gt;
&lt;p&gt;재시작 방법은 gradient-free 최적화에서 멀티모달 함수에 적용할 때 자주 쓰인다. 부분적 재시작 또한 gradient기반 최적화에서 ill-conditioned 함수에서 수렴도를 개선하기 위해 자주 쓰이는 추세이다. 이 논문에서는 SGD를 위한 간단한 재시작 테크닉을 소개하는데, 딥네트워크를 학습시킬 때 항상(anytime) 결과를 향상시킬 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;1-introduction&quot;&gt;[1] Introduction&lt;/h2&gt;
&lt;p&gt;GD를 쓸때 hessian을 쓰면 더 좋은데 계산량이 많다. AdaDelta와 Adam은 hessian을 잘 줄여서 사용한 좋은 예이다. 그런데 sota 결과는 사실 특별한 방법을 쓴게 아니라 SGD에 momentum만 추가한 것이었다.&lt;/p&gt;

&lt;p&gt;보통의 learning rate schedule은 정해진 상수를 일정 간격의 상수로 나누는 것이었는데, 이 논문에서 제안하는 새로운 learning rate schedule은 주기적으로 SGD를 재시작하는 방법이다. 실험 결과에 의하면 재시작 방법은 원래 쓰이던 방법보다 2배에서 4배정도 적은 epoch만으로 비슷한 결과를 낼 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;2-related-work&quot;&gt;[2] Related Work&lt;/h2&gt;
&lt;p&gt;gradient-free optimization에서는 많은 local optima를 찾는 것이 목적이다. niching 방법 기반 방법들은 local optimizer를 전체 space에 다 적용시킬 수 있는데, 차원의 저주 때문에 확장시킬 수는 없다. 최근에는 다양한 재시작 매커니즘들을 사용하는데, 한 방법에서는 많은 후보를 쓰면 더 글로벌한 검색이 가능한데, 각 재시작 처음엔 적은 후보를 쓰고 각 재시작 후에는 키우는 방법을 사용하는 것이 일반적이다.&lt;/p&gt;

&lt;p&gt;gradient-based optimization에서는 gradient-free 에서보다 $n$배의 속도 향상이 있다. 이때 재시작 방법은 multimodality를 해결하기 위함보다는 수렴도를 개선하기 위해 사용된다.&lt;/p&gt;

&lt;h2 id=&quot;3-sgdr&quot;&gt;[3] SGDR&lt;/h2&gt;
&lt;p&gt;현존하는 재시작 방법은 SGD에도 적용될 수 있다. 데이터의 덩어리에 따라 loss value와 기울기가 다양할 수 있어서, 기울기나 loss의 평균을 내는 등의 노이즈의 제거가 필요하다.&lt;/p&gt;

&lt;p&gt;이 논문에서는 지정된 epoch까지 도달하면 다시 재시작을 하는 가장 간단한 재시작 방법을 사용한다. 그리고 제안된 cosine annealing이라고도 불리는 learning rate schedule은 다음과 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\eta_t=\eta_{min}^i+\frac{1}{2}(\eta_{max}^i−\eta_{min}^i)(1 + \cos(\frac{T_{cur}}{T_i}\pi))&lt;/script&gt;

&lt;p&gt;여기서 $\eta_t$는 learning rate, $\eta_{min}^i$와 $\eta_{max}^i$는 learning rate의 범위, $T_{cur}$는 현재 epoch, $T_i$는 지정된 epoch(이만큼 지나면 재시작)이다.&lt;/p&gt;

&lt;p&gt;재시작은 learning rate ($\eta_t$)을 증가시키므로써 수행되고, $x_t$는 초기 해로 사용된다. learning rate는 $\eta_{max}^i$부터 $\eta_{min}^i$까지 줄어들고, 정해진 epoch를 돌면 다시 처음부터 시작한다. $T_{mult}$라는 변수를 이용하여 재시작마다 줄어드는 간격을 점점 넓힐 수도 있다.&lt;/p&gt;

&lt;p&gt;처음 재시작 전에는 $x_t$를 초기 해로 사용하지만, 그 다음에는 이전의 최소 learning rate로부터 얻어진 $x$를 초기 해로 사용한다. (이 점이 중요한 부분임)
&lt;em&gt;그런데 계속 저렇게 사용하는거? 아님 재시작마다 저렇게 사용하는거?&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;4-experimental-results&quot;&gt;[4] Experimental Results&lt;/h2&gt;
&lt;h3 id=&quot;42-single-model-result&quot;&gt;[4.2] Single-Model Result&lt;/h3&gt;
&lt;p&gt;$T_0=200$의 결과가 가장 좋은데, 가장 마지막 몇 epoch에서 좋아진다. $T_{mult}=2$는 재시작 후 주기를 2배로 늘려주는데, 이렇게 하는 이유는 좋은 테스트 에러에 가장 빨리 도달하기 위함이다. SGDR이 좋은 성능에 빠르게 도달할 수 있기 때문에, 더 큰 신경망을 학습시킬 수 있다. 따라서 WRN을 2배 넓게 만들어서 학습시켰다.&lt;/p&gt;

&lt;p&gt;SGDR 자체 실험에서는 SGDR과 기본 스케줄을 비교했는데, 120 epoch까지는 더 빨리 training loss가 줄었다. 이 이후에 기본 스케줄은 overfit되었다. 결론적으로, SGDR은 overfit이 잘 되지 않는다.&lt;/p&gt;

&lt;h3 id=&quot;43-ensemble-results&quot;&gt;[4.3] Ensemble Results&lt;/h3&gt;
&lt;p&gt;SGDR은 WRN논문의 follow-up study에서 영감을 얻었다.  여기서는 재시작 전마다 snapshot을 찍고 그것으로 앙상블 모델을 만든다. 결과로는, 3번 돌려서 앙상블하는 것보다 한번 돌려서 3번 스냅샷 찍어서 앙상블하는게 낫다. SGDR에서 찍은 스냅샷은 앙상블을 할 때의 유용한 다양성을 제공해 준다. 이 결과는 WRN보다 더 좋은 모델에서 더 좋은 결과를 낼 것이다.&lt;/p&gt;

&lt;h3 id=&quot;45-preliminary-experiments-on-a-downsampled-imagenet-dataset&quot;&gt;[4.5] Preliminary Experiments on a Downsampled ImageNet Dataset&lt;/h3&gt;
&lt;p&gt;다운샘플된 이미지넷 데이터는 원래보다 더 어렵고 이미지의 대부분을 대상이 차지하는 CIFAR-10보다도 더 어렵다.&lt;/p&gt;

&lt;h2 id=&quot;5-discussion&quot;&gt;[5] Discussion&lt;/h2&gt;
&lt;p&gt;이 learnin rate schedule은 재시작 없이도 충분히 경쟁적이고, 단 두 개의 파라미터(초기 lr, epoch 수)만을 필요로 한다. 재시작 방법의 목적은 ‘항상(anytime)’ 좋은 성능을 내기 위함이다. 매 재시작마다 $\eta_{max}$와 $\eta_{min}$을 줄이는 방법도 가능하다. SGDR 중 얻어진 중간 모델은 앙상블에 사용될 수 있고, cost도 들지 않는다는 점을 이용했다.&lt;/p&gt;

&lt;h2 id=&quot;6-conclusion&quot;&gt;[6] Conclusion&lt;/h2&gt;
&lt;p&gt;WRN에서는 더 넓은 모델을 사용하고 스냅샷을 앙상블에 사용해 sota 결과를 만들어냈고, EEG에서는 더 재시작을 많이 하고 더 스냅샷을 많이 찍으면 더 좋은 결과를 낸다는 것을 알았다. Downsampled ImageNet 데이터에서는 SGDR이 scan을 통해 lr을 선택하는 문제를 줄여준다는 것을 알았다. 다음 연구는 AdaDelta나 Adam에 적용하는 것이 될 것이다.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;다음에 읽어볼 논문 : &lt;a href=&quot;https://arxiv.org/abs/1704.00109&quot;&gt;Snapshot&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1605.07146&quot;&gt;WRN&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Sun, 27 Oct 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/thesis/2019/10/27/SGDR.html</link>
        <guid isPermaLink="true">http://localhost:4000/thesis/2019/10/27/SGDR.html</guid>
        
        <category>Optimization</category>
        
        
        <category>Thesis</category>
        
      </item>
    
  </channel>
</rss>
