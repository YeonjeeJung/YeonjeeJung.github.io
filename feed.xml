<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>YeonjeeJung's Blog</title>
    <description></description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sun, 04 Aug 2019 15:47:13 +0900</pubDate>
    <lastBuildDate>Sun, 04 Aug 2019 15:47:13 +0900</lastBuildDate>
    <generator>Jekyll v3.8.6</generator>
    
      <item>
        <title>CV Lecture 9 - High-Dynamic-Range Imaging</title>
        <description>&lt;h1 id=&quot;high-dynamic-range-imaging&quot;&gt;High-Dynamic-Range Imaging&lt;/h1&gt;

&lt;h3 id=&quot;high-dynamic-range-imaging-ldrrightarrowhdr&quot;&gt;High Dynamic Range Imaging (LDR$\rightarrow$HDR)&lt;/h3&gt;

&lt;p&gt;현실은 dynamic range가 매우 높다. 이를 낮은 대비를 가지는 사진으로 옮기려면 전체 range의 간격을 좁히거나, 특정 범위만 옮기는 방법이 있다. 이 때 어떤 범위를 저장할 것인가는 노출(&lt;a href=&quot;https://yeonjeejung.github.io/lecture/computervision/2019/07/28/Lecture7.html&quot;&gt;exposure&lt;/a&gt;)이 결정하는데, 이 노출을 조절하는 방법에는 서터 속도, 조리개, 감도, 중성 농도 필터 등이 있다.&lt;/p&gt;

&lt;p&gt;셔터 속도의 조절 범위는 30sec~1/4000sec이고, 확실한 방법이고 선형적인 효과를 가져오지만, 긴 노출에 대해서는 노이즈가 섞일 우려가 있다.&lt;/p&gt;

&lt;p&gt;조리개의 조절 범위는 f/1.4~f/22이고 DOF가 변경된다는 단점이 있고, 최후의 방법으로 유용하다.&lt;/p&gt;

&lt;p&gt;감도의 조절 범위는 100~1600이고, 노이즈가 생긴다는 단점이 있다. 조리개값과 마찬가지로 최후의 방법으로 유용하다.&lt;/p&gt;

&lt;p&gt;중성 농도 필터는 4농도까지 조절할 수 있으며, 쌓을 수도 있다. 단점은 완벽히 중성은 아니고 색감 이동이 있을 수 있고, 정확하지는 않다. 장점은 번쩍거리는 빛에도 사용할 수 있다는 점이고, 최후의 보충 방법으로 사용될 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;response-curve&quot;&gt;Response Curve&lt;/h3&gt;

&lt;p&gt;만약 response curve를 알고 있다면 어떤 노출값을 갖는 사진을 찍어야 원하는 빛의 양을 얻을 수 있는지 알 수 있다. response curve의 역 값을 보면 된다. response curve를 얻는 방법에는 두가지가 있다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;장면의 빛의 양(scene luminance)을 조절한 뒤 픽셀값을 본다.&lt;/li&gt;
  &lt;li&gt;노출값을 조절한 뒤 픽셀값을 본다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;가장 좋은 방법은 노출값을 조절한 뒤 많은 범위의 픽셀값을 고루 관찰하는 것이다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Z=f(H)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H=E\cdot\Delta t&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\log(H)=\log(E)+\log(\Delta t)&lt;/script&gt;

&lt;p&gt;$Z$ 는 픽셀값, $H$는 노출, $E$는 빛의 양, $\Delta t$는 노출시간이다.&lt;/p&gt;

&lt;p&gt;$g(Z)$가 response curve의 역에 $\log$를 취한 함수라고 하자.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Z=f(H)\rightarrow H=f^{-1}(Z)\rightarrow \log H=\log f^{-1}(Z)\rightarrow\log H=g(Z)&lt;/script&gt;

&lt;p&gt;$j$번째 이미지의 $i$번째 픽셀에 대해서&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;g(Z_{ij})=\log(E_i)+\log(\Delta t_j)&lt;/script&gt;

&lt;p&gt;임을 원하기 때문에&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{i=1}^N\sum_{j=1}^P[\log(E_i)+\log(\Delta t_j)-g(Z_{ij})]^2 + \lambda\sum_{z=Z_{min}}^{Z_{max}}g''(z)^2&lt;/script&gt;

&lt;p&gt;을 최소화시키면 된다. 앞의 항은 Data term, 뒤의 항은 Regularization term이다.&lt;/p&gt;

&lt;p&gt;radiance를 재건하려면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\log E_i=\frac{\sum_{j=1}^Pw(Z_{ij})(g(Z_{ij})-\log(\Delta t_j))}{\sum_{j=1}^Pw(Z_{ij})}&lt;/script&gt;

&lt;p&gt;을 사용하면 된다. $w(Z_{ij})$는 해당 픽셀의 가중치이고, 해당 조각들의 노이즈 관여를 조절한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://cybertron.cg.tu-berlin.de/eitz/hdr/exposure_series_response_curve.jpg&quot; alt=&quot;&quot; /&gt;
&lt;a href=&quot;http://cybertron.cg.tu-berlin.de/eitz/hdr/exposure_series_response_curve.jpg&quot;&gt;Source&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;response curve를 그리기 위해 사용하는 이미지&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://cybertron.cg.tu-berlin.de/eitz/hdr/response_curve.jpg&quot; alt=&quot;&quot; /&gt;
&lt;a href=&quot;http://cybertron.cg.tu-berlin.de/eitz/hdr/response_curve.jpg&quot;&gt;Source&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;위 이미지를 통해 그린 response curve&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://vision.gel.ulaval.ca/~jflalonde/cours/4105/h14/tps/results/tp5/raziehtoony/Image/Radiance%20Map.jpg&quot; alt=&quot;&quot; /&gt;
&lt;a href=&quot;http://vision.gel.ulaval.ca/~jflalonde/cours/4105/h14/tps/results/tp5/raziehtoony/Image/Radiance%20Map.jpg&quot;&gt;Source&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;디지털 카메라를 이용해서 장면 밝기 (scene radiance)를 측정한 radiance map&lt;/p&gt;

&lt;h3 id=&quot;image-registration&quot;&gt;Image Registration&lt;/h3&gt;

&lt;p&gt;서로 다른 노출을 갖는 이미지를 비교하기 위한 방법이다. Median-Threshold Bitmap (MTB)를 이용해서 흑백으로 만들고, &lt;strong&gt;&lt;em&gt;그 차이를 최소화하는 translation을 찾는다. 그리고 피라미드를 이용해서 더 촉진한다?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;file-formats&quot;&gt;File Formats&lt;/h3&gt;

&lt;p&gt;.pfm : Portable Float Map. 채널당 32비트 float을 사용하고, 3채널을 저장한다. 따라서 픽셀당 96비트를 사용한다.&lt;/p&gt;

&lt;p&gt;.pic, .hdr : Radiance Format. 픽셀당 32비트를 사용한다. (채널당 8비트, 지수에 8비트)&lt;/p&gt;

&lt;p&gt;.exr : ILM’s OpenEXR. 비손실 압축을 지원한다. half-precision float (채널당 16비트)을 사용하며, 멀티채널을 지원한다.&lt;/p&gt;

&lt;h3 id=&quot;high-dynamic-range-display-hdrrightarrowhdr&quot;&gt;High Dynamic Range Display (HDR$\rightarrow$HDR)&lt;/h3&gt;

&lt;p&gt;Sunnybrook HDR Display는 저해상도의 광원과 2개의 8비트 모듈레이터를 이용하여 HDR 영상을 띄울 수 있다. LED 백라이트와 LCD 스크린을 결합하여 결과를 만들어 낸다.&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;terms&quot;&gt;Terms&lt;/h2&gt;
&lt;p&gt;dynamic range : 증폭 회로 등에서, 다룰 수 있는 가장 큰 신호와 가장 작은 신호와의 크기의 비율을 데시벨로 나타낸 것&lt;br /&gt;
precision : 정밀도&lt;/p&gt;
</description>
        <pubDate>Sun, 04 Aug 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/lecture/computervision/2019/08/04/Lecture9.html</link>
        <guid isPermaLink="true">http://localhost:4000/lecture/computervision/2019/08/04/Lecture9.html</guid>
        
        
        <category>Lecture</category>
        
        <category>ComputerVision</category>
        
      </item>
    
      <item>
        <title>CV Lecture 8 - Light-Field Imaging</title>
        <description>&lt;h1 id=&quot;lightfields&quot;&gt;Lightfields&lt;/h1&gt;

&lt;h3 id=&quot;ray&quot;&gt;Ray&lt;/h3&gt;

&lt;p&gt;Ray는 5D 정보로 이루어져 있다.(3D 위치, 2D 방향) plenoptic 기술은 이런 5D정보를 가진 ray들을 활용하여 다양한 영상처리를 할 수 있다.&lt;/p&gt;

&lt;p&gt;반면, line은 4D의 정보를 가지고 있다. (2D 위치, 2D 방향) &lt;strong&gt;&lt;em&gt;discretize 한 후에 다시 interpolate 할 수 있다?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;image&quot;&gt;Image&lt;/h3&gt;
&lt;p&gt;이미지란, 눈(또는 렌즈)이라는 한 점으로 들어오는 빛의 정보들을 수치화한 것이다. 앞 단원들에서 계속 나오던 &lt;a href=&quot;https://yeonjeejung.github.io/lecture/computervision/2019/07/07/Lecture3.html&quot;&gt;이미지 평면&lt;/a&gt;도 결국에는 사물에서 반사된 빛이 눈이라는 한 점으로 들어오는 도중의 한 평면과의 교점들이라고 할 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;object&quot;&gt;Object&lt;/h3&gt;

&lt;p&gt;하지만 대상에서 반사되는 모든 빛을 알 수는 없다. 이미지란 대상의 모든 곳에서 반사되는 빛 중 렌즈로 들어오는 빛만 캡쳐한 것이다. 그러므로 정보의 손실이 일어나게 되는데, 이 정보들 (4D정보들)을 다 모으면 렌즈 방향이 아닌 다른 방향에서 어떻게 보일지도 알 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;lumigraph&quot;&gt;Lumigraph&lt;/h3&gt;

&lt;p&gt;Lumigraph는 위에서 설명한 빛의 정보를 저장하는 방법이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.lightfield-info.com/lightfield-image.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.lightfield-info.com/lightfield-image.jpg&quot;&gt;Source&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;2개의 평면을 이용할 수 있다. 한 평면은 대상이 존재하는 평면($u, v$평면)이고, 다른 평면은 카메라 평면($s, t$평면)이다. 카메라는 카메라 평면 위에서 움직이고, 각 이미지 평면을 통과하는 빛을 캡쳐한다.&lt;/p&gt;

&lt;p&gt;캡쳐 방법에는 두 가지가 있는데, &lt;strong&gt;&lt;em&gt;카메라를 $s, t$평면에서 움직이는 방법(lightfield rendering)과 카메라를 아무데나 움직인 뒤 rebinnig하는 방법(lumigraph)이 있다?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;우리가 알고 있는 평면은 2개이기 때문에 $u, v$평면의 각 점에 $s, t$평면의 모든 정보를 넣을 수도 있고, 반대로 할 수도 있다. 첫 번째 경우는 off-axis perspective view와 같고, 두 번째의 경우는 reflectance map과 같다.&lt;/p&gt;

&lt;h1 id=&quot;light-field-rendering&quot;&gt;Light Field Rendering&lt;/h1&gt;

&lt;p&gt;렌더링을 하기 위해서는 특정 위치에서 바라볼 때 빛이 어디서 어떻게 들어오는지의 정보를 lumigraph로부터 알아내야 한다. 우선 렌더링 시점은 카메라 평면보다 뒤쪽이다. 내 위치에서 모든 방향으로부터 들어오는 빛의 정보가 필요한데, 카메라가 모든 카메라평면에 대해 존재하는 것이 아니기 때문에 모든 점에 대해서 정보를 가지고 있는 것이 아니므로 정보가 없는 점은 interpolation을 이용해서 정보를 만들어낸다. 이 때는 quadrilinear interpolation을 사용한다. 원래의 bilinear interpolation은 주변 네 개의 점을 이용한다. 식은 다음과 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;s = \alpha_0s_0+\alpha_1s_1 \text{  where  } \alpha_0+\alpha_1=1&lt;/script&gt;

&lt;p&gt;quadrilinear interpolation은 점 $(s, t_0)$와 $(u_0, v_0)$가 주어질 때 $s_0$와 $s_1$에 대해서 계산해야 하기 때문에&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L(s, t_0, u_0, v_0)=\alpha_0L(s_0, t_0, u_0, v_0)+\alpha_1L(s_1, t_0, u_0, v_0)&lt;/script&gt;

&lt;p&gt;이 되는데, 이 계산을 $s, t, u, v$에 대해서 다 해야하므로&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L(s, t, u, v)=\sum_{i=0}^1\sum_{j=0}^1\sum_{k=0}^1\sum_{l=0}^1\alpha_i\beta_j\gamma_k\delta_lL(s_i, t_j, u_k, v_l)&lt;/script&gt;

&lt;p&gt;이라는 식으로 설명할 수 있다.&lt;/p&gt;

&lt;p&gt;표현을 쉽게 하기 위해 2D Ray Space를 사용할 수 있는데, 편의를 위해 특정 $t$와 특정 $v$가 주어졌다고 할 때의 $s, u$의 조합을 그려보면 $n_s\times n_u$개의 조합이 생긴다.(가로가 $u$, 세로가 $s$) 이를 2D평면으로 옮긴 것으로, 하나의 조합은 하나의 점으로 표현된다. 그리고 여러 조합의 교점은 직선으로 표현된다. 이 때, 2D Ray Space에서의 직선의 기울기가 작으면($s$는 조금 움직이는데 $u$는 많이 움직임) 해당 점이 더 가까운 것이므로 depth가 작다고 볼 수 있고, 반대로 직선의 기울기가 작으면 ($s$는 많이 움직이는데 $u$는 거의 움직이지 않음) 해당 점이 더 먼 것이므로 depth가 크다고 볼 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;various-lumigraph&quot;&gt;Various Lumigraph&lt;/h3&gt;

&lt;p&gt;우리는 카메라 평면을 사용하여 4D lumigraph를 만들었는데, 여기서 $t$를 빼고 3D lumigraph를 만들 수도 있다. 4D에서와 똑같지만 카메라를 한 직선에서만 움직이는 방법을 사용할 수 있다.&lt;/p&gt;

&lt;p&gt;이때 직선이 아닌 원을 그리며 카메라를 움직이면 Concentric Mosaic를 만들 수도 있다.&lt;/p&gt;

&lt;h3 id=&quot;layered-depth-image&quot;&gt;Layered Depth Image&lt;/h3&gt;

&lt;p&gt;2.5D representation이라는 것도 있다. Layered Depth Image라고도 하는데, 최소 3개의 카메라를 이용하여 대상을 둘러싸서 이미지를 얻는다. 그러면 가운데 카메라에서는 보이지 않는 깊이 정보가 양 옆 카메라에 의해 얻어지게 된다. 평면이지만, depth정보에 따라 더 앞으로 나오거나 더 뒤로 들어간 평면들이 걸과물로 나오게 된다.&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;terms&quot;&gt;Terms&lt;/h2&gt;
&lt;p&gt;aperture : 틈&lt;br /&gt;
collimate : 일직선의&lt;br /&gt;
lenticular : 수정체의&lt;br /&gt;
light field : 물체에서 발산하는 광선의 분포를 재현하는 기술
dispersive : 전파성의&lt;br /&gt;
medium : 매개&lt;/p&gt;
</description>
        <pubDate>Tue, 30 Jul 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/lecture/computervision/2019/07/30/Lecture8.html</link>
        <guid isPermaLink="true">http://localhost:4000/lecture/computervision/2019/07/30/Lecture8.html</guid>
        
        
        <category>Lecture</category>
        
        <category>ComputerVision</category>
        
      </item>
    
      <item>
        <title>CV Lecture 7 - Thin Lens Optics</title>
        <description>&lt;h1 id=&quot;camera-with-lens&quot;&gt;Camera with Lens&lt;/h1&gt;

&lt;p&gt;pinhole 모델은 들어오는 빛이 매우 적어 노출 시간이 길어야 제대로 된 이미지를 얻을 수 있고, sharpness에 약하다는 단점이 있었다.&lt;/p&gt;

&lt;p&gt;렌즈는 필름에 빛을 모아주므로 이런 단점들을 극복할 수 있었다. 렌즈는 focal length만큼 렌즈에서 떨어진 지점에 렌즈에 수직으로 들어오는 평행한 빛들을 모아준다. 렌즈의 중심부로 모이는 빛은 변형되지 않고 pinhole 모델에서와 똑같이 행동한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.photonics.com/images/Web/Articles/2009/3/8/GaussianNewtonianThinLensFormulas_Table1.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.photonics.com/images/Web/Articles/2009/3/8/GaussianNewtonianThinLensFormulas_Table1.jpg&quot;&gt;Source&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;대상에서부터 렌즈까지의 거리를 $s’$, 렌즈에서 센서까지의 거리를 $s$라고 하자. 렌즈 중심에서 대상에 내린 수선의 발과 렌즈 끝에서 대상에 내린 수선의 발까지의 거리를 $y’$라고 하고, 렌즈 중심에서 센서평면에 내린 수선의 발과 렌즈 끝에서 대상에 내린 수선의 발에서부터 들어온 빛이 센서평면에 닿는 점까지의 거리를 $y$이라고 하자. 그러면 닮음 삼각형 때문에 $\frac{y}{y’}=\frac{s}{s’}$가 된다.&lt;/p&gt;

&lt;p&gt;focal length가 형성되는 지점을 중심으로 또다른 닮은 삼각형이 있다. 이 때는 $\frac{y}{y’}=\frac{s-f}{f’}$의 식을 만들 수 있다. 위 두 식을 이용하면, $\frac{1}{f}=\frac{1}{s’}+\frac{1}{s}$을 얻을 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;depth-of-field-dof&quot;&gt;Depth of Field (DOF)&lt;/h3&gt;

&lt;p&gt;Depth of Field는 focus된 이미지를 얻을 수 있는 최소 거리와 최대 거리의 차이를 말한다. 만약 조리개 크기를 $f/2.0\rightarrow f/4.0$으로 변화시킨다면 DOF는 2배가 된다. focusing distance를 반으로 줄이면 DOF도 반으로 줄어든다.&lt;/p&gt;

&lt;h3 id=&quot;field-of-view-fov&quot;&gt;Field of View (FOV)&lt;/h3&gt;

&lt;p&gt;시야각이라고 한다. 사진에 담을 수 있는 끝점과 렌즈에 직교하는 선이 이루는 각도이다. $\phi$로 나타내며, 위에서 쓴 notation을 이용하면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\phi=\tan^{-1}\left(\frac{y}{f}\right)&lt;/script&gt;

&lt;p&gt;이다.&lt;/p&gt;

&lt;h3 id=&quot;lens-flaws&quot;&gt;Lens Flaws&lt;/h3&gt;

&lt;p&gt;렌즈는 빛의 주파수에 따라 다른 굴절율을 갖고 있기 때문에 색채 이상 현상이 나타난다. 이를 chromatic aberration(색 수차)이라고 하며, 가장자리에서 더욱 심하다.&lt;/p&gt;

&lt;p&gt;구면 렌즈에서는 렌즈 가장자리로 들어오는 빛의 초점과 렌즈 중앙으로 들어오는 빛의 초점이 정확하게 일치하지 않는 특징 때문에 spherical aberration(구면 수차)가 발생하기도 한다. 이 때는 초점이 한 점으로 생기는 것이 아니라 원 모양으로 생긴다.&lt;/p&gt;

&lt;p&gt;또한 렌즈에 수직이 아닌 각도로 빛이 들어올 때는 물방울 모양으로 초점이 생기는 현상인 comatic aberration(혜성형 수차)가 나타나기도 한다.&lt;/p&gt;

&lt;p&gt;사람 눈에서 나타나는 난시와 같이, 렌즈의 가로, 세로에서의 초점이 일치하지 않아서 생기는 astigmatism(비점수차)현상도 있다.&lt;/p&gt;

&lt;p&gt;이 외에도 geometrical aberration, wave optics, vignetting, lens flare, &lt;a href=&quot;https://yeonjeejung.github.io/lecture/computervision/2019/07/07/Lecture3.html&quot;&gt;radial distortion&lt;/a&gt; 등의 이상 현상이 일어날 수 있다.&lt;/p&gt;

&lt;h1 id=&quot;digital-camera&quot;&gt;Digital Camera&lt;/h1&gt;

&lt;p&gt;디지털 카메라는 기존 필름의 역할을 센서의 행렬로 교체했다. 각 셀은 빛에 민감한 다이오드이며, 이들이 광양자를 전기신호로 바꿔준다. 주로 이용되는 방식은 Charge Coupled Devive(CCD)와 Complementary Metal Oxide Semiconductor(CMOS)가 있다.&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\left[\begin{matrix}B&amp;G\\G&amp;R\end{matrix}\right] %]]&gt;&lt;/script&gt;의 필터가 사람의 추상체 역할을 하며 각 색상에 반응한다. 전체의 $\frac{1}{4}$는 빨강, $\frac{1}{4}$은 파랑, 나머지 $\frac{1}{2}$은 초록색 센서로, 해당 색상이 없는 칸은 옆 칸의 색상으로 보간을 해준다.&lt;/p&gt;

&lt;h3 id=&quot;demosaicing&quot;&gt;Demosaicing&lt;/h3&gt;

&lt;p&gt;보간을 하는 가장 쉬운 방법은 nearest neighbor 보간법이다. 주변의 가장 가까운 곳에 있는 센서의 값을 가져오는 방법인데, 화질 저하를 불러올 수 있다.&lt;/p&gt;

&lt;p&gt;두번째 방법으로는 bilinear interpolation이 있다. 해당 센서 사방에 있는 값을 평균내는 방법이다. 더 넓은 공간의 정보를 이용하는 bicubic interpolation도 있다.&lt;/p&gt;

&lt;h3 id=&quot;focal-length--sensor&quot;&gt;Focal length &amp;amp; Sensor&lt;/h3&gt;

&lt;p&gt;센서의 크기가 작아지면 그에 비례하여 FOV도 작아진다.&lt;/p&gt;

&lt;h3 id=&quot;digital-camera-artifacts&quot;&gt;Digital Camera Artifacts&lt;/h3&gt;

&lt;p&gt;디지털 카메라는 센서로 나뉘어 있기 때문에 필름 카메라에 비해 여러 단점이 있을 수 있다. 우선 양자화로 인해 노이즈가 생길 수 있고, 카메라 내에서 프로세싱이 추가로 진행된다. 또한 압축으로 인해 blocking 현상이 생길 수 있고 주변 픽셀로 번지는 blooming 현상도 있을 수 있다. 또한 색이 의도했던 바와는 다르게 표현될 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;explosure&quot;&gt;Explosure&lt;/h3&gt;

&lt;p&gt;노출이란, 대상으로부터 반사된 빛이 카메라 디텍터에 들어오는 빛의 양이다. $H$로 표현되고, 빛의 양 $E$와 노출시간 $t$에 의해&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H=E\cdot t&lt;/script&gt;

&lt;p&gt;라고 쓸 수 있다. $E$의 단위는 illuminance(lux)이고 $t$의 단위는 time(second)이므로, 노출의 단위는 $\text{lx}\cdot \text{sec}$ 이다.&lt;/p&gt;

&lt;p&gt;노출에는 두 가지의 주요 파라미터가 있는데, 조리개값과 셔터 속도이다. 더 작은 조리개값으로 같은 노출값을 유지하고 싶다면, 셔터 속도를 늘이면 된다. 단, 셔터 속도는 짧을수록 순간적인 장면을 더 잘 포착할 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;sensitivity-iso&quot;&gt;Sensitivity (ISO)&lt;/h3&gt;

&lt;p&gt;감도라고 한다. 조리개값, 셔터 속도와 함께 노출을 조절해주는 요소이다. 감도가 높으면 더 밝은 사진을 찍을 수 있지만, 노이즈가 더 추가된다.&lt;/p&gt;

&lt;h3 id=&quot;digital-imaging-workflow&quot;&gt;Digital Imaging Workflow&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;Scene Radiance&lt;/li&gt;
  &lt;li&gt;Amplifying &amp;amp; Quantization of voltage&lt;/li&gt;
  &lt;li&gt;Demosaic &amp;amp; Denoising&lt;/li&gt;
  &lt;li&gt;White Balancing&lt;/li&gt;
  &lt;li&gt;Camera Output&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;terms&quot;&gt;Terms&lt;/h2&gt;
&lt;p&gt;optics : 광학&lt;br /&gt;
deviate : 벗어나다&lt;br /&gt;
aparture : 틈, 구멍&lt;br /&gt;
focal length : 렌즈와 상이 맺히는 지점(focusing plane)까지의 거리&lt;br /&gt;
focusing distance : 대상과 상이 맺히는 지점까지의 거리&lt;br /&gt;
aberrated : 비정상의
flaw : 결함&lt;br /&gt;
chromatic : 색채의&lt;br /&gt;
refractive : 굴절에 의한&lt;br /&gt;
astigmatism : 난시, 비점수차
diffraction : 회절&lt;br /&gt;
vignetting : 감광&lt;br /&gt;
photon : 광양자&lt;br /&gt;
trichromatic : 삼원색의&lt;br /&gt;
halo : 강한 빛을 내는 피사체를 찍을 때, 그 주변의 빛번짐 현상&lt;/p&gt;
</description>
        <pubDate>Sun, 28 Jul 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/lecture/computervision/2019/07/28/Lecture7.html</link>
        <guid isPermaLink="true">http://localhost:4000/lecture/computervision/2019/07/28/Lecture7.html</guid>
        
        
        <category>Lecture</category>
        
        <category>ComputerVision</category>
        
      </item>
    
      <item>
        <title>CV Lecture 6 - Multiview Geometry</title>
        <description>&lt;h1 id=&quot;structure-from-motion&quot;&gt;Structure from Motion&lt;/h1&gt;

&lt;p&gt;Multiview geometry 문제에는 세 종류가 있다.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Scene geometry (Structure) : 2D 좌표가 주어질 때, 3D 좌표는 어디에 있는가?&lt;/li&gt;
  &lt;li&gt;Correspondence (stereo matching) : 한 이미지에서의 한 점이 다른 이미지에서는 어디에 존재할 가능성이 있는가?&lt;/li&gt;
  &lt;li&gt;Camera geometry (motion) : 한 대상에 대해 여러 관점의 이미지 좌표가 주어질 때, 카메라 파라미터 행렬은 무엇인가?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;이번 단원에서는 1번에 대해 다룰 것이다. (2, 3번은 각각 &lt;a href=&quot;https://yeonjeejung.github.io/lecture/computervision/2019/07/20/Lecture5.html&quot;&gt;2번&lt;/a&gt;, &lt;a href=&quot;https://yeonjeejung.github.io/lecture/computervision/2019/07/07/Lecture3.html&quot;&gt;3번&lt;/a&gt;에서 다루었다.)&lt;/p&gt;

&lt;h3 id=&quot;structure-from-motion-ambiguity&quot;&gt;Structure from Motion Ambiguity&lt;/h3&gt;

&lt;p&gt;Scene geometry는 m개의 이미지와 n개의 3D 좌표가 주어졌을 때, m개의 projection matrix $P_i$와 n개의 3D 좌표 $X_j$를 $m\times n$개의 $x_{ij}$로부터 찾는 문제이다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{x}_{ij}=P_i\mathbf{X}_j&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;i=1, \cdots , m, j=1, \cdots, n&lt;/script&gt;

&lt;p&gt;만약 scale factor $k$가 존재하고, 카메라 파라미터들을 모두 $k$로 나눈다면, 그렇지 않았을 때와 같은 식이 나오게 되므로 정확한 scale 측정이 불가능하다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{x}=P\mathbf{X}=\left(\frac{1}{k}P\right)(k\mathbf{X})&lt;/script&gt;

&lt;p&gt;또는, transformation matrix $Q$를 이용해도 똑같은 결과가 나온다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{x}=P\mathbf{X}=(PQ^{-1})(Q\mathbf{X})&lt;/script&gt;

&lt;h3 id=&quot;affine-camera&quot;&gt;Affine Camera&lt;/h3&gt;

&lt;p&gt;Orthographic projection은 이미지 좌표에서 world좌표로 변환 시 $z$축의 변화가 없는 변환이었다. affine 카메라는 3D 공간상에서의 affine 변환, orthographic projection, 이미지상의 affine 변환 효과가 결합된 것이다. 따라서 다음 식으로 나타낼 수 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
P=\left[3\times3 \text{affine}\right]\left[\begin{matrix}1 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 1\end{matrix}\right]\left[4\times4 \text{affine}\right]=\left[\begin{matrix}a_{11} &amp; a_{12} &amp;a_{13} &amp;b_1 \\a_{21} &amp; a_{22} &amp;a_{23} &amp;b_2 \\ 0 &amp; 0 &amp; 0 &amp; 1\end{matrix}\right]=\left[\begin{matrix}A &amp; b \\ 0 &amp; 1\end{matrix}\right] %]]&gt;&lt;/script&gt;

&lt;p&gt;결국 affine 카메라는 linear mapping에 translation을 더한 것이다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\mathbf{x}=\left(\begin{matrix}x \\ y\end{matrix}\right)=\left[\begin{matrix}a_{11} &amp; a_{12} &amp; a_{13} \\ a_{21} &amp; a_{22} &amp; a_{23}\end{matrix}\right]\left(\begin{matrix}X \\ Y \\ Z \end{matrix}\right)+\left(\begin{matrix}b_1 \\ b_2\end{matrix}\right)=A\mathbf{X+b} %]]&gt;&lt;/script&gt;

&lt;p&gt;이 해는 유일하지 않고, 임의의 affine transformation $Q$에 의해 &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\left[\begin{matrix}A &amp; b \\ 0 &amp; 1\end{matrix}\right] \rightarrow \left[\begin{matrix}A &amp; b \\ 0 &amp; 1\end{matrix}\right]Q^{-1} %]]&gt;&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;\left(\begin{matrix}X\\1\end{matrix}\right)\rightarrow Q\left(\begin{matrix}X\\1\end{matrix}\right)&lt;/script&gt;가 해가 될 수도 있다.&lt;/p&gt;

&lt;h3 id=&quot;affine-structure-from-motion&quot;&gt;Affine Structure from Motion&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;우리는 $2mn$개의 변수를 알고 있고, $8m+3n$개를 모르고 $Q$가 12의 degree of freedom을 가지므로 $2mn \geq (8m+3n)-12$여야 이 문제를 풀수 있다?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;먼저, 이미지 좌표의 중심을 빼므로써 centering을 한다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{\mathbf{x}}_{ij}=\mathbf{x}_{ij}-\frac{1}{n}\sum_{k=1}^n\mathbf{x}_{ik}=A_i\mathbf{X}_j+b_i-\frac{1}{n}\sum_{k=1}^n(A_i\mathbf{X}_k+b_i)=A_i\hat{\mathbf{X}}_j&lt;/script&gt;

&lt;p&gt;계산을 쉽게 하기 위해 world좌표계의 원점이 3D좌표계의 center라고 하자.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{\mathbf{x}}_{ij}=A_i\mathbf{X}_j&lt;/script&gt;

&lt;p&gt;그리고 $2m\times n$개의 데이터 행렬을 만든다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
D=\left[\begin{matrix}\hat{\mathbf{x}}_{11} &amp; \hat{\mathbf{x}}_{12} &amp; \cdots &amp; \hat{\mathbf{x}}_{1n} \\ \hat{\mathbf{x}}_{21} &amp; \hat{\mathbf{x}}_{22} &amp; \cdots &amp; \hat{\mathbf{x}}_{2n} \\
&amp; &amp; \ddots &amp; \\
\hat{\mathbf{x}}_{m1} &amp; \hat{\mathbf{x}}_{m2} &amp; \cdots &amp; \hat{\mathbf{x}}_{mn} \end{matrix}\right]=
\left[\begin{matrix}A_1 \\ A_2 \\ \vdots \\ A_m\end{matrix}\right]
\left[\begin{matrix}\mathbf{X}_1 &amp; \mathbf{X}_2&amp; \cdots &amp; \mathbf{X}_n\end{matrix}\right]=MS %]]&gt;&lt;/script&gt;

&lt;p&gt;이 행렬은 rank가 3이다. ($A$행렬이 열 3개이고 $D$는 $A$와 $X$들의 조합으로 만들어지므로)이제 Singular Value Decomposition (SVD)를 이용해 D를 세 개의 행렬로 나눌 수 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;D=UWV^T&lt;/script&gt;

&lt;p&gt;rank가 3이기 때문에 W의 대각성분 3개 빼고는 다 0이다. 우리는 $U, W, V$행렬에서 좌측 3개만 가져와서 차원을 3으로 압축할 수 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;D=U_3W_3V_3^T&lt;/script&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;M=U_3W_3^{\frac{1}{2}}, S=W_3^{\frac{1}{2}}V_3^T&lt;/script&gt;라고 한다면, 원하는 차원의 $M$과 $S$를 얻을 수 있다.&lt;/p&gt;

&lt;p&gt;이 decomposition도 유일하지 않아서, 임의의 $3\times3$ 행렬 $C$를 이용하여 $M\rightarrow MC$, $S\rightarrow C^{-1}S$의 해를 만들 수도 있다.&lt;/p&gt;

&lt;h3 id=&quot;eliminating-the-affine-ambiguity&quot;&gt;Eliminating the Affine Ambiguity&lt;/h3&gt;

&lt;p&gt;서로 직교하고 scale이 1인 축을 기준으로 하게 한다. 이 축은 다음 두 식을 만족한다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;a_1\cdot a_2=0&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\|a_1\|^2=\|a_2\|^2=1&lt;/script&gt;

&lt;p&gt;이미지 한장당 $a_1, a_2$가 각각 존재할 것이다. 이들은 각각&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;a_1La_1^T=1&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;a_2La_2^T=1&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;a_1La_2^T=0&lt;/script&gt;

&lt;p&gt;을 만족한다.&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;A_i=\left[\begin{matrix}a_{1i} \\ a_{2i}\end{matrix}\right]&lt;/script&gt;라고 하면 $3m$개의 방정식&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;A_iLA_i^T=I&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;i=1, \cdots, m&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;을 얻을 수 있다. 이 방정식을 풀고, cholesky decomposition을 이용해 $L=CC^T$로 분해한 뒤 $M\rightarrow MC$, $S\rightarrow C^{-1}S$로 바꿔주면 &lt;strong&gt;&lt;em&gt;ambigudity를 없앨 수 있다?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;missing-data&quot;&gt;Missing Data&lt;/h3&gt;

&lt;p&gt;실제로 어떤 3D 좌표는 어떤 view의 이미지에서는 보이지 않을 수도 있기 때문에 $M$행렬이 원하는 모양으로 나오지 않을 수도 있다. 이 때는 이 행렬을 dense sub-block으로 나누고, 그 sub-block들을 factorize한 뒤 사용하는 방법이 있다. &lt;strong&gt;&lt;em&gt;incremental bilinear refinement를 사용한다는데.. 어떻게 하는거지&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;projective-structure-from-motion&quot;&gt;Projective Structure from Motion&lt;/h3&gt;

&lt;p&gt;m개의 이미지와 n개의 3D 좌표가 주어졌을 때, m개의 projection matrix $P_i$와 n개의 3D 좌표 $X_j$를 $m\times n$개의 $x_{ij}$로부터 찾는 문제이다. 아까와 똑같지만, $z_{ij}$라는 depth가 추가로 존재한다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;z_{ij}\mathbf{x}_{ij}=P_i\mathbf{X}_j&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;i=1, \cdots , m, j=1, \cdots, n&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;calibration 정보가 없으면 카메라와 좌표들은 $4\times4$ projective transformation 행렬 $Q$를 통해서만 발견할 수 있다?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;우리는 $2mn$개의 변수를 알고 있고, $11m+3n$개를 모르고 $Q$가 15의 degree of freedom을 가지므로 $2mn \geq (11m+3n)-15$여야 이 문제를 풀수 있다?&lt;/em&gt;&lt;/strong&gt; 따라서 2개의 카메라에 대해서는 최소 7개 점에 대한 정보가 필요하다.&lt;/p&gt;

&lt;p&gt;우선 두 뷰 사이의 fundamental matrix $F$를 계산한다. 기준이 될 카메라의 행렬을 &lt;script type=&quot;math/tex&quot;&gt;\left[\begin{matrix}I \mid 0\end{matrix}\right]&lt;/script&gt;, 다른 카메라의 행렬을 &lt;script type=&quot;math/tex&quot;&gt;\left[\begin{matrix}A \mid b \end{matrix}\right]&lt;/script&gt;라고 하자. 그러면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;z\mathbf{x}=\left[I \mid 0\right]\mathbf{X}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;z'\mathbf{x}'=\left[A\mid b\right]\mathbf{X}=A\left[I\mid 0\right]\mathbf{X}+b=zA\mathbf{x}+b&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;z'\mathbf{x}'\times b=zA\mathbf{x}\times b&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;(z'\mathbf{x}'\times b)\cdot \mathbf{x}'=(zA\mathbf{x}\times b)\cdot \mathbf{x}'=0&lt;/script&gt;

&lt;p&gt;($\mathbf{x}’\times b$와 $\mathbf{x}’$는 서로 수직이므로 내적하면 0이 된다.)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{x}'^T[b_{\times}]A\mathbf{x}=0&lt;/script&gt;

&lt;p&gt;이므로, $F=[b_{\times}]A$이다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
D=\left[\begin{matrix}z_{11}\mathbf{x}_{11} &amp; z_{12}\mathbf{x}_{12} &amp; \cdots &amp; z_{1n}\mathbf{x}_{1n} \\ z_{21}\mathbf{x}_{21} &amp; z_{22}\mathbf{x}_{22} &amp; \cdots &amp; z_{2n}\mathbf{x}_{2n} \\
&amp; &amp; \ddots &amp; \\
z_{m1}\mathbf{x}_{m1} &amp; z_{m2}\mathbf{x}_{m2} &amp; \cdots &amp; z_{mn}\mathbf{x}_{mn} \end{matrix}\right]=
\left[\begin{matrix}P_1 \\ P_2 \\ \vdots \\ P_m\end{matrix}\right]
\left[\begin{matrix}\mathbf{X}_1 &amp; \mathbf{X}_2&amp; \cdots &amp; \mathbf{X}_n\end{matrix}\right]=MS %]]&gt;&lt;/script&gt;

&lt;p&gt;$z$를 알고 있다면 factorize를 통해 $M, S$를 구할 수 있고, $M, S$를 알고 있다면 $z$를 구할 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;self-calibration&quot;&gt;Self-Calibration&lt;/h3&gt;

&lt;p&gt;Self-Calibration (auto-calibration)은 카메라의 intrinsic 파라미터를 바로 알아내는 것이다. intrinsic 파라미터는 모든 이미지에 대해 상관없이 일정하다는 조건을 가지고 찾아낼 수 있다.&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;terms&quot;&gt;Terms&lt;/h2&gt;
&lt;p&gt;tangency : 접합  &lt;br /&gt;
fuse : 사용하다&lt;/p&gt;
</description>
        <pubDate>Fri, 26 Jul 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/lecture/computervision/2019/07/26/Lecture6.html</link>
        <guid isPermaLink="true">http://localhost:4000/lecture/computervision/2019/07/26/Lecture6.html</guid>
        
        
        <category>Lecture</category>
        
        <category>ComputerVision</category>
        
      </item>
    
      <item>
        <title>Project 1</title>
        <description>&lt;p&gt;이 프로젝트는 Camera Calibration과 Disparity Map, Depth Map을 생성하는 프로젝트이다.&lt;/p&gt;

&lt;h1 id=&quot;camera-calibration&quot;&gt;Camera Calibration&lt;/h1&gt;

&lt;p&gt;먼저, 체커보드를 이용해서 Camera Calibration을 수행한다. 6개의 체커보드 이미지가 있고, 모서리를 찾아서 이를 평면좌표(world point)로 만들어주는 homography matrix를 찾고, 그 matrix에서 intrinsic, extrinsic 파라미터를 찾는 과정이다. &lt;a href=&quot;http://staff.fh-hagenberg.at/burger/publications/reports/2016Calibration/Burger-CameraCalibration-20160516.pdf&quot;&gt;Zhang’s method&lt;/a&gt;를 사용하여 수행할 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;homography-calculation&quot;&gt;Homography Calculation&lt;/h3&gt;

&lt;p&gt;먼저 $s\tilde{q}=H\tilde{p}$를 만족하는 행렬 $H$를 찾아야 한다. ($\tilde{q}$는 world point, $\tilde{p}$는 체커보드 좌표)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
H = \left[\begin{matrix}h_{11} &amp; h_{12} &amp; h_{13}\\ h_{21} &amp; h_{22} &amp; h_{23} \\ h_{31} &amp; h_{32} &amp; h_{33}\end{matrix}\right] %]]&gt;&lt;/script&gt;

&lt;p&gt;으로 표현되고, $\min_H\sum_{j=1}^m\left|\left|q_j-\hat{q_j}\right|\right|^2$를 최소화하는 $H$를 찾으면 된다.
(&lt;script type=&quot;math/tex&quot;&gt;q_j = \left[\begin{matrix}u_j \\ v_j\end{matrix}\right]&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;\hat{q_j} = \frac{1}{h_{31}X_j+h_{32}Y_j+h_{33}}\left[\begin{matrix}h_{11}X_j+h_{12}Y_j+h_{33}\\h_{21}X_j+h_{22}Y_j+h_{23}\end{matrix}\right]&lt;/script&gt;)&lt;/p&gt;

&lt;p&gt;이를 정리하면,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\min_H\sum_j\left|\left|\left[\begin{matrix}u_0(h_{31}X_j+h_{32}Y_j+h_{33})-(h_{11}X_j+h_{12}Y_j+h_{13})\\v_0(h_{31}X_j+h_{32}Y_j+h_{33})-(h_{21}X_j+h_{22}Y_j+h_{23})\end{matrix}\right]\right|\right|^2&lt;/script&gt;

&lt;p&gt;이 되는데, 각 항을 풀어보면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\left[\begin{matrix}-X_j &amp; -Y_j &amp; -1 &amp; 0 &amp; 0 &amp; 0 &amp; u_jX_j &amp; u_jY_j &amp; u_j \\  0 &amp; 0 &amp; 0 &amp; -X_j &amp; -Y_j &amp; -1 &amp; v_jX_j &amp; v_jY_j &amp; v_j\end{matrix}\right]
\left[\begin{matrix}h_{11}\\ h_{12} \\h_{13} \\h_{21} \\h_{22} \\h_{23} \\h_{31} \\h_{32} \\h_{33}\end{matrix}\right] %]]&gt;&lt;/script&gt;

&lt;p&gt;이라고 쓸 수 있다. 앞에 곱해진 행렬을 $L_j$라고 하면 이 문제는 $\min_{\mathbf{x}}||L\mathbf{x}||^2$ 문제가 된다. ($L$은 $L_j$를 세로로 늘어뜨린 행렬) 따라서 $L^TL$의 가장 작은 eigenvalue중 가장 작은 것에 대응하는 eigenvector를 찾으면 된다.&lt;/p&gt;

&lt;p&gt;이 때의 $H$ 행렬은 한 이미지당 하나를 찾을 수 있으므로 이 프로젝트에서는 6개의 $H$ 행렬이 나오게 된다.&lt;/p&gt;

&lt;h3 id=&quot;intrinsic-parameter&quot;&gt;Intrinsic Parameter&lt;/h3&gt;

&lt;p&gt;우리는 $H$행렬을 알고 있고, &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
H = K\left[\begin{matrix}r_1 &amp; r_2 &amp; t\end{matrix}\right] %]]&gt;&lt;/script&gt;이다. 그리고 &lt;script type=&quot;math/tex&quot;&gt;r_1^Tr_1=r_2^Tr_2&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;r_1^Tr_2=0&lt;/script&gt;이라는 제약을 갖고 있으므로, 우리는 intrinsic matrix $K$를 알아낼 수 있다.&lt;/p&gt;

&lt;p&gt;먼저 &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
H=\left[\begin{matrix}h_1 &amp; h_2 &amp; h_3\end{matrix}\right] %]]&gt;&lt;/script&gt;이라고 하자. $h_1=Kr_1$, $h_2=Kr_2$이기 때문에 $r_1=K^{-1}h_1$, $r_2=K^{-1}h_2$이다.&lt;/p&gt;

&lt;p&gt;위 조건들을 대입해 보면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;h_1^TK^{-T}K^{-1}h_1=h_2^TK^{-T}k^{-1}h_2&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;h_1^TK^{-T}K^{-1}h_2=0&lt;/script&gt;

&lt;p&gt;라는 식을 만들어낼 수 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
B=K^{-T}K^{-1}=\left[\begin{matrix}B_{11} &amp; B_{12} &amp; B_{13} \\B_{21} &amp; B_{22} &amp; B_{23} \\B_{31} &amp; B_{32} &amp; B_{33}\end{matrix}\right] %]]&gt;&lt;/script&gt;

&lt;p&gt;이라고 하면,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
B=\left[\begin{matrix}\frac{1}{\alpha^2} &amp; -\frac{\gamma}{\alpha^2\beta} &amp; \frac{v_0\gamma-u_0\beta}{\alpha^2\beta} \\ -\frac{\gamma}{\alpha^2}{\beta} &amp; -\frac{\gamma}{\alpha^2\beta}+\frac{1}{\beta^2} &amp; -\frac{\gamma(v_0\gamma-u_0\beta)}{\alpha^2\beta^2}\\ \frac{v_0\gamma-u_0\beta}{\alpha^2\beta} &amp; -\frac{\gamma(v_0\gamma-u_0\beta)}{\alpha^2\beta^2}-\frac{v_0}{\beta^2} &amp; \frac{(v_0\gamma-u_0\beta)^2}{\alpha^2\beta^2}+\frac{v_0^2}{\beta^2}+1\end{matrix}\right] %]]&gt;&lt;/script&gt;

&lt;p&gt;이다. (&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
K=\left[\begin{matrix}\alpha &amp; \gamma &amp; u_0 \\ 0 &amp; \beta &amp; v_0 \\ 0 &amp; 0 &amp; 1\end{matrix}\right] %]]&gt;&lt;/script&gt;이므로)&lt;/p&gt;

&lt;p&gt;$B_{11}, B_{12}, B_{13}, B_{22}, B_{23}, B_{33}$만 알면 $K$의 변수들을 구할 수 있으므로 &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
b=\left[\begin{matrix}B_{11} &amp; B_{12} &amp; B_{13} &amp; B_{22} &amp; B_{23} &amp; B_{33}\end{matrix}\right]^T %]]&gt;&lt;/script&gt;를 구하는 것을 목표로 한다.&lt;/p&gt;

&lt;p&gt;두 개의 조건을 압축해서&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\left[\begin{matrix}v_{12}^T \\ (v_{11}-v_{22})^T\end{matrix}\right]b=0&lt;/script&gt;

&lt;p&gt;이라고 쓸 수 있다. (&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
v_{kl}=\left[\begin{matrix}h_{1k}h_{1l} &amp; h_{1k}h_{2l}+h_{2k}h_{1l} &amp; h_{1k}h_{3l}+h_{3k}h_{1l} &amp; h_{2k}h_{2l} &amp; h_{2k}h_{3l}+h_{3k}h_{2l} &amp; h_{3k}h_{3l} \end{matrix}\right]^T %]]&gt;&lt;/script&gt;)&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;V=\left[\begin{matrix}v_{12}^T \\ (v_{11}-v_{22})^T\end{matrix}\right]&lt;/script&gt;라고 하면 각 $V$들은 이미지당 하나씩 있으므로, 우리는 총 6개의 $V$를 가지고 $B$를 찾을 수 있다. 이 $V$들을 세로로 합쳐서 $V’$행렬을 만들고, &lt;script type=&quot;math/tex&quot;&gt;\min_b\|V'b\|^2&lt;/script&gt; 문제를 풀면 $b$를 구할 수 있다. 이 때의 해는 $V’^TV’$의 eigenvalue중 가장 작은 것에 대응하는 eigenvector를 구하면 된다.&lt;/p&gt;

&lt;p&gt;이렇게 구한 b를 이용해서 각각의 변수에&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;v_0=\left(\frac{B_{12}B_{13}-B_{11}B_{23}}{B_{11}B_{22}-B_{12}^2}\right)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\lambda=B_{33}-\frac{B_{13}^2+v_0(B_{12}B_{13}-B_{11}B_{23})}{B_{11}}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\alpha=\sqrt{\frac{\lambda}{B_{11}}}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\beta=\sqrt{\frac{\lambda B_{11}}{B_{11}B_{22}-B_{12}^2}}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\gamma=-\frac{B_{12}\alpha^2\beta}{\lambda}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;u_0=\frac{\gamma v_0}{\beta}-\frac{B_{13}\alpha^2}{\lambda}&lt;/script&gt;

&lt;p&gt;를 대입하면 intrinsic matrix &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
K=\left[\begin{matrix}\alpha &amp; \gamma &amp; u_0 \\ 0 &amp; \beta &amp; v_0 \\ 0 &amp; 0 &amp; 1\end{matrix}\right] %]]&gt;&lt;/script&gt;를 구할 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;extrinsic-parameter&quot;&gt;Extrinsic Parameter&lt;/h3&gt;

&lt;p&gt;지금까지 구한 $H$와 $K$로 extrinsic parameter들을 구할 수 있다. 먼저 정규화를 위한 상수 &lt;script type=&quot;math/tex&quot;&gt;\lambda'=\frac{\frac{1}{\|K^{-1}h_1\|}+\frac{1}{\|K^{-1}h_2\|}}{2}&lt;/script&gt;를 구한다. 그 후에는&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;r_1=\lambda' K^{-1}h_1&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;r_2=\lambda' K^{-1}h_2&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;r_3=r_1\times r_2&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;t=\lambda'K^{-1}h_3&lt;/script&gt;

&lt;p&gt;을 이용해 extrinsic parameter들을 구한다.&lt;/p&gt;

&lt;p&gt;여기서 구한 rotation matrix &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
R=\left[\begin{matrix}r_1 &amp; r_2 &amp; r_3\end{matrix}\right] %]]&gt;&lt;/script&gt;은 일반적으로 rotation matrix의 성질을 만족하지 않으므로, 보정을 해줘야 한다. $R=U\Sigma V^T$로 분해할 수 있는데, 여기서 $R’=UV^T$를 구하면 된다.&lt;/p&gt;

&lt;h3 id=&quot;maximum-likelihood-estimation-mle&quot;&gt;Maximum Likelihood Estimation (MLE)&lt;/h3&gt;

&lt;p&gt;우리가 위에서 구한 값은 그저 방정식을 푼 것이지, 물리적으로 아무 의미가 없는 숫자이다. 따라서 MLE를 통해 값을 보정해줄 필요가 있다. MLE추정은 &lt;script type=&quot;math/tex&quot;&gt;\min_{K, R_i, t_i}\sum_i\sum_j\|q_{ij}-\hat{q_{ij}}\|^2&lt;/script&gt;를 통해 할 수 있다. (사실 이 식은 $H$를 찾을 때 썼던 식이다.)&lt;/p&gt;

&lt;p&gt;코드에서는 intrinsic, extrinsic 파라미터들을 벡터로 만들어 넘겨서 lsqnonlin 함수를 사용하여 최적화했다.(Matlab)&lt;/p&gt;

&lt;h1 id=&quot;depth-map&quot;&gt;Depth Map&lt;/h1&gt;

&lt;p&gt;Depth map을 구하기 위한 과정은 다음과 같다. (좌, 우에서 찍은 이미지가 주어진다.)&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;이미지를 흑백화시킨다.&lt;/li&gt;
  &lt;li&gt;cost function을 정해서 cost volume을 만든다.&lt;/li&gt;
  &lt;li&gt;cost aggregation을 한다.&lt;/li&gt;
  &lt;li&gt;최소 cost를 갖는 인덱스를 골라 disparity map을 만든다.&lt;/li&gt;
  &lt;li&gt;disparity map을 카메라 파라미터를 이용해 depth map으로 변환시킨다.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;preprocessing&quot;&gt;Preprocessing&lt;/h3&gt;

&lt;p&gt;단순히 두 이미지를 흑백화시킨다.&lt;/p&gt;

&lt;h3 id=&quot;cost-volume&quot;&gt;Cost Volume&lt;/h3&gt;

&lt;p&gt;좌측 이미지를 기준으로 한다. 좌측 이미지의 모든 픽셀에 대하여 오른쪽 이미지에서는 왼쪽으로 $d$만큼 떨어진 픽셀을 고른다. (좌측 이미지에서 $(10,5)$에 대해, $d=5$라면 우측 이미지에서는 $(5,5)$를 고르는 셈.) 좌우 이미지 모두 해당 픽셀 주변의 window크기만큼의 행렬을 골라 벡터화시킨 후, cost를 계산해 cost volume에 저장한다. 이 계산을 minDisparity부터 maxDisparity까지 수행해 cost volume에 저장한다. (이 프로젝트에서 window 크기는 15로 지정하였고, minDisparity는 11, maxDisparity는 140으로 지정하였다. cost 대신에 &lt;script type=&quot;math/tex&quot;&gt;NCC=\frac{\sum_i\sum_jA_{ij}B_{ij}}{\sqrt{\sum_i\sum_jA_{ij}^2}\sqrt{\sum_i\sum_j{B_{ij}^2}}}&lt;/script&gt;를 사용하였다.)&lt;/p&gt;

&lt;h3 id=&quot;cost-aggregation&quot;&gt;Cost Aggregation&lt;/h3&gt;

&lt;p&gt;각 layer의 cost를 filter를 이용하여 aggregation한다. 이 프로젝트에서는 imguidedfilter 함수에 해당 이미지를 가이드로 사용해서 필터링하였다.&lt;/p&gt;

&lt;h3 id=&quot;disparity-map&quot;&gt;Disparity Map&lt;/h3&gt;

&lt;p&gt;각 픽셀에 대하여 가장 작은 cost를 가지는 disparity를 고른다. (이 프로젝트에서는 NCC를 사용하였으므로 값이 가장 큰 것을 고른다.)&lt;/p&gt;

&lt;h3 id=&quot;depth-map-1&quot;&gt;Depth Map&lt;/h3&gt;

&lt;p&gt;각 픽셀에 대하여 $Z=\frac{fT}{d}$ 공식을 적용하여 depth map을 만든다.&lt;/p&gt;

&lt;h1 id=&quot;result&quot;&gt;Result&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/YeonjeeJung/ComputervisionProject1/blob/master/code/calibration.jpg?raw=true&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위 표는 카메라 파라미터 측정에 대한 결과이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/YeonjeeJung/ComputervisionProject1/blob/master/code/scene2.jpg?raw=true&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위 이미지는 각각 좌, 우에서 찍은 이미지이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/YeonjeeJung/ComputervisionProject1/blob/master/code/scne2_disparity.jpg?raw=true&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위 이미지는 disparity map이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/YeonjeeJung/ComputervisionProject1/blob/master/code/scene2_depth.jpg?raw=true&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위 이미지는 depth map이다.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;이 프로젝트에 대한 코드는 &lt;a href=&quot;https://github.com/YeonjeeJung/ComputervisionProject1&quot;&gt;링크&lt;/a&gt;에서 볼 수 있다.&lt;/p&gt;
</description>
        <pubDate>Thu, 25 Jul 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/lecture/computervision/2019/07/25/Project1.html</link>
        <guid isPermaLink="true">http://localhost:4000/lecture/computervision/2019/07/25/Project1.html</guid>
        
        
        <category>Lecture</category>
        
        <category>ComputerVision</category>
        
      </item>
    
      <item>
        <title>CV Lecture 5 - Stereo Matching</title>
        <description>&lt;h1 id=&quot;stereo-matching&quot;&gt;Stereo Matching&lt;/h1&gt;

&lt;h3 id=&quot;stereo-matching-1&quot;&gt;Stereo Matching&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://www.researchgate.net/profile/Nathaniel_Short/publication/265115132/figure/fig7/AS:305305712906251@1449801962495/Simple-geometry-for-stereo-ranging-The-usual-goal-is-to-find-the-range-Z-from-the.png&quot; alt=&quot;&quot; /&gt;
&lt;a href=&quot;https://www.researchgate.net/profile/Nathaniel_Short/publication/265115132/figure/fig7/AS:305305712906251@1449801962495/Simple-geometry-for-stereo-ranging-The-usual-goal-is-to-find-the-range-Z-from-the.png&quot;&gt;source&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;($f$:focal length, $p^l, p^r$:image point(left, right), $P$:world point, $Z$:depth of $P$)
(length of Baseline을 $t$라고 하자. 그리고 $x_l$은 $O_l$을 기준으로 한 선으로부터 음의 방향으로 $p_l$까지의 거리, $x_r$은 $O_r$을 기준으로 한 선으로부터 양의 방향으로 $p_r$까지의 거리이다.)&lt;/p&gt;

&lt;p&gt;그러면 $\triangle(p_l, P, p_r)$과 $\triangle(O_l, P, O_r)$은 닮음이므로&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{t+x_l-x_r}{Z-f} = \frac{t}{Z}&lt;/script&gt;

&lt;p&gt;이 성립하므로,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Z = f\frac{t}{x_r-x_l}&lt;/script&gt;

&lt;p&gt;이 성립하는데, 이때 $x_r-x_l$을 disparity라고 하고, $D = x_r-x_l$이라고 한다.&lt;/p&gt;

&lt;h3 id=&quot;essential-matrix&quot;&gt;Essential Matrix&lt;/h3&gt;

&lt;p&gt;회전변환은 없으므로(두 평면이 평행하므로) rotation matrix $R = I$이고, translation matrix $T = [-d, 0, 0]^T$가 된다. 그리고 $E = [T_\times]R = \left(\begin{matrix}0 &amp;amp; 0 &amp;amp; 0 \ 0 &amp;amp; 0 &amp;amp; d \ 0 &amp;amp; -d &amp;amp; 0\end{matrix}\right)$이다. $p_l = [x, y, f], p’ = [x’, y’, f]$라고 하면 $p’^TEp=0$을 이용하여&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\left[\begin{matrix}x' &amp; y' &amp; f\end{matrix}\right]
\left[\begin{matrix}0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; d \\ 0 &amp; -d &amp; 0\end{matrix}\right]\left[\begin{matrix}x\\y\\f\end{matrix}\right]=0 %]]&gt;&lt;/script&gt;
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\Leftrightarrow\left[\begin{matrix}x' &amp; y' &amp; f\end{matrix}\right]
\left[\begin{matrix}0\\df\\-dy\end{matrix}\right]=0  \Leftrightarrow y=y' %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;임을 알 수 있다. 이는 카메라가 평행이동하여 찍은 두 사진에서, 같은 점은 같은 $y$축 좌표가 같아야 함을 알려준다.&lt;/p&gt;

&lt;h3 id=&quot;sum-of-square-difference-ssd&quot;&gt;Sum of Square Difference (SSD)&lt;/h3&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;SSD = \sum_i(f_{1i}-f_{2i})^2&lt;/script&gt;

&lt;p&gt;왼쪽 사진에서 한 픽셀에 대하여 오른쪽 사진에는 epipolar line이 있을 것이다. 그 epipolar line 위에 있는 모든 픽셀에 대해서, 왼쪽 사진 픽셀과의 차이를 제곱해서 모두 더한다. 그 cost가 가장 작은 것이 best match이다. 그러나 이 방법은 비슷한 패턴들이 많이 나타날 때에는 best match가 많이 나타날 수도 있다.&lt;/p&gt;

&lt;p&gt;따라서, ratio distance를 정의한다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;ratio \ distance = \frac{SSD(f_1, f_2)}{SSD(f_1, f_2')}&lt;/script&gt;

&lt;p&gt;$f_2$는 $f_1$과의 best match이고, $f_2’$는 두 번째 best match이다. &lt;strong&gt;&lt;em&gt;근데 이게 그냥 SSD보다 왜 좋은걸까?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;normalized-cross-correlation-ncc&quot;&gt;Normalized Cross Correlation (NCC)&lt;/h3&gt;
&lt;p&gt;SSD와 비슷한데, 점끼리의 차가 아닌 region의 차의 제곱을 계산한다. 중앙에 해당 픽셀이 있고, 그 주위 픽셀까지 정사각형으로 묶는다. 그 두 사각형이 $A$와 $B$라고 하면, 이들을 $a$, $b$라는 벡터로 만든다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;NCC = \frac{a\cdot b}{\left|a\right|\left|b\right|}, -1\le NCC \le 1&lt;/script&gt;

&lt;p&gt;$A$는 고정해 놓고, $B$를 움직이면서 NCC cost를 계산한다. 이 cost가 가장 작은 점이 best match이다. 이 때, window size가 너무 작으면 노이즈까지 전부 세세하게 고려하게 되고, 너무 크면 디테일이 사라지게 되어 적당한 window size를 고르는 것이 중요하다.&lt;/p&gt;

&lt;h3 id=&quot;stereo-matching-framework&quot;&gt;Stereo Matching Framework&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;모든 차이에 대해서 raw matching cost를 계산한다. ($E_0(x,y;d)=\rho(I_L(x’+d,y’)-I_R(x’,y’))$)
이때 $\rho$는 robust function인데, input $u$가 작으면 제곱에 가까운 값을, 크면 1에 수렴하는 값을 내놓는다. ($\rho(u;\sigma) = \frac{u^2}{\sigma^2+u^2}$) &lt;strong&gt;&lt;em&gt;폐쇄된 부분(occlusion)이나 튀는 값 때문에 이 함수를 쓴다?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;&lt;em&gt;cost를 공간적으로 합친다?&lt;/em&gt;&lt;/strong&gt; ($E(x,y;d)=\sum_{(x’,y’)\in N(x,y)}E_0(x’,y’,d)$)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;각각의 픽셀에 대해 winning disparity를 찾는다. ($d(x,y)=argmin_d(x,y;d)$)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;sub-pixel accuracy로 보간한다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Baseline은 너무 작으면 depth를 계산하는 데에 에러가 있을 수 있고, 너무 크면 같은 점 찾기가 어려워진다. 전통적인 Stereo Matching은 디테일한 표면 예측을 하고, 빠른 계산을 할 수 있다는 장점이 있다. 그러나 Baseline이 작으면 노이즈가 많을 수 있고, 질감에 따라 결과가 다르다는 점, &lt;strong&gt;&lt;em&gt;가까운 폐쇄 경계를 찾을 때 어렵다는 단점이 있다?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;data-interpolation&quot;&gt;Data Interpolation&lt;/h3&gt;

&lt;p&gt;sparse한 3D 점들이 있을 때 이것을 보간하는 방법에는 여러가지가 있는데, 먼저 energy minimization은 에너지의 최소점을 찾는 것이다.&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;E_{total}(d)=E_{data}(d)+\lambda E_{membrane}(d)&lt;/script&gt;
&lt;script type=&quot;math/tex&quot;&gt;E_{total}(d)=\sum_{x,y}(d_{x,y}-z_{x,y})^2+\lambda\sum_{x,y}(d_{x,y}-d_{x-1,y})^2&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;각 항을 미분하면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial}{\partial d_{x,y}}E_{data}(d)=\frac{\partial}{\partial d_{x,y}}\sum_{x,y}(d_{x,y}-z_{x,y})^2=\frac{\partial}{\partial d_{x,y}}\left[(d_{x,y}-z_{x,y})^2\right]=2(d_{x,y}-z_{x,y})&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial}{\partial d_{x,y}}E_{membrane}(d)=\frac{\partial}{\partial d_{x,y}}\sum_{x,y}(d_{x,y}-d_{x-1,y})^2=\frac{\partial}{\partial d_{x,y}}\left[(d_{x,y}-d_{x-1,y})^2+(d_{x+1,y}-d_{x,y})^2\right]&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;=2(2d_{x,y}-d_{x-1,y}-d_{x+1,y})&lt;/script&gt;

&lt;p&gt;이고, 우리는 $E_{total}(d)$를 줄이는 것이 목표이기 때문에&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial}{\partial d_{x,y}}E_{total}(d)=2(d_{x,y}-z_{x,y})+2\lambda (2d_{x,y}-d_{x-1,y}-d_{x+1,y})=0&lt;/script&gt;

&lt;p&gt;으로 만들어 주는 방향으로 가면 된다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;d_{x,y}\leftarrow\frac{1}{1+2\lambda}(z_{x,y}+d_{x-1,y}+d_{x+1,y})&lt;/script&gt;

&lt;p&gt;이렇게 iteration을 돌려주면 된다. 이걸 dynamic programming으로 푸는 방법이 있다. 먼저 가장 쉬운 1차원 cost function을 살펴본다. 우선 아까와는 다르게 &lt;script type=&quot;math/tex&quot;&gt;E_{membrane}(d)=\sum_{x,y}\|d_{x,y}-d_{x-1,y}\|&lt;/script&gt;로 정의한다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E_{data}(d)=\sum_{x,y}\|d_{x+1,y}-d_{x,y}\|+\sum_{x,y}E_0(x,y;d)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\tilde{E}(x,y,d)=E_0(x,y;d)+\min_{d'}(\tilde{E}(x-1,y,d')+\| d_{x,y}-d_{x-1,y}'\|)&lt;/script&gt;

&lt;p&gt;2D에는 이를 적용할 수 없다. ($d_{x,y-1}$과 $d_{x-1,y}$가 서로 다른 $d_{x-1,y-1}$값에 의해 결정되기 때문) 그래서 Graph cuts optimization 이라는 알고리즘을 사용한다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E_{data}(d)=\sum_{x,y}f_{x,y}(d_{x,y})&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E_{membrane}(d)=\sum_{x,y}\|d_{x,y}-d_{x-1,y}\|+\sum_{x,y}\|d_{x,y}-d_{x,y-1}\|&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;그래서 그래프컷을 어떻게 사용함?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;terms&quot;&gt;Terms&lt;/h2&gt;
&lt;p&gt;disparity : 차이, 이격도&lt;br /&gt;
occlusion : 폐쇄&lt;br /&gt;
specularity : 반사&lt;br /&gt;
sweep : 쓸다&lt;/p&gt;
</description>
        <pubDate>Sat, 20 Jul 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/lecture/computervision/2019/07/20/Lecture5.html</link>
        <guid isPermaLink="true">http://localhost:4000/lecture/computervision/2019/07/20/Lecture5.html</guid>
        
        
        <category>Lecture</category>
        
        <category>ComputerVision</category>
        
      </item>
    
      <item>
        <title>CV Lecture 4 - Camera Calibration</title>
        <description>&lt;h1 id=&quot;camera-calibration&quot;&gt;Camera Calibration&lt;/h1&gt;

&lt;h3 id=&quot;stereo-reconstruction&quot;&gt;Stereo Reconstruction&lt;/h3&gt;
&lt;p&gt;Stereo Reconstruction은 두 장 이상의 2D 이미지를 이용해서 이를 3D상에서 재건하는 것인데, 단계는 다음과 같다.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;카메라 변수 찾기&lt;/li&gt;
  &lt;li&gt;이미지 바로잡기&lt;/li&gt;
  &lt;li&gt;차이 계산&lt;/li&gt;
  &lt;li&gt;깊이 예측&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;가장 간단한 케이스는 평행한 두 지점에서 찍힌 이미지를 이용해 재건하는 것이다. 이미지 $I’$가 이미지 $I$보다 $x$축으로 $T$만큼 이동했다면, $R = I$(identity)이고 $t=(T, 0, 0)$이 된다. 따라서&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
E=t\times R=\left[\begin{matrix}0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; -T \\ 0 &amp; T &amp; 0\end{matrix}\right] %]]&gt;&lt;/script&gt;

&lt;p&gt;이 된다.&lt;/p&gt;

&lt;h3 id=&quot;stereo-image-rectification&quot;&gt;Stereo Image Rectification&lt;/h3&gt;
&lt;p&gt;두 카메라가 평행하면 한 좌표를 보는 두 이미지 평면이 평행하지 않을 수 있는데, 이렇게 얻은 이미지 평면을 두 카메라를 잇는 선에 평행한 평면에 옮겨주는 것을 image rectification이라고 한다. 이것을 하고 나면 위에서 쓴 essential 행렬을 사용할 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;homography&quot;&gt;Homography&lt;/h3&gt;
&lt;p&gt;3D평면에 있는 점을 image 평면으로 변환시킬 때 사용하는 행렬을 homography ($H$)라고 한다.&lt;/p&gt;

&lt;p&gt;이 두 시점 변환에 translation이 없으므로 rotation만이 존재하게 되고, $P_r = RP_l$라고 쓸 수 있다. ($P_r$은 첫번째, $P_l$은 두번째 각도의 projection 행렬) $P_l = K[I\mid0]$이므로&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x_l = K[I\mid0]\left[\begin{matrix} X\\ 1 \end{matrix}\right] = KX&lt;/script&gt;

&lt;p&gt;이고, $P_r = K[R\mid0]$이므로&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x_r = K[R\mid0]\left[\begin{matrix}X\\ 1\end{matrix}\right] = KRX&lt;/script&gt;

&lt;p&gt;이다. 따라서, $x_r = KRK^{-1}x_l$이고, $H = KRK^{-1}$이 된다.&lt;/p&gt;

&lt;p&gt;이미지의 원점이 같은 평면의 다른 각도에서 촬영되었을 때만 homography가 서로 연관될 수 있다. (translation이 없어야 한다.) 그리고, 같은 카메라로 촬영되었을 때만 연관될 수 있다.&lt;/p&gt;

&lt;p&gt;만약 rotation 행렬 $R$을 모른다면, $x_r^TFx_l = 0$에서 $F$를 알아낼 때 썼던 방법을 이용해 찾을 수 있다. &lt;strong&gt;&lt;em&gt;사실, $K^{-T}K^{-1}$의 가장 작은 eigenvalue에 대응하는 eigenvector를 찾으면 된다.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;2d-checkerboard-pattern&quot;&gt;2D Checkerboard Pattern&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;http://www.campi3d.com/External/MariExtensionPack/userGuide4R1/lib/CHeckerSimpleB.png&quot; alt=&quot;&quot; /&gt;
&lt;a href=&quot;http://www.campi3d.com/External/MariExtensionPack/userGuide4R1/lib/CHeckerSimpleB.png&quot;&gt;Source&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;2D 체스판을 생각해 보자. 체스판의 모든 좌표는 하나의 평면에 있다. 따라서, 모든 점의 $z$좌표는 모두 $0$이다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\left(\begin{matrix}u\\v\\1\end{matrix}\right)=K
\left[\begin{matrix}
r_{11} &amp; r_{12} &amp; r_{13} &amp; t_1 \\
r_{21} &amp; r_{22} &amp; r_{23} &amp; t_2 \\
r_{31} &amp; r_{32} &amp; r_{33} &amp; t_3 \\
\end{matrix}\right]\left(\begin{matrix}x\\y\\z\\1\end{matrix}\right)=K
\left[\begin{matrix}
r_{11} &amp; r_{12} &amp; r_{13} &amp; t_1 \\
r_{21} &amp; r_{22} &amp; r_{23} &amp; t_2 \\
r_{31} &amp; r_{32} &amp; r_{33} &amp; t_3 \\
\end{matrix}\right]\left(\begin{matrix}x\\y\\0\\1\end{matrix}\right) %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\therefore \left(\begin{matrix}u\\v\\1\end{matrix}\right)=K
\left[\begin{matrix}
r_{11} &amp; r_{12} &amp; t_1 \\
r_{21} &amp; r_{22} &amp; t_2 \\
r_{31} &amp; r_{32} &amp; t_3 \\
\end{matrix}\right]\left(\begin{matrix}x\\y\\1\end{matrix}\right)=H
\left(\begin{matrix}x\\y\\1\end{matrix}\right) %]]&gt;&lt;/script&gt;

&lt;p&gt;($(u, v, 1)$은 image plane위의 점이고, $(x, y, 1)$은 체스판 위의 점이다.)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
H=K
\left[\begin{matrix}
r_{11} &amp; r_{12} &amp; t_1 \\
r_{21} &amp; r_{22} &amp; t_2 \\
r_{31} &amp; r_{32} &amp; t_3 \\
\end{matrix}\right]=
\left[\begin{matrix}
h_{11} &amp; h_{12} &amp; h_{13} \\
h_{21} &amp; h_{22} &amp; h_{23} \\
h_{31} &amp; h_{32} &amp; h_{33} \\
\end{matrix}\right]=
\left[\begin{matrix}
h_{11}' &amp; h_{12}' &amp; h_{13}' \\
h_{21}' &amp; h_{22}' &amp; h_{23}' \\
h_{31}' &amp; h_{32}' &amp; 1 \\
\end{matrix}\right] %]]&gt;&lt;/script&gt;

&lt;p&gt;$H$는 변수가 8개이므로, 8개의 식을 알면 풀 수 있다. 각 점들은 2개의 관계식을 주기 때문에, 우리는 4개의 점만 알면 $H$를 알아낼 수 있다.&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;terms&quot;&gt;Terms&lt;/h2&gt;
&lt;p&gt;rectify : 바로잡기&lt;br /&gt;
disparity : 차이, 이격&lt;br /&gt;
anaglyph : 애너글리프. 인간의 두 눈의 시차를 이용하여 3D영상을 보는 것처럼 하는 것.&lt;br /&gt;
tripod : 삼각대&lt;/p&gt;
</description>
        <pubDate>Mon, 08 Jul 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/lecture/computervision/2019/07/08/Lecture4.html</link>
        <guid isPermaLink="true">http://localhost:4000/lecture/computervision/2019/07/08/Lecture4.html</guid>
        
        
        <category>Lecture</category>
        
        <category>ComputerVision</category>
        
      </item>
    
      <item>
        <title>CV Lecture 3 - Epipolar Geometry</title>
        <description>&lt;h1 id=&quot;single-view-geometry&quot;&gt;Single-View Geometry&lt;/h1&gt;
&lt;p&gt;일단 우리의 목표는 한 3D 구조를 재건하는 것이다. 그러나 하나의 이미지는 깊이 정보를 갖고 있지 않기 때문에 정확하게 재건할 수 없다. 따라서 우리는 multi-view geometry가 필요하다.&lt;/p&gt;

&lt;h3 id=&quot;principal-point-offset&quot;&gt;Principal Point Offset&lt;/h3&gt;
&lt;p&gt;principal axis는 카메라 중심에서부터 이미지 평면에 수직으로 들어오는 선이다. principal point란 principal axis가 이미지 평면과 만나는 점이다. 따라서 normalized coordinate system 상의 원점은 principal point이다. 그런데 image coordinate system상의 원점은 좌측 하단이다. 따라서 image coordinate system에서 얻은 좌표를 normalized coordinate system에서의 좌표로 변환해야 한다. (normalized coordinate system에서의 좌표에는 음수도 존재한다.)&lt;/p&gt;

&lt;p&gt;principal point의 좌표가 $(p_x, p_y)$라고 하면 normalized coordinate system에서의 좌표는 $(x, y, z) \rightarrow (f\frac{x}{z}, f\frac{y}{z})$가 되고, 이를 image coordinate system의 좌표로 나타내면 $(f\frac{x}{z}+p_x, f\frac{y}{z}+p_y)$가 된다. 이를 하나의 식으로 나타내면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\left[\begin{matrix} fx+zp_x \\ fy+zp_y \\ z \end{matrix}\right]=
\left[\begin{matrix}
f &amp; 0 &amp; p_x\\
0 &amp; f &amp; p_y\\
0 &amp; 0 &amp; 1
\end{matrix} \right]
\left[\begin{matrix}
1 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 1 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 1 &amp; 0
\end{matrix} \right]
\left[\begin{matrix} x \\ y \\ z \\ 1 \end{matrix}\right] %]]&gt;&lt;/script&gt;

&lt;p&gt;인데, 첫번째 행렬은 intrinsic 행렬이고, 두번째 행렬은 projection 행렬이다. 이 때 이 intrinsic 행렬을 calibration 행렬이라고 하고, $K$로 표기한다. projection 행렬은 $[I\mid0]$로 표현될 수 있다. 따라서 이 때 곱해지는 행렬 $P$는 $K[I\mid0]$가 된다.&lt;/p&gt;

&lt;p&gt;실제 이미지에서는 이렇게 바뀐 좌표를 pixel coordinate로 바꿔야 한다. 가로에는 $1$미터 안에 $m_x$개의 픽셀, 세로에는 $1$미터 안에 $m_y$개의 픽셀이 있다고 하자. 원래의 $K$행렬은 각 좌표를 미터 단위로 변환시켜주는데, 이 앞에 $1$미터 안에 들어있는 픽셀수를 곱해주면 픽셀 단위로 좌표를 변환시킬 수 있다. 따라서 pixel coordinate로 변환시켜줄 새로운 $K$는&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
K=\left[\begin{matrix}
m_x &amp; 0 &amp; 0 \\
0 &amp; m_y &amp; 0 \\
0 &amp; 0 &amp; 1\\
\end{matrix}\right]
\left[\begin{matrix}
f &amp; 0 &amp; p_x \\
0 &amp; f &amp; p_y \\
0 &amp; 0 &amp; 1
\end{matrix}\right]=
\left[\begin{matrix}
\alpha_x &amp; 0 &amp; \beta_x \\
0 &amp; \alpha_y &amp; \beta_y \\
0 &amp; 0 &amp; 1
\end{matrix}\right] %]]&gt;&lt;/script&gt;

&lt;p&gt;가 된다.&lt;/p&gt;

&lt;h3 id=&quot;camera-rotation-and-translation&quot;&gt;Camera Rotation and Translation&lt;/h3&gt;
&lt;p&gt;카메라의 extrinsic 파라미터에 해당한다. 일반적으로 카메라를 땅과 평행하게 두고 촬영하지 않기 때문에 projection에 앞서 실제 좌표를 카메라 좌표로 옮기는 일도 필요하다. 여기에는 두 가지 과정이 있는데, 중심을 $(0, 0)$으로 맞추는 과정과 회전변환을 하는 과정이다. 이를 식으로 나타내면 $\tilde{\mathbf{X}}_{cam} = R(\tilde{\mathbf{X}}-\tilde{C})$ 가 된다. 하지만 이것은 $\tilde{\mathbf{X}} = (x, y, z)$라고 가정했을 때의 식이고, 이는 non-homogeneous이므로, homogeneous한 식으로 바꾸면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\mathbf{X}_{cam}=
\left[\begin{matrix}
R &amp; -R\tilde{C}\\ 0 &amp; 1
\end{matrix}\right]\mathbf{X} %]]&gt;&lt;/script&gt;

&lt;p&gt;라고 쓸 수 있다. (한 차원을 추가해서 그 차원을 상수 $1$로 고정, $\mathbf{X}=\left(\begin{matrix} \tilde{\mathbf{X}} \ 1\end{matrix}\right)$)&lt;/p&gt;

&lt;p&gt;최종적으로 $\mathbf{x} = K[I\mid0]\mathbf{X}_{cam} = K[R\mid-R\tilde{C}]\mathbf{X}$가 되고, projection 행렬 $P$는 $P = K[R\mid-R\tilde{C}]$가 된다.&lt;/p&gt;

&lt;h3 id=&quot;radial-distortion&quot;&gt;Radial Distortion&lt;/h3&gt;
&lt;p&gt;intrinsic 파라미터 중 하나이다. 카메라 렌즈의 굴절에 의해 일어나는 왜곡으로써, 렌즈의 중앙과 멀어질수록 왜곡이 심하게 일어난다. 왜곡 모델은 다음과 같다.&lt;/p&gt;

&lt;p&gt;먼저, $(\hat{x}, \hat{y}, \hat{z})$를 normalized coordinate로 옮긴다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;(x_n', y_n') = (\frac{\hat{x}}{\hat{z}}, \frac{\hat{y}}{\hat{z}})&lt;/script&gt;

&lt;p&gt;그 후 radial distortion을 적용한다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;r^2 = x_n'^2 + y_n'^2&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\left(\begin{matrix}x_d' \\ y_d'\end{matrix}\right)=
(1+k_1r^2+k_2r^4)\left(\begin{matrix}x_n' \\ y_n'\end{matrix}\right)&lt;/script&gt;

&lt;p&gt;마지막으로 &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
K = \left[\begin{matrix}f &amp; 0 &amp; p_x \\ 0 &amp; f &amp; p_y \\ 0 &amp; 0 &amp; 1\end{matrix}\right] %]]&gt;&lt;/script&gt;을 적용한다.&lt;/p&gt;

&lt;h3 id=&quot;camera-calibration--linear-method&quot;&gt;Camera calibration : Linear Method&lt;/h3&gt;
&lt;p&gt;그렇다면 $P$는 어떻게 찾을 수 있을까? 이미지 상의 좌표 $\mathbf{x}$에 대해 $\mathbf{x}=P\mathbf{X}$를 만족하고, $P \in \mathbb{R}^{3\times4}$이므로, 12개의 변수를 가지고 있다. (하지만 마지막은 항상 1이므로, 사실은 11개의 변수를 가지고 있다.) 따라서 최소 12개의 식을 알고 있다면 이 방정식을 풀어 $P$를 찾을 수 있다. 하나의 $\mathbf{x}$는 $x, y$의 두 좌표를 가지고 있으므로, 최소 6개의 $\mathbf{x}, \mathbf{X}$ 쌍을 알고 있다면 $P$를 알아낼 수 있다.&lt;/p&gt;

&lt;p&gt;$\mathbf{X}$가 projection에 의해 $\mathbf{x}$로 사영되므로, 둘의 방향은 같다. 따라서 $\mathbf{x}_i \times P\mathbf{X}_i = 0$이라는 관계식을 가지고 있다. 이 식에서 linearly independent한 두 개의 식만 가지고 와서&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\left[\begin{matrix}
0 &amp; -\mathbf{X}_i^T &amp; y_i\mathbf{X}_i^T \\
\mathbf{X}_i^T &amp; 0 &amp; -x_i\mathbf{X}_i^T
\end{matrix}\right]
\left(\begin{matrix}P_x \\ P_y \\ P_z\end{matrix}\right)=0 %]]&gt;&lt;/script&gt;

&lt;p&gt;의 식을 만들 수 있다.($P_{x, y, z} \in \mathbb{R}^{4\times1}$) 6개의 점을 알고 있다면 여기에 대입하여&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\left[\begin{matrix}
0^T &amp; \mathbf{X}_1^T &amp; -y_1\mathbf{X}_1^T \\
\mathbf{X}_1^T &amp; 0^T &amp; x_1\mathbf{X}_1^T \\
\cdots &amp; \cdots &amp; \cdots \\
0^T &amp; \mathbf{X}_6^T &amp; -y_6\mathbf{X}_6^T \\
\mathbf{X}_6^T &amp; 0^T &amp; x_6\mathbf{X}_6^T \\
\end{matrix}\right]
\left(\begin{matrix}P_x \\ P_y \\ P_z\end{matrix}\right)=0 %]]&gt;&lt;/script&gt;

&lt;p&gt;을 풀면 $P$를 알아낼 수 있다.&lt;/p&gt;

&lt;p&gt;Linear method의 장점은 수식화가 쉽고 풀기도 쉽다는 점이다. 하지만 직접적으로 카메라 파라미터를 알 수는 없으며 radial distortion도 고려하지 않는다. 또한 제한 사항도 집어넣을 수 없다는 단점이 있다. 따라서 non-Linear method가 더 선호된다.&lt;/p&gt;

&lt;h1 id=&quot;epipolar-geometry&quot;&gt;Epipolar Geometry&lt;/h1&gt;

&lt;h3 id=&quot;triangulation&quot;&gt;Triangulation&lt;/h3&gt;
&lt;p&gt;두개, 혹은 그 이상의 이미지를 가지고 3D상의 점을 찾는 것을 triangulation이라고 한다. $O_1$ 을 원점으로 갖는 이미지에서의 점 $\mathbf{x}_1$, $O_2$를 원점으로 갖는 이미지에서의 점 $\mathbf{x}_2$를 각각의 원점과 이은 선은 실제 점 $\mathbf{x}$에서 교차해야 한다. 하지만 노이즈나 에러때문에 정확하게 만나지는 않는다. 따라서 두 선의 거리가 가장 가까운 두 점을 이은 선분의 중점을 $\mathbf{X}$라고 예측한다.&lt;/p&gt;

&lt;p&gt;$\mathbf{X}$를 알아내는 linear approach는 다음과 같다. $\mathbf{x}_1$과 $\mathbf{x}_2$는 $P_i\mathbf{X}$와 평행하기 때문에 각각 다음의 식이 성립한다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{x}_i \times P_i\mathbf{X}=0&lt;/script&gt;

&lt;p&gt;그리고 &lt;script type=&quot;math/tex&quot;&gt;\mathbf{x}_i\times P\mathbf{X}=[\mathbf{x}_{i\times}]P\mathbf{X}&lt;/script&gt;를 만족하는 &lt;script type=&quot;math/tex&quot;&gt;[\mathbf{x}_{i\times}]&lt;/script&gt;를 찾을 수 있다. 하나의 &lt;script type=&quot;math/tex&quot;&gt;\mathbf{x}_i&lt;/script&gt;에 대해 &lt;script type=&quot;math/tex&quot;&gt;[\mathbf{x}_{i\times}]P_x\mathbf{X}=0&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;[\mathbf{x}_{i\times}]P_y\mathbf{X}=0&lt;/script&gt;의 두 식을 알아낼 수 있고(두 개만 linearly independent하므로) 우리는 두 개의 &lt;script type=&quot;math/tex&quot;&gt;\mathbf{x}_i&lt;/script&gt;를 알고 있으므로 총 4개의 식을 알고 있다. 알아내야 하는 &lt;script type=&quot;math/tex&quot;&gt;\mathbf{X}&lt;/script&gt;의 변수가 3개이므로, 방정식을 풀어 &lt;script type=&quot;math/tex&quot;&gt;\mathbf{X}&lt;/script&gt;를 알아낼 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\mathbf{X}&lt;/script&gt;를 알아내는 non-linear approach는 &lt;script type=&quot;math/tex&quot;&gt;\mathbf{X} = \text{argmin}(d^2(\mathbf{x}_1, P_1\mathbf{X})+d^2(\mathbf{x}_2, P_2\mathbf{X}))&lt;/script&gt;를 찾아내면 된다.&lt;/p&gt;

&lt;h3 id=&quot;epipolar-geometry-1&quot;&gt;Epipolar Geometry&lt;/h3&gt;
&lt;p&gt;두 개의 이미지에서 한 점의 좌표를 알아내는 상황을 가정한다. baseline은 이미지의 두 원점을 이은 선이고, epipolar plane은 그 선을 포함하는 평면들이다. epipole은 baseline과 각 이미지 평면들의 교점이다. 마지막으로 epipolar line은 epipolar plane과 이미지 평면의 교점이다.&lt;/p&gt;

&lt;h3 id=&quot;epipolar-constraint&quot;&gt;Epipolar Constraint&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://slideplayer.com/slide/4824174/15/images/43/Epipolar+constraint%3A+Calibrated+case.jpg&quot; alt=&quot;&quot; /&gt;
&lt;a href=&quot;https://slideplayer.com/slide/4824174/15/images/43/Epipolar+constraint%3A+Calibrated+case.jpg&quot;&gt;Source&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;만약 $I$이미지 상의 $\mathbf{x}$가 존재한다면, $I’$이미지에서 이 $\mathbf{x}$에 대응하는 점 $\mathbf{x}’$은 epipolar line에서 찾을 수 있다는 것이 epipolar constraint이다.&lt;/p&gt;

&lt;p&gt;원점 $O$에서 $I$위의 점 $\mathbf{x}$로 향하는 벡터 $\mathbf{x}$와 원점 $O’$에서 $I’$위의 점 $\mathbf{x}’$로 향하는 벡터 $R\mathbf{x}’$($R$은 $I$에서 $I’$로의 변환행렬), 그리고 $O$와 $O’$를 잇는 선인 $t$는 같은 평면(삼각형 $\mathbf{X}OO’$가 이루는 평면)상의 벡터이다. 따라서 $\mathbf{x}\cdot[t\times(R\mathbf{x}’)]=0$이 성립하고, 이는 $\mathbf{x}^TE\mathbf{x}’=0$으로 표현될 수 있다. ($E=[t_\times]R$) 이때 $E$를 essential 행렬이라고 부른다. 사실 이 $E$가 반드시 존재한다는 것이 epipolar constraint이다.&lt;/p&gt;

&lt;p&gt;위 식을 이용하면 epipolar line인 $l$과 $l’$이 $l=E\mathbf{x}’$, $l’=E^T\mathbf{x}$임을 알 수 있다. (삼각형 $\mathbf{X}OO’$에서 $x$벡터에 수직인 선분은 $l$밖에 없으므로, $\mathbf{x}’$에 대해서도 마찬가지) 따라서 epipolar line상에 있는 epipole들도 $Ee’=0$, $E^Te=0$을 만족한다. $E$는 자유도 3의 회전, 자유도 2의 평행이동으로 이루어져 있으므로 5개의 점의 좌표를 알면 구할 수 있다.&lt;/p&gt;

&lt;p&gt;$E$는 normalized coordinate 상에서의 좌표들의 관계를 나타내는 행렬이고, 다음은 정규화도 되지 않은 실제 공간에서의 좌표들의 관계를 나타내는 행렬 $F$를 구하는 방법이다. 일단 우리는 calibration matrix $K$와 $K’$를 모른다고 가정하고, 이들은 다음을 만족한다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{x} = K\hat{\mathbf{x}}, \mathbf{x}'=K'\hat{\mathbf{x}}'&lt;/script&gt;

&lt;p&gt;$\hat{\mathbf{x}}$와 $\hat{\mathbf{x}}’$ 를 이용한 epipolar constraint의 식은 $\hat{\mathbf{x}}^TE\hat{\mathbf{x}}’=0$ 이다. 이제 $F=K^{-T}EK’^{-1}$에 대해 $\mathbf{x}^TF\mathbf{x}’=0$이 성립한다. 이 때의 $F$를 fundamental matrix라고 한다. $F$를 구하기 위해서는 $E$를 구할 때보다 $K$, $K’$의 정보가 더 필요하기 때문에 7개의 점의 좌표가 필요하다.&lt;/p&gt;

&lt;h3 id=&quot;the-eight-point-algorithm&quot;&gt;The eight-point algorithm&lt;/h3&gt;
&lt;p&gt;다른 방식으로 $F$를 구할 수도 있다. $F$는 어쨌건 $3\times3$ 행렬이므로 9개의 식을 알고 있으면 구할 수 있다. 게다가, 하나만 존재하게 하기 위해 마지막 수는 항상 1이 되어야 한다는 조건을 추가하면, 8개의 점을 알면 구할 수 있다. 이를 eight-point algorithm이라고 부르며, $\sum_{i=1}^N(\mathbf{x}_i^TF\mathbf{x}_i’)^2$를 최소화시키는 $F$를 구하면 된다. 이 에러의 의미는 어떤 점 $\mathbf{x}$와 epipolar line $F\mathbf{x}’$사이의 유클리드 거리의 합을 최소화시킨다는 것이다. eight-point algorithm은 원래 좌표를 normalize시킨 후 계산하고, 다시 원래 좌표로 변환시켜 계산했을 때 더 정확하다.&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;terms&quot;&gt;Terms&lt;/h2&gt;
&lt;p&gt;calibration : 교정, 눈금&lt;br /&gt;
epipolar : 등극선 (이라는데 명확하지 않은 번역이므로 그냥 epipolar라고 쓴다)&lt;br /&gt;
coplanar : 동일 평면상의&lt;/p&gt;
</description>
        <pubDate>Sun, 07 Jul 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/lecture/computervision/2019/07/07/Lecture3.html</link>
        <guid isPermaLink="true">http://localhost:4000/lecture/computervision/2019/07/07/Lecture3.html</guid>
        
        
        <category>Lecture</category>
        
        <category>ComputerVision</category>
        
      </item>
    
      <item>
        <title>CV Lecture 2 - Image Formation and Camera</title>
        <description>&lt;h1 id=&quot;pinhole-camera-model&quot;&gt;Pinhole Camera Model&lt;/h1&gt;

&lt;h3 id=&quot;pinhole-camera&quot;&gt;Pinhole Camera&lt;/h3&gt;
&lt;p&gt;어떤 물체에서 반사된 빛을 필름에 바로 닿게 하면 물체의 여러 점에서 반사된 빛이 필름의 한 점에 모이게 되어 물체의 모양이 제대로 나올 수 없고 블러된 이미지가 나온다.&lt;/p&gt;

&lt;p&gt;Pinhole은 바늘구멍이다. 물체와 필름 사이에 작은 구멍이 뚫린 장애물을 놓고 반사된 빛이 필름에 닿게 하면 그 바늘구멍을 통과한 빛만이 필름에 닿게 된다. 이는 블러를 줄이게 된다. 이 바늘구멍이 카메라의 조리개 역할을 한다.&lt;/p&gt;

&lt;p&gt;하지만 바늘구멍에 통과된 이미지는 위아래, 좌우가 바뀌게 된다.&lt;/p&gt;

&lt;h3 id=&quot;focal-length&quot;&gt;Focal Length&lt;/h3&gt;
&lt;p&gt;필름과 핀홀 사이의 거리를 Focal Length라고 한다. 이 Focal Length가 2배가 되면 필름에 사영된 이미지의 크기도 2배가 되고, 빛의 양은 $1 \over 4$이 된다.&lt;/p&gt;

&lt;h3 id=&quot;aperture-size&quot;&gt;Aperture Size&lt;/h3&gt;
&lt;p&gt;조리개의 크기가 크면 빛이 많이 들어와서 더 블러된 이미지가 생성되고, 조리개가 작으면 빛이 적게 들어와서 더 확실한 이미지가 생성된다. 그러나 조리개의 크기가 적당 크기 이하로 더 작아지면 Diffraction limit으로 인해 오히려 더 블러된 이미지가 생성된다.&lt;/p&gt;

&lt;p&gt;빛은 파동의 성질을 갖고 있기 때문에 구멍이 작아지면 더 많이 회절하게 되어 더 블러된 이미지가 생성된다. 이런 한계를 Diffraction limit이라고 한다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;이를 극복하기 위해서는 point-spread function(PSF)를 deconvolution 하면 된다??&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;vanishing-points-and-lines&quot;&gt;Vanishing Points and Lines&lt;/h3&gt;
&lt;p&gt;카메라로 사진을 찍으면 3D 대상이 2D 이미지로 사영되는 것이기 때문에 길이와 각도 정보를 잃게 된다. 하지만 직선은 보존된다.&lt;/p&gt;

&lt;p&gt;모든 직선(이미지에 평행한 직선 제외)은 각자의 소실점을 갖고 있다. 한 직선은 그에 대응하는 소실점으로 수렴한다. 마찬가지로 한 평면은 그에 대응하는 소실선으로 수렴한다.&lt;/p&gt;

&lt;h3 id=&quot;perspective-distortion&quot;&gt;Perspective Distortion&lt;/h3&gt;
&lt;p&gt;흔히 건물같이 거대한 대상을 촬영할 때 문제가 되는 현상이다. 건물을 아래에서 올려다보는 각도로 찍으면 아래부분은 더 확대되어 보이고, 윗부분은 작게 보인다. 건물과 평행한 각도로 찍으면 건물의 아랫부분밖에 보이지 않는다.&lt;/p&gt;

&lt;p&gt;이 왜곡에 대한 대안점은 렌즈를 건물에 평행한 각도에서 위로 높여 찍는 것이다. 렌즈를 건물 높이에 따라 움직일 수 있는 view camera라는 것이 있다.&lt;/p&gt;

&lt;p&gt;세로에서 일어나는 것고 같이, 가로에서도 같은 현상이 일어난다. 렌즈의 중심에서보다 렌즈의 외곽에 있는 대상에서 왜곡이 많이 일어나 대상이 크게 퍼져 보인다.&lt;/p&gt;

&lt;h1 id=&quot;modeling-projection&quot;&gt;Modeling Projection&lt;/h1&gt;
&lt;p&gt;$(x, y, z)$에서 반사된 점이 $y$축과의 거리가 $f$인 PP를 지나 $(0, 0, 0)$의 조리개에 들어간다고 하자. 이 때 PP에 사영된 $(x, y, z)$의 빛은 $(-f \frac{x}{z}, -f\frac{y}{z}, -f)$를 지난다. 우리는 2D 이미지를 얻을 것이므로 z축은 무시되고, 따라서 $(x, y, z)$는 PP의 $(-f \frac{x}{z}, -f\frac{y}{z})$으로 사영된다.&lt;/p&gt;

&lt;p&gt;이것은 $z$로 나누어지기 때문에 비선형적이므로, 하나의 차원을 더 늘려주는 트릭을 사용하므로써 선형으로 만들 수 있다. $(x, y)$를 $(x, y, 1)$로 표현하는 것이다. 이를 homogeneous coordinates라고 한다. 반대도 마찬가지로, homogeneous coordinates에서 $(x, y, \omega)$로 표현된 좌표는 2차원에서 $(\frac{x}{\omega}, \frac{y}{\omega})$로 표현될 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;perspective-projection&quot;&gt;Perspective Projection&lt;/h3&gt;
&lt;p&gt;사영은 homogeneous coordinates를 사용해서 행렬곱을 하는 것이다. 아까의 예를 들어 보면, $(x, y, z)$의 점을 COP로부터 $f$만큼 떨어져 있는 PP에 사영할 때&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\left[\begin{matrix}
1 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 1 &amp; 0 &amp; 0\\
0 &amp; 0 &amp;-\frac{1}{f} &amp; 0
\end{matrix} \right]
\left[\begin{matrix} x \\ y \\ z \\ 1 \end{matrix}\right]=
\left[\begin{matrix}x \\ y \\ -\frac{z}{f}\end{matrix}\right] %]]&gt;&lt;/script&gt;

&lt;p&gt;이 되고, 이는 homogeneous coordinates이기 때문에 2차원으로 바꾸면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;(-f\frac{x}{z}, -f\frac{y}{z})&lt;/script&gt;

&lt;p&gt;가 된다. 마지막 차원의 수가 1이 되도록 나눠야 하기 때문에 곱해지는 행렬에 다른 수가 곱해져도 결과는 같다.&lt;/p&gt;

&lt;h3 id=&quot;orthographic-projection&quot;&gt;Orthographic Projection&lt;/h3&gt;
&lt;p&gt;원근 사영의 특이 케이스로, 실제 모양 그대로를 깊이 정보만 없앤 채 PP에 사영시킨다. 곱셈식은 다음과 같고, 이는 실제 정보의 $(x, y, z)$ 정보에서 $(x, y)$만 가지고 오는 것과 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\left[\begin{matrix}
1 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 1 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 1
\end{matrix} \right]
\left[\begin{matrix} x \\ y \\ z \\ 1 \end{matrix}\right]=
\left[\begin{matrix} x \\ y \\ 1 \end{matrix}\right] %]]&gt;&lt;/script&gt;

&lt;p&gt;telecentric lens로 orthogonal projection한 영상을 얻을 수 있다. 이 렌즈는 일반 렌즈와 달리 거리에 따라 상의 크기가 달라지지 않으며, 치수를 측정해야 하는 산업용 카메라에 쓰인다. orthogonal projection의 여러 변이들 중에는 확대되어보이게 해주는 weak perspective, 모양을 변형시키는 Affine projection 등이 있다.&lt;/p&gt;

&lt;h3 id=&quot;camera-parameters&quot;&gt;Camera Parameters&lt;/h3&gt;
&lt;p&gt;카메라의 extrinsic 파라미터들에는 translation T와 rotation R이 있고, intrinsic 파라미터에는 focal length $f$가 있다.&lt;/p&gt;

&lt;p&gt;projection equation에서 $X$에 곱해지는 행렬 $\Pi$는 intrinsic matrix와 projection matrix, extrinsic matrix의 곱으로 이루어진다.&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;terms&quot;&gt;Terms&lt;/h2&gt;
&lt;p&gt;aperture : 조리개&lt;br /&gt;
pencil of rays : 광선속&lt;br /&gt;
diffraction limit : 회절 한계&lt;br /&gt;
camera obscura : 암실&lt;br /&gt;
point-spread function : 점분산함수&lt;br /&gt;
COP : Centor Of Projection&lt;br /&gt;
PP : Projection Plane&lt;br /&gt;
perspective : 원근&lt;/p&gt;
</description>
        <pubDate>Sat, 06 Jul 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/lecture/computervision/2019/07/06/Lecture2.html</link>
        <guid isPermaLink="true">http://localhost:4000/lecture/computervision/2019/07/06/Lecture2.html</guid>
        
        
        <category>Lecture</category>
        
        <category>ComputerVision</category>
        
      </item>
    
      <item>
        <title>Deep Video Super-Resolution Network Using Dynamic Upsampling Filters Without Explicit Motion Compensation</title>
        <description>&lt;hr /&gt;
&lt;p&gt;이 논문은 Video Super-Resolution을 위한 end-to-end 구조를 제안하는 논문이다.&lt;/p&gt;

&lt;h2 id=&quot;video-super-resolution-vsr&quot;&gt;Video Super Resolution (VSR)&lt;/h2&gt;
&lt;p&gt;VSR을 하는 직접적인 방법은 single-image super-resolution(SISR)을 프레임마다 하는 것인데, SISR은 프레임간의 관계를 고려하지 않기 때문에, 깜빡거리는 결과가 나올 수 있다. 기존의 VSR은 여러 low-resolution(LR) 프레임들을 인풋으로 받고, 연속된 LR 프레임들의 움직임을 고려하여 high-resolution(HR) 프레임들을 내놓는다. 딥러닝 기반의 VSR은 보통 모션 예측과 보정 과정으로 이루어진다. 문제점은 모션 예측에 의존성이 높고, CNN을 이용하기 때문에 블러된 아웃풋이 나온다는 점이다.&lt;/p&gt;

&lt;p&gt;이 논문에서는 새로운 end-to-end 뉴럴넷을 제안한다. 모션 정보가 dynamic upsampling filter를 만드는 데에 쓰인다. 만들어진 dynamic upsampling filter와 LR의 center frame을 가지고 HR frame이 만들어진다. 이 연구는 state-of-the-art인 VSRnet보다 더 날카로운 결과물을 만들 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;dynamic-upsampling-filters&quot;&gt;Dynamic Upsampling Filters&lt;/h2&gt;
&lt;p&gt;기존의 bilinear나 bicubic upsampling등의 방법들은 직접적인 모션 보정으로, 날카롭게 보정하기 힘들다. 이와 반대로 이 논문에서는 dynamic filter network를 사용한다. 이 filter들은 지역적으로 생성된다. dynamic filter들은 LR에서 주변 픽셀을 관찰하여 만들어지는데, 직접적인 모션 보정을 피할 수 있게 해준다.&lt;/p&gt;

&lt;h2 id=&quot;residual-learning&quot;&gt;Residual Learning&lt;/h2&gt;
&lt;p&gt;디테일들이 linear filtering으로는 보정이 되지 않을 수 있는데, 이 문제를 해결하기 위해 고주파수 디테일을 얻기 위해 residual image를 추가적으로 예측한다. 이 residual image는 여러 input 프레임(앞뒤 몇개의 프레임)을 통해 얻어진다. Dynamic Upsampling을 한 이미지가 이 residual image와 합쳐지면 더 날카롭고 앞뒤 문맥에 적합한 프레임이 얻어진다.&lt;/p&gt;

&lt;h2 id=&quot;network-design&quot;&gt;Network Design&lt;/h2&gt;
&lt;p&gt;filter와 residual generation network는 weight를 공유함으로써 오버헤드를 줄여준다. 공유된 부분의 네트워크 구조(3D layer)는 dense block에서 착안되었다. 각 input 프레임들은 공유된 2D convolutional layer들로 프로세싱되고 시간적 순서로 합쳐진다. 이것이 시공간 feature map인데, 이 3D dense block으로 들어가고, 각 가지들로 나눠져 처리되어 2개의 output(dynamic filter, residual image)를 만든다. 이후 LR 프레임이 dynamic filter와 convolution되어 upsampling된 후 residual image와 합쳐지면 HR 프레임이 얻어진다.&lt;/p&gt;

&lt;h2 id=&quot;temporal-augmentation&quot;&gt;Temporal Augmentation&lt;/h2&gt;
&lt;p&gt;training data를 만들기 위해서는 정방향, 역방향, 프레임 건너뛰기 등을 이용할 수 있다.&lt;/p&gt;

&lt;hr /&gt;
</description>
        <pubDate>Mon, 01 Jul 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/thesis/2019/07/01/VSR.html</link>
        <guid isPermaLink="true">http://localhost:4000/thesis/2019/07/01/VSR.html</guid>
        
        <category>VideoSuperResolution</category>
        
        
        <category>Thesis</category>
        
      </item>
    
  </channel>
</rss>
