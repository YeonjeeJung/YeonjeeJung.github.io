<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>YeonjeeJung's Blog</title>
    <description></description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Tue, 29 Oct 2019 22:23:34 +0900</pubDate>
    <lastBuildDate>Tue, 29 Oct 2019 22:23:34 +0900</lastBuildDate>
    <generator>Jekyll v3.8.6</generator>
    
      <item>
        <title>SGDR - Stochastic Gradient Descent with warm Restarts</title>
        <description>&lt;hr /&gt;

&lt;h2 id=&quot;abstract&quot;&gt;[Abstract]&lt;/h2&gt;
&lt;p&gt;재시작 방법은 gradient-free 최적화에서 멀티모달 함수에 적용할 때 자주 쓰인다. 부분적 재시작 또한 gradient기반 최적화에서 ill-conditioned 함수에서 수렴도를 개선하기 위해 자주 쓰이는 추세이다. 이 논문에서는 SGD를 위한 간단한 재시작 테크닉을 소개하는데, 딥네트워크를 학습시킬 때 항상(anytime) 결과를 향상시킬 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;1-introduction&quot;&gt;[1] Introduction&lt;/h2&gt;
&lt;p&gt;GD를 쓸때 hessian을 쓰면 더 좋은데 계산량이 많다. AdaDelta와 Adam은 hessian을 잘 줄여서 사용한 좋은 예이다. 그런데 sota 결과는 사실 특별한 방법을 쓴게 아니라 SGD에 momentum만 추가한 것이었다.&lt;/p&gt;

&lt;p&gt;보통의 learning rate schedule은 정해진 상수를 일정 간격의 상수로 나누는 것이었는데, 이 논문에서 제안하는 새로운 learning rate schedule은 주기적으로 SGD를 재시작하는 방법이다. 실험 결과에 의하면 재시작 방법은 원래 쓰이던 방법보다 2배에서 4배정도 적은 epoch만으로 비슷한 결과를 낼 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;2-related-work&quot;&gt;[2] Related Work&lt;/h2&gt;
&lt;p&gt;gradient-free optimization에서는 많은 local optima를 찾는 것이 목적이다. niching 방법 기반 방법들은 local optimizer를 전체 space에 다 적용시킬 수 있는데, 차원의 저주 때문에 확장시킬 수는 없다. 최근에는 다양한 재시작 매커니즘들을 사용하는데, 한 방법에서는 많은 후보를 쓰면 더 글로벌한 검색이 가능한데, 각 재시작 처음엔 적은 후보를 쓰고 각 재시작 후에는 키우는 방법을 사용하는 것이 일반적이다.&lt;/p&gt;

&lt;p&gt;gradient-based optimization에서는 gradient-free 에서보다 $n$배의 속도 향상이 있다. 이때 재시작 방법은 multimodality를 해결하기 위함보다는 수렴도를 개선하기 위해 사용된다.&lt;/p&gt;

&lt;h2 id=&quot;3-sgdr&quot;&gt;[3] SGDR&lt;/h2&gt;
&lt;p&gt;현존하는 재시작 방법은 SGD에도 적용될 수 있다. 데이터의 덩어리에 따라 loss value와 기울기가 다양할 수 있어서, 기울기나 loss의 평균을 내는 등의 노이즈의 제거가 필요하다.&lt;/p&gt;

&lt;p&gt;이 논문에서는 지정된 epoch까지 도달하면 다시 재시작을 하는 가장 간단한 재시작 방법을 사용한다. 그리고 제안된 cosine annealing이라고도 불리는 learning rate schedule은 다음과 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\eta_t=\eta_{min}^i+\frac{1}{2}(\eta_{max}^i−\eta_{min}^i)(1 + \cos(\frac{T_{cur}}{T_i}\pi))&lt;/script&gt;

&lt;p&gt;여기서 $\eta_t$는 learning rate, $\eta_{min}^i$와 $\eta_{max}^i$는 learning rate의 범위, $T_{cur}$는 현재 epoch, $T_i$는 지정된 epoch(이만큼 지나면 재시작)이다.&lt;/p&gt;

&lt;p&gt;재시작은 learning rate ($\eta_t$)을 증가시키므로써 수행되고, $x_t$는 초기 해로 사용된다. learning rate는 $\eta_{max}^i$부터 $\eta_{min}^i$까지 줄어들고, 정해진 epoch를 돌면 다시 처음부터 시작한다. $T_{mult}$라는 변수를 이용하여 재시작마다 줄어드는 간격을 점점 넓힐 수도 있다.&lt;/p&gt;

&lt;p&gt;처음 재시작 전에는 $x_t$를 초기 해로 사용하지만, 그 다음에는 이전의 최소 learning rate로부터 얻어진 $x$를 초기 해로 사용한다. (이 점이 중요한 부분임)
&lt;em&gt;그런데 계속 저렇게 사용하는거? 아님 재시작마다 저렇게 사용하는거?&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;4-experimental-results&quot;&gt;[4] Experimental Results&lt;/h2&gt;
&lt;h3 id=&quot;42-single-model-result&quot;&gt;[4.2] Single-Model Result&lt;/h3&gt;
&lt;p&gt;$T_0=200$의 결과가 가장 좋은데, 가장 마지막 몇 epoch에서 좋아진다. $T_{mult}=2$는 재시작 후 주기를 2배로 늘려주는데, 이렇게 하는 이유는 좋은 테스트 에러에 가장 빨리 도달하기 위함이다. SGDR이 좋은 성능에 빠르게 도달할 수 있기 때문에, 더 큰 신경망을 학습시킬 수 있다. 따라서 WRN을 2배 넓게 만들어서 학습시켰다.&lt;/p&gt;

&lt;p&gt;SGDR 자체 실험에서는 SGDR과 기본 스케줄을 비교했는데, 120 epoch까지는 더 빨리 training loss가 줄었다. 이 이후에 기본 스케줄은 overfit되었다. 결론적으로, SGDR은 overfit이 잘 되지 않는다.&lt;/p&gt;

&lt;h3 id=&quot;43-ensemble-results&quot;&gt;[4.3] Ensemble Results&lt;/h3&gt;
&lt;p&gt;SGDR은 WRN논문의 follow-up study에서 영감을 얻었다.  여기서는 재시작 전마다 snapshot을 찍고 그것으로 앙상블 모델을 만든다. 결과로는, 3번 돌려서 앙상블하는 것보다 한번 돌려서 3번 스냅샷 찍어서 앙상블하는게 낫다. SGDR에서 찍은 스냅샷은 앙상블을 할 때의 유용한 다양성을 제공해 준다. 이 결과는 WRN보다 더 좋은 모델에서 더 좋은 결과를 낼 것이다.&lt;/p&gt;

&lt;h3 id=&quot;45-preliminary-experiments-on-a-downsampled-imagenet-dataset&quot;&gt;[4.5] Preliminary Experiments on a Downsampled ImageNet Dataset&lt;/h3&gt;
&lt;p&gt;다운샘플된 이미지넷 데이터는 원래보다 더 어렵고 이미지의 대부분을 대상이 차지하는 CIFAR-10보다도 더 어렵다.&lt;/p&gt;

&lt;h2 id=&quot;5-discussion&quot;&gt;[5] Discussion&lt;/h2&gt;
&lt;p&gt;이 learnin rate schedule은 재시작 없이도 충분히 경쟁적이고, 단 두 개의 파라미터(초기 lr, epoch 수)만을 필요로 한다. 재시작 방법의 목적은 ‘항상(anytime)’ 좋은 성능을 내기 위함이다. 매 재시작마다 $\eta_{max}$와 $\eta_{min}$을 줄이는 방법도 가능하다. SGDR 중 얻어진 중간 모델은 앙상블에 사용될 수 있고, cost도 들지 않는다는 점을 이용했다.&lt;/p&gt;

&lt;h2 id=&quot;6-conclusion&quot;&gt;[6] Conclusion&lt;/h2&gt;
&lt;p&gt;WRN에서는 더 넓은 모델을 사용하고 스냅샷을 앙상블에 사용해 sota 결과를 만들어냈고, EEG에서는 더 재시작을 많이 하고 더 스냅샷을 많이 찍으면 더 좋은 결과를 낸다는 것을 알았다. Downsampled ImageNet 데이터에서는 SGDR이 scan을 통해 lr을 선택하는 문제를 줄여준다는 것을 알았다. 다음 연구는 AdaDelta나 Adam에 적용하는 것이 될 것이다.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;다음에 읽어볼 논문 : &lt;a href=&quot;https://arxiv.org/abs/1704.00109&quot;&gt;Snapshot&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1605.07146&quot;&gt;WRN&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Sun, 27 Oct 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/thesis/2019/10/27/SGDR.html</link>
        <guid isPermaLink="true">http://localhost:4000/thesis/2019/10/27/SGDR.html</guid>
        
        <category>Optimization</category>
        
        
        <category>Thesis</category>
        
      </item>
    
      <item>
        <title>Optimization Lecture 8</title>
        <description>&lt;h1 id=&quot;subgradient--mirror-descent&quot;&gt;Subgradient &amp;amp; Mirror Descent&lt;/h1&gt;

&lt;h2 id=&quot;subgradient&quot;&gt;Subgradient&lt;/h2&gt;

&lt;p&gt;만약 함수 $f(x)$가 미분불가능하다면, 우리는 임의의 gradient를 정해야 한다. subgradient를 정의할 수 있는데,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(y) \ge f(x) + g^T(y-x), \forall y\in X&lt;/script&gt;

&lt;p&gt;인 모든 $g$를 subgradient라고 한다. $\partial f(x)$가 subgradient의 집합을 의미한다. 원래함수 $f(x)$가 convex라면 subgradient에서 gradient descent를 써서 같은 결과를 낼 수 있다. subgradient는 저 조건만 충족하면 되기 때문에 한 점에서 여러 개의 subgradient가 발생할 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;subgradient-descent--l-lipschitz-continuous&quot;&gt;Subgradient Descent : L-Lipschitz continuous&lt;/h3&gt;

&lt;p&gt;여태까지 우리가 $\gamma = \frac{1}{\beta}$를 쓸 수 있었던 것은 $\beta$-smooth를 가정했기 때문인데, 실제 미분값 대신 subgradient를 사용해야 하는 함수라면 smooth한 함수가 아닐 가능성이 크다. 따라서 모든 subgradient에서는 Lipschitz continuous를 가정한다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}f(x_t) -f(x^* ) &amp;\le g_t^T(x_t-x^* )\\
&amp;=\frac{1}{\gamma}(x_t-x_{t+1})^T(x_t-x^* )\\
&amp;= \frac{1}{2\gamma}(\|x_t-x_{t+1}\|^2+\|x_t-x^* \|^2-\|x_{t+1}-x^* \|^2)\\
&amp;\le \|g_t\|^2+\frac{1}{2\gamma}(\|x_t-x^* \|^2-\|x_{t+1}-x^* \|^2)\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;라고 할 수 있는데, 모든 $t$에 대하여 다 더하면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{t=0}^{T-1}(f(x_t)-f(x^* ))\le\frac{1}{2\gamma}(\|x_0-x^* \|^2-\|x_T-x^* \|^2)+\sum_{t=0}^{T-1}\frac{\gamma}{2}\|g_t\|^2&lt;/script&gt;

&lt;p&gt;이라고 할 수 있다. 다시 말하면,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}f(\bar{x})-f(x^* ) &amp;\le \frac{1}{T}\sum_{t=0}^{T-1}(f(x_t)-f(x^* ))\\
&amp;\le \frac{1}{T}\{\frac{1}{2\gamma}(\|x_0-x^* \|^2-\|x_T-x^* \|^2)+\sum_{t=0}^{T-1}\frac{\gamma}{2}\|g_t\|^2\}\\
&amp;\le \frac{1}{T}(\frac{1}{2\gamma}\|x_0-x^* \|^2+\sum_{t=0}^{T-1}\frac{\gamma}{2}\|g_t\|^2) \\
&amp;\le(\frac{1}{2\gamma}R^2+\frac{\gamma}{2}B^2T)\\
&amp;= \frac{1}{T}(\frac{B\sqrt{T}}{2R}R^2+\frac{R}{2R\sqrt{T}}B^2T)\\
&amp;\le \frac{BR}{\sqrt{T}}\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;라고 할 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;mirror-descent&quot;&gt;Mirror Descent&lt;/h2&gt;

&lt;p&gt;지금까지의 모든 결과들(특히 Lipschitz에 관해서)은 유클리드 공간에서 정의되었다. 그런데 Lipschitz는 norm에 따라서 크기가 달라지는데, 다른 norm에 관해서는 어떤 convergence speed를 가지게 될까 하는 궁금증이 생기게 된다.&lt;/p&gt;

&lt;h3 id=&quot;dual-space&quot;&gt;Dual Space&lt;/h3&gt;

&lt;p&gt;이 궁금증을 해결하기 위해, 먼저 Dual space를 정의한다. 모든 벡터공간 $V$는 $V$에서 정의된 모든 선형 함수에 대해서 항상 dual space $V^* $를 갖는다. 모든 Tangent Line ($y=f(x^* )+f’(x^* )(x-x^* )$)는 항상 선형이기 때문에, 모든 gradient에 대해서는 항상 dual space를 갖는다.&lt;/p&gt;

&lt;h3 id=&quot;dual-norm&quot;&gt;Dual Norm&lt;/h3&gt;

&lt;p&gt;$\mathbb{R}^n$에서 정의된 모든 norm &lt;script type=&quot;math/tex&quot;&gt;\|\cdot \|&lt;/script&gt;에 대해 dual space에서의 norm 또한 항상 존재하는데, dual norm &lt;script type=&quot;math/tex&quot;&gt;\|\cdot\|_ *&lt;/script&gt;은 다음과 같이 정의한다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\|g\|_ * =\sup_{x\in\mathbb{R}^n:\|x\|\le 1}g^T x&lt;/script&gt;

&lt;p&gt;말이 어려운데, $p$-norm에 대해 생각해 보면 다음과 같은 관계가 있다고 한다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{1}{p}+\frac{1}{q} = 1&lt;/script&gt;

&lt;p&gt;즉, 원래 공간에서 $p$-norm을 사용하였다면 dual space에서는 $q$-norm을 사용하면 된다. 이러한 새로운 dual의 정의에서, Lipschitz는 다음과 같이 쓸 수 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\|g\|_ * \le L, \forall x \in X, \forall g \in \partial f(x)&lt;/script&gt;

&lt;h3 id=&quot;bregman-divergence&quot;&gt;Bregman Divergence&lt;/h3&gt;

&lt;p&gt;Dual space에서의 convergence를 해석하기 위해 Bregman divergence라는 것을 정의한다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;D_f(x,y)=f(x)-f(y)-\triangledown f(y)^T(x-y)&lt;/script&gt;

&lt;p&gt;사실 첫 항을 제외한 항은 Tangent Line을 의미하는 식이다. 결국 Bregman divergence는 한 점에서의 접선에 대해 같은 $y$값에서 원래 함수와 Tangent Line의 차이를 의미한다. Bregman divergence에 관한 특성으로는 다음과 같은 것이 있다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;$(\triangledown f(x)-\triangledown f(y)^T(x-z)=D_f(x,y)+D_f(z, x)-D_f(z, y)$&lt;/li&gt;
  &lt;li&gt;$\lambda$-strongly convex인 함수 $h$에 대하여 &lt;script type=&quot;math/tex&quot;&gt;D_f(x, y)\ge \frac{\lambda}{2}\|x-y\|^2\ge 0&lt;/script&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;mirror-map&quot;&gt;Mirror Map&lt;/h3&gt;

&lt;p&gt;우선 $D\in\mathbb{R}^n$은 $X\subset\bar{D}$인 open set이라고 하자. Mirror Map $\Phi$는 $D$에서 $\mathbb{R}^n$으로의 mapping function인데, 다음과 같은 조건이 있다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;$\Phi$는 convex하고 미분가능한 함수이다.&lt;/li&gt;
  &lt;li&gt;$\Phi$의 gradient는 어떤 숫자든 가능하다.&lt;/li&gt;
  &lt;li&gt;$\Phi$의 gradient는 $D$의 가장자리에서 발산한다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;이렇게 놓고 보면, 이전에 우리가 썼던 gradient descent 식 $x_{t+1}=x_t-\gamma\triangledown f(x_t)$가 좀 이상해 보이기 시작한다. $x_t$는 원래 공간인데, $\triangledown f(x_t)$는 dual space에서 정의되는 것이기 때문이다. 사실 이전에는 유클리드 norm을 기준으로 진행했기 때문에 dual space의 norm도 유클리드 norm이 되어서 상관이 없었다. 그렇지만 이제는 다르므로 gradient descent를 새롭게 정의할 필요가 있다.&lt;/p&gt;

&lt;h3 id=&quot;mirror-descent-1&quot;&gt;Mirror Descent&lt;/h3&gt;

&lt;p&gt;다음은 Dual space 공간에서의 gradient descent 알고리즘이다. Mirror Descent라고도 한다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;$x_t$를 mirror map $\triangledown \Phi (x_t)$에 매핑시킨다. 이후는 모두 dual space이다.&lt;/li&gt;
  &lt;li&gt;$\triangledown \Phi (x_t)-\gamma \triangledown f(x_t)$&lt;/li&gt;
  &lt;li&gt;$\triangledown\Phi(y_{t+1})=\triangledown\Phi(x_t)-\gamma\triangledown f(x_t)$를 만족하는 $y_{t+1}$를 찾는다.&lt;/li&gt;
  &lt;li&gt;다시 원래 공간으로 가져오는데, constrained set $X$ 안에 $x_{t+1}$이 있어야 하기 때문에 projection을 한다. &lt;script type=&quot;math/tex&quot;&gt;x_{t+1}=\Pi_X^\Phi(y_{t+1})=\arg\min_{x\in X}D_\Phi(x, y_{t+1})&lt;/script&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Mirror Descent는 proximal gradient와도 연결된다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}x_{t+1}&amp;=\arg\min_{x\in X}D_\Phi(x, y_{t+1})\\
&amp;=\arg\min_{x\in X}\{\Phi(x)-\triangledown \Phi(y_{t+1})^Tx-\Phi(y_{t+1})+\triangledown\Phi(y_{t+1})^Ty_{t+1}\}\\
&amp;=\arg\min_{x\in X}\{\Phi(x)-\triangledown\Phi(y_{t+1})^Tx\}\\
&amp;=\arg\min_{x\in X}\{\Phi(x)-(\triangledown\Phi(x_t)-\gamma\triangledown f(x_t))^Tx\}\\
&amp;=\arg\min_{x\in X}\{\gamma\triangledown f(x_t)^Tx+\Phi(x)-\Phi(x_t)-\triangledown\Phi(x_t)^T(x-x_t)\}\\
&amp;=\arg\min_{x\in X}\{\gamma\triangledown f(x_t)^Tx+D_\Phi(x, x_t)\}\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;중간에 $y$만에 관한 항들은 $\arg\min$이므로 마음대로 넣어도 상관 없고, 마찬가지로 $x_t$만에 관한 항은 마음대로 넣어도 상관없다.&lt;/p&gt;

&lt;h3 id=&quot;mirror-descent--l-lipschitz-continuous&quot;&gt;Mirror Descent : L-Lipschitz continuous&lt;/h3&gt;

&lt;p&gt;우선 함수에 대해서, $\Phi$는 $\rho$-strongly convex이고, $f$는 convex이고 L-Lipschitz이다. 그리고 &lt;script type=&quot;math/tex&quot;&gt;R^2=\sup_{x\in X}\{\Phi(x)-\Phi(x_1)\}&lt;/script&gt;이다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}f(x_t)-f(x^* ) &amp; \le g_t^T(x_t-x^* )\\
&amp;= \frac{1}{\gamma}(\triangledown\Phi(x_t)-\triangledown\Phi(y_{t+1}))^T(x_t-x^* )\\
&amp;=\frac{1}{\gamma}(D_\Phi(x^* , x_t)+D_\Phi(x_t, y_{t+1})-D_\Phi(x^* , y_{t+1}))\\
&amp;\le\frac{1}{\gamma}(D_\Phi(x^* , x_t)+D_\Phi(x_t, y_{t+1})-D_\Phi(x^* , x_{t+1})-D_\Phi(x_{t+1}, y_{t+1}))\text{ (by } \triangledown\Phi(x^* , y_{t+1})\ge D_\Phi(x^* , x_{t+1})+D_\Phi(x_{t+1}, y_{t+1})\text{)}\\
&amp;= \frac{1}{\gamma}(D_\Phi(x^* , x_t)-D_\Phi(x^* , x_{t+1}))+\frac{1}{\gamma}(D_\Phi(x_t, y_{t+1})-D_\Phi(x_{t+1}, y_{t+1}))\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;모든 $T$에 대해서 다 더하면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{t=1}^T(f(x_t)-f(x^* ))=\frac{1}{\gamma}(D_\Phi(x^* , x_1)-D_\Phi(x^* , x_{T+1}))+\frac{1}{\gamma}\sum_{t=1}^T(D_\Phi(x_t, y_{t+1})-D_\Phi(x_{t+1}, y_{t+1}))&lt;/script&gt;

&lt;p&gt;인데, 마지막 $\sum$항만 bound할 수 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}D_\Phi(x_t, y_{t+1})-D_\Phi(x_{t+1}, y_{t+1})&amp;=\Phi(x_t)-\Phi(x_{t+1})-\triangledown\Phi(y_{t+1})^T(x_t-x_{t+1})\\
&amp;\le(\triangledown\Phi(x_t)-\triangledown\Phi(y_{t+1}))^T(x_t-x_{t+1})-\frac{\rho}{2}\|x_t-x_{t+1}\|^2\text{ (by }\rho\text{-strongly convex)}\\
&amp;=\gamma g_t^T(x_t-x_{t+1})-\frac{\rho}{2}\|x_t-x_{t+1}\|^2\\
&amp;\le \gamma L\|x_t-x_{t+1}\|-\frac{\rho}{2}\|x_t-x_{t+1}\|^2\text{ (by L-Lipschitz)}\\
&amp;\le\frac{\gamma^2L^2}{2\rho}\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;마지막 항은, 그 전 식이 &lt;script type=&quot;math/tex&quot;&gt;\|x_t-x_{t+1}\|^2&lt;/script&gt;에 관한 이차식이고, 위로 볼록한 함수이기 때문에 미분해서 $0$이 되는 점이 최대점이라는 점을 이용했다. 다시 $\sum$으로 돌아가면,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}\sum_{t=1}^T(f(x_t)-f(x^* ))&amp;\le\frac{1}{\gamma}(D_\Phi(x^* , x_1)-D_\Phi(x^* , x_{T+1}))+\frac{1}{\gamma}\cdot T\cdot\frac{\gamma^2L^2}{2\rho}\\
&amp;=\frac{1}{\gamma}(D_\Phi(x^* , x_1)-D_\Phi(x^* , x_{T+1}))+\frac{\gamma TL^2}{2\rho}\\
&amp;\le\frac{1}{\gamma}D_\Phi(x^* , x_1)+\frac{\gamma TL^2}{2\rho}\text{ (by Bregman Divergence property, }D_\Phi(x^* , x_{T+1})\ge 0\text{)}\\
&amp;\le \frac{R^2}{\gamma}+\frac{\gamma TL^2}{2\rho}\text{ (아직도 왜이런지 모르겠음)}\\
&amp;\le RL\sqrt{\frac{2T}{\rho}}\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;따라서, 다음과 같은 결론이 나온다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(\bar{x})-f(x^* )\le RL\sqrt{\frac{2}{\rho T}}&lt;/script&gt;

&lt;h3 id=&quot;proof-d_phix-y_t1ge-d_phix-x_t1d_phix_t1-y_t1&quot;&gt;(Proof) &lt;script type=&quot;math/tex&quot;&gt;D_\Phi(x, y_{t+1})\ge D_\Phi(x, x_{t+1})+D_\Phi(x_{t+1}, y_{t+1})&lt;/script&gt;&lt;/h3&gt;

&lt;p&gt;위 증명에서 그냥 넘어갔던 위 명제를 증명해보자.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}D_\Phi(x, x_{t+1})&amp;=D_\Phi(x)-\Phi(x_{t+1})-\triangledown\Phi(x_{t+1})^T(x-x_{t+1})\\
D_\Phi(x_{t+1}, y_{t+1})&amp;=\Phi(x_{t+1})-\Phi(y_{t+1})-\triangledown\Phi(x_{t+1})^T(x_{t+1}-y_{t+1})\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;이다. 두 식을 더하면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;D_\Phi(x, x_{t+1})+D_\Phi(x_{t+1}, y_{t+1})=\Phi(x)-\Phi(y_{t+1})-\triangledown\Phi(y_{t+1})^T(x-y_{t+1})-(\triangledown\Phi(x_{t+1})-\triangledown\Phi(y_{t+1}))^T(x-x_{t+1})&lt;/script&gt;

&lt;p&gt;을 얻게 되는데, 마지막 항을 제외한 식은 $D_\Phi(x, y_{t+1})$이다. 마지막 항은 $-\triangledown_xD_\Phi(x_{t+1}, y_{t+1})^T(x-x_{t+1})$ 라고도 쓸 수 있는데, $x_{t+1}$은 정의에 따라 Bregman Divergence에서의 최적값이므로, 이 항은 $0$보다 무조건 작다. 따라서&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;D_\Phi(x, y_{t+1})\ge D_\Phi(x, x_{t+1})+D_\Phi(x_{t+1}, y_{t+1})&lt;/script&gt;

&lt;p&gt;가 성립함을 알 수 있다.&lt;/p&gt;
</description>
        <pubDate>Wed, 25 Sep 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/lecture/optimization/2019/09/25/OptLecture8.html</link>
        <guid isPermaLink="true">http://localhost:4000/lecture/optimization/2019/09/25/OptLecture8.html</guid>
        
        
        <category>Lecture</category>
        
        <category>Optimization</category>
        
      </item>
    
      <item>
        <title>Optimization Lecture 7</title>
        <description>&lt;h1 id=&quot;barrier-method--proximal-gradient&quot;&gt;Barrier Method &amp;amp; Proximal Gradient&lt;/h1&gt;

&lt;h2 id=&quot;barrier-method&quot;&gt;Barrier Method&lt;/h2&gt;

&lt;h3 id=&quot;inequality-constrained-problems&quot;&gt;Inequality Constrained Problems&lt;/h3&gt;

&lt;p&gt;최적화해야 하는 함수 $f(x)$가 제한된 집합 $X$에서 정의될 때 projected gradient descent 외의 다른 방법을 소개한다. 이런 constrained optimization 문제를 다르게 말하면, ‘어떤 함수 $h, g$에 대하여 모든 $1\le i \le m$인 $i$들에 대해 $h_i(x)=0$을 만족하고, 모든 $1\le j \le r$인 $j$들에 대해 $g_j(x)\le 0$을 만족하면서 $f(x)$를 최소화시켜라’라고 표현할 수 있다. 이것을 Inequality Constrained Problem이라고 한다.&lt;/p&gt;

&lt;h3 id=&quot;barrier-method-1&quot;&gt;Barrier Method&lt;/h3&gt;

&lt;p&gt;Barrier Method란, constrained 문제를 unconstrained 문제로 바꾸는 방법을 말한다. 이 때 원래의 최적화해야 하는 함수 뒤에 penalty function을 더하게 된다. 그런데 여기서, 과연 이렇게 찾은 해가 우리가 실제로 원하던 해일까? 라는 의문을 갖게 된다.&lt;/p&gt;

&lt;h3 id=&quot;lagrange-multipliers&quot;&gt;Lagrange Multipliers&lt;/h3&gt;

&lt;p&gt;라그랑지 계수로 이를 해결할 수 있는데, 위에서 소개한 Inequality Constrained Problem을 이용하자. 먼저 Lagrange function을 다음과 같이 정의할 수 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Lambda(x, \mu, \lambda):=f(x) + \sum_{i=1}^m{\mu_ih_i(x)} + \sum_{j=1}^r{\lambda_jg_j(x)}&lt;/script&gt;

&lt;p&gt;여기서 $\mu_1, \cdots, \mu_m, \lambda_1, \cdots, \lambda_r$을 Lagrange multiplier라고 한다.&lt;/p&gt;

&lt;h3 id=&quot;lagrange-dual&quot;&gt;Lagrange Dual&lt;/h3&gt;

&lt;p&gt;다음은 Lagrange function을 최적화하는 방법이다. 먼저 Lagrange dual function과 Lagrange dual problem을 정의한다.&lt;/p&gt;

&lt;p&gt;Lagrange dual function :&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L(\mu, \lambda) := \min_x\Lambda(x, \mu, \lambda)&lt;/script&gt;

&lt;p&gt;Lagrange dual problem :&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\max_{\mu, \lambda}L(\mu, \lambda) \text{ s.t. }\lambda_j\ge, \forall 1 \le j\le r&lt;/script&gt;

&lt;p&gt;먼저 Lagrange dual function 값을 구한 뒤, Lagrange dual problem을 푼다. 그 후 $L(\mu, \lambda)$를 최대화시키는 $\lambda$와 $\mu$를 찾아서 $\Lambda(x, \mu, \lambda)$에 넣고 다시 Lagrange dual function을 풀고 $\cdots$를 반복한다. 최종 나오는 $x$값이 우리가 원하던 $x^* $값이다.&lt;/p&gt;

&lt;p&gt;Lagrange dual problem을 푸는 과정에서는, $L(\mu, \lambda)$가 concave function이라는 점을 활용하면 gradient ascent를 이용해서 maximization을 할 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;proof-lmu-lambda-is-convex&quot;&gt;(Proof) $L(\mu, \lambda)$ is convex&lt;/h3&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}&amp;L(\alpha \mu^{(1)}+(1-\alpha)\mu^{(2)}, \alpha\lambda^{(1)}+(1-\alpha)\lambda^{(2)})\\
&amp;=\min_x(f(x)+\sum(\alpha\mu^{(1)}+(1-\alpha)\mu^{(2)})h_i(x) + \sum(\alpha\lambda^{(1)}+(1-\alpha)\lambda^{(2)})g_j(x))\\
&amp;\ge \alpha(\min(f(x)+\sum\mu^{(1)}h_i(x))+\sum\lambda^{(1)}g_j(x))+(1-\alpha)(\min(f(x)+\sum\mu^{(2)}h_i(x))+\sum\lambda^{(2)}g_j(x))\\
&amp;=\alpha L(\mu^{(1)}, \lambda^{(1)})+(1-\alpha)L(\mu^{(2)}, \lambda^{(2)})\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;따라서 $L(\mu, \lambda)$는 concave 함수이다.&lt;/p&gt;

&lt;h3 id=&quot;lagrange-dual-and-barrier&quot;&gt;Lagrange Dual and Barrier&lt;/h3&gt;

&lt;p&gt;다시 Barrier로 돌아오면, Lagrange dual에서 penalty function은 $\sum\mu_ih_i(x)+\sum\lambda_jg_j(x)$이다. Lagrange dual problem에서 우리는 $L(\mu, \lambda)$를 최대화시켜야 하므로,&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;$g_j(x)\lt 0$이면 $\lambda_j^* =0$이어야 하고,&lt;/li&gt;
  &lt;li&gt;$g_j(x) \gt 0$이면 $\lambda_j^* = \infty$,&lt;/li&gt;
  &lt;li&gt;$g_j(x)=0$이면 $\lambda_j^* $는 아무 숫자나 되어도 상관없으므로 $\lambda_j^* \gt 0$이게 된다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;barrier의 관점에서 봐도 말이 된다. barrier를 만족하지 못하면 penalty function이 무한대로 가게 되므로, 해가 constrained set 안으로 무조건 들어가도록 하게 된다. 따라서, Lagrange dual로 구한 $x^* $는 우리가 원하던 최적 해가 맞다.&lt;/p&gt;

&lt;p&gt;이렇게 구한 해인 $x^* (\mu^* , \lambda^* )$에는 몇 가지의 특성이 있다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;$\triangledown f(x^* (\mu^* , \lambda^* ))+\sum\mu_i^* \triangledown h_i(x^* (\mu^* , \lambda^* ))+\sum\lambda_j^* \triangledown g_j(x^* (\mu^* , \lambda^* ))=0$&lt;/li&gt;
  &lt;li&gt;$h_i(x^* (\mu^* , \lambda^* ))=0, \forall i$&lt;/li&gt;
  &lt;li&gt;$\lambda_j^* g_j(x^* (\mu^* , \lambda^* ))=0 \text{ when }\lambda_j^* =0, g_j(x^* (\mu^* , \lambda^* ))\lt 0$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;$f(x)$나 constrained set이 convex가 아니면 진짜 해와 우리가 구한 해가 차이가 나게 되는데, 이것을 Lagrange Gap이라고 한다.&lt;/p&gt;

&lt;h2 id=&quot;proximal-gradient&quot;&gt;Proximal Gradient&lt;/h2&gt;

&lt;h3 id=&quot;proximal-gradient-descent&quot;&gt;Proximal Gradient Descent&lt;/h3&gt;

&lt;p&gt;일반적으로 우리는 $f(x)$가 미분가능한 함수라고 생각하고 문제를 풀었지만, 사실 그렇지 않은 경우가 더 많다. 이런 경우를 잘 해결하기 위해 $f(x)$를 $g(x)$와 $h(x)$로 쪼갤 수 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(x) = g(x) + h(x)&lt;/script&gt;

&lt;p&gt;여기서 $g(x)$는 미분가능한 nice function이고, $h(x)$는 미분불가능할 수도 있지만 해석하기 쉬운 additional function 이다. $g(x)$와 $h(x)$는 둘다 convex이다.&lt;/p&gt;

&lt;p&gt;사실 gradient descent 방법의 함수 다음 $x$를 구하는 방법의 식은 다음과 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}x_{t+1} &amp;= x_t - \gamma\triangledown f(x)\\
&amp;= \arg\min_y\{f(x_t)+\triangledown f(x_t)^T(y-x_t)+\frac{1}{2\gamma}\|y-x_t\|^2\}\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;테일러 전개를 이용한 것인데, 마지막 항은 $\triangledown^2f(x)=\frac{1}{\gamma}I$로 대체한 것이다. &lt;em&gt;왜 이렇게 대체를 할 수 있는지는 모르겠다&lt;/em&gt; 다시 써보면,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}x_{t+1}&amp;=\arg\min_y\{g(y)+h(y)\}\\
&amp;=\arg\min_y\{g(x_t) + \triangledown g(x_t)^T(y-x_t)+\frac{1}{2\gamma}\|y-x_t\|^2 + h(y)\}\\
&amp;=\arg\min_y\{\triangledown g(x_t)^T(y-x_t)+\frac{1}{2\gamma}\|y-x_t\|^2 \frac{\gamma}{2}\|\triangledown g(x_t)\|^2+ h(y)\}\\
&amp;=\arg\min_y\{\frac{1}{2\gamma}(\|y-x_t\|^2 + 2\gamma\triangledown g(x_t)^T(y-x_t)+\gamma^2\|\triangledown g(x_t)\|^2)+h(y)\}\\
&amp;=\arg\min_y\{\frac{1}{2\gamma}\|y-(x_t-\gamma\triangledown g(x_t))\|^2+h(y)\}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;이다. 중간에 뜬금없이 추가되거나 삭제된 항은 $y$와 관계없는 항이기 때문에 추가가 가능하다. 여기서 &lt;script type=&quot;math/tex&quot;&gt;\text{prox}_{h, \gamma}(z) = \arg\min_y\{\frac{1}{2\gamma}\|y-z\|^2+h(y)\}&lt;/script&gt;라고 정의하면,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x_{t+1} = \text{prox}_{h, \gamma}(x_t-\gamma\triangledown g(x_t))&lt;/script&gt;

&lt;p&gt;라고 쓸 수 있다. 항상 이 방법이 좋은 것은 아니며, $f(x)$를 어떤 $g(x)$와 $h(x)$로 나누는지에 따라 효과가 달라진다.&lt;/p&gt;

&lt;h3 id=&quot;generalized-gradient&quot;&gt;Generalized Gradient&lt;/h3&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;G_{h, \gamma}(x) = \frac{1}{\gamma}(x-\text{prox}_{h, \gamma}(x-\gamma\triangledown g(x)))&lt;/script&gt;

&lt;p&gt;라고 하면, $x_{t+1}= x_t-\gamma G_{h, \gamma}(x)$라고 할 수 있다. 이 식으로 그냥 gradient descent와 projected gradient descent도 포함시킬 수 있는데, 만약 $h(x)=0$이면 그냥 gradient descent이고, &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
h(x)=\begin{cases}0 &amp; \text{if }x\in X \\ \infty &amp; \text{otherwise}\end{cases} %]]&gt;&lt;/script&gt; 라고 정의한다면 projected gradient descent이다.&lt;/p&gt;

&lt;h3 id=&quot;convergence-analysis-beta-smooth&quot;&gt;Convergence Analysis ($\beta$-smooth)&lt;/h3&gt;

&lt;p&gt;$\beta$-smooth function이므로 $\gamma=\frac{1}{\beta}$ 라고 하자. $G(x)=G_{h, \gamma}(x)$라고 하자.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}g(x-\gamma G(x))&amp;\le g(x) - \gamma\triangledown g(x)^TG(x)+\frac{\gamma^2\beta}{2}\|G(x)\|^2 \text{ (by }\beta\text{-smooth)}\\
&amp;\le g(x)-\frac{1}{\beta}\triangledown g(x)^TG(x)+\frac{1}{2\beta}\|G(x)\|^2 \text{ (by }\gamma=\frac{1}{\beta}\text{)}\\
f(x-\frac{1}{\beta}G(x))&amp;\le g(x)-\frac{1}{\beta}\triangledown g(x)^TG(x)+\frac{1}{2\beta}\|G(x)\|^2 + h(x-\frac{1}{\beta}G(x))\\
&amp;\le g(z)+\triangledown g(x)^T(x-z)-\frac{1}{\beta}\triangledown g(x)^TG(x)+\frac{1}{2\beta}\|G(x)\|^2 + h(z)+(G(x)-\triangledown g(x))^T(x-z-\frac{1}{\beta}G(x))\text{ (by Convexity)}\\
&amp; \le g(z)+h(z)+G(x)^T(x-z)-\frac{1}{2\beta}\|G(x)\|^2 \text{ (by }G(x)-\triangledown g(x)=\triangledown h(x-\frac{1}{\beta}G(x)\text{)})\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;$x=x_t, z=x^* $를 대입하면,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(x_t-\frac{1}{\beta}G(x_t))\le g(x^* )+h(x^* )+G(x_t)^T(x_t-x^* )-\frac{1}{2\beta}\|G(x_t)\|^2&lt;/script&gt;

&lt;p&gt;이고, $f(x_t-\frac{1}{\beta}G(x))=f(x_{t+1})$, $g(x^* )+h(x^* )=f(x^* )$이므로&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(x_{t+1})-f(x^* )\le G(x_t)^T(x_t-x^* )-\frac{1}{2\beta}\|G(x_t)\|^2&lt;/script&gt;

&lt;p&gt;이 된다.&lt;/p&gt;

&lt;h3 id=&quot;proof-gx-triangledown-gxtriangledown-hx-gamma-gx&quot;&gt;(Proof) &lt;script type=&quot;math/tex&quot;&gt;G(x)-\triangledown g(x)=\triangledown h(x-\gamma G(x))&lt;/script&gt;&lt;/h3&gt;

&lt;p&gt;위에서 그냥 넘어간 이 명제를 증명해보자.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}G(x) &amp;= \frac{1}{\gamma}(x-\text{prox}_{h, \gamma}(x-\gamma\triangledown g(x)))\\
&amp;=\frac{1}{\gamma}(x-\arg\min_y\{\frac{1}{2\gamma}\|y-(x-\gamma\triangledown g(x))\|^2 + h(y)\})\\
\Rightarrow x-\gamma G(x)&amp;=\arg\min_y\{\frac{1}{2\gamma}\|y-(x-\gamma\triangledown g(x))\|^2 + h(y)\}\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;$\arg\min$이므로 마지막 식의 우항을 미분해서 좌항을 넣으면 $0$이 되어야 한다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}&amp;\Rightarrow \frac{1}{\gamma}(x-\gamma G(x)-x+\gamma \triangledown g(x))+\triangledown h(x-\gamma G(x))=0\\
&amp;\Rightarrow G(x)-\triangledown g(x) = \triangledown h(x-\gamma G(x))\end{align} %]]&gt;&lt;/script&gt;
</description>
        <pubDate>Mon, 23 Sep 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/lecture/optimization/2019/09/23/OptLecture7.html</link>
        <guid isPermaLink="true">http://localhost:4000/lecture/optimization/2019/09/23/OptLecture7.html</guid>
        
        
        <category>Lecture</category>
        
        <category>Optimization</category>
        
      </item>
    
      <item>
        <title>Optimization Lecture 6</title>
        <description>&lt;h1 id=&quot;projected-gradient-descent&quot;&gt;Projected Gradient Descent&lt;/h1&gt;

&lt;h2 id=&quot;projected-gradient--alpha-strongly-convex--beta-smooth&quot;&gt;Projected Gradient : $\alpha$-strongly convex &amp;amp; $\beta$-smooth&lt;/h2&gt;

&lt;h3 id=&quot;recall-unconstrained-vanilla-analysis&quot;&gt;(Recall) Unconstrained Vanilla Analysis&lt;/h3&gt;

&lt;p&gt;vanilla analysis에서는&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}\|x_{t+1}-x^* \|^2&amp;\le\frac{2}{\beta}(f(x^* )-f(x_t))+\frac{1}{\beta}\|\triangledown f(x_t)\|^2+(1-\frac{\alpha}{beta})\|x_t-x^* \|^2\\
&amp;\le(1-\frac{\alpha}{\beta})\|x_t-x^* \|^2\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;을 얻을 수 있었다. 이 때 마지막 부등식은&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(x^* )-f(x_t)\le f(x_{t+1})-f(x_t)\le-\frac{1}{2\beta}\|\triangledown f(x_t)\|^2\cdots(* 1)&lt;/script&gt;

&lt;p&gt;라는 성질에 의해&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{2}{\beta}(f(x^* )-f(x_t))\le -\frac{1}{\beta^2}\|\triangledown f(x_t)\|^2&lt;/script&gt;

&lt;p&gt;를 얻어서 성립한 것이다.&lt;/p&gt;

&lt;h3 id=&quot;constrained-vanilla-analyasis&quot;&gt;Constrained Vanilla Analyasis&lt;/h3&gt;

&lt;p&gt;constrained에서는 $(* 1)$가 아닌&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(x^* )-f(x_t)\le f(x_{t+1})- f(x_t)\le-\frac{1}{2\beta}\|\triangledown f(x_t)\|^2+\frac{\beta}{2}\|y_{t+1}-x_{t+1}\|^2\cdots(* 2)&lt;/script&gt;

&lt;p&gt;가 성립한다. 따라서,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}\|x_{t+1}-x^* \|^2&amp;\le\frac{2}{\beta}(f(x^* )-f(x_t))+\frac{1}{\beta^2}\|\triangledown f(x_t)\|^2-\|y_{t+1}-x_{t+1}\|^2+(1-\frac{\alpha}{\beta})\|x_t-x^* \|^2\\
&amp;\le(1-\frac{\alpha}{\beta})\|x_t-x^* \|^2(\text{by }(* 2))\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;가 성립하므로,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}\|x_T-x^* \|^2&amp;\le(1-\frac{\alpha}{\beta})^T\|x_0-x^* \|^2\\
\|x_T-x^* \|&amp;\le(1-\frac{\alpha}{\beta})^{\frac{T}{2}}\|x_0-x^* \|\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;이다. 따라서&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}f(x_T)-f(x^* )&amp;\le\triangledown f(x^* )^T(x_T-x^* )+\frac{\beta}{2}\|x_T-x^* \|^2\\
&amp;\le\|\triangledown f(x^* )\|\|x_T-x^* \|+\frac{\beta}{2}\|x_T-x^* \|^2\\
&amp;\le\|\triangledown f(x^* )\|(1-\frac{\alpha}{\beta})^{\frac{T}{2}}\|x_0-x^* \|+\frac{\beta}{2}(1-\frac{\alpha}{\beta})^T\|x_0-x^* \|^2\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;결론적으로, Projected Gradient Descent를 사용하면 Unconstrained에서와 거의 비슷하게 수렴하고 bound하게 된다. 하지만 Projection에 많은 계산량이 사용되기 때문에 이 방법은 거의 쓰지 않는다.&lt;/p&gt;
</description>
        <pubDate>Wed, 18 Sep 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/lecture/optimization/2019/09/18/OptLecture6.html</link>
        <guid isPermaLink="true">http://localhost:4000/lecture/optimization/2019/09/18/OptLecture6.html</guid>
        
        
        <category>Lecture</category>
        
        <category>Optimization</category>
        
      </item>
    
      <item>
        <title>Optimization Lecture 5</title>
        <description>&lt;h1 id=&quot;projected-gradient-descent&quot;&gt;Projected Gradient Descent&lt;/h1&gt;

&lt;h2 id=&quot;constrained-optimization&quot;&gt;Constrained Optimization&lt;/h2&gt;

&lt;p&gt;Constrained Problem은 $f(x)$를 최소화하는 $x$를 찾는 문제인데, $x$의 범위 $X$가 주어져 있다는 점에서 이전과 다르다. 이 문제를 해결하는 방법에는 두 가지가 있는데,&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Projected Gradient Descent를 이용하는 방법과&lt;/li&gt;
  &lt;li&gt;unconstrained problem으로 바꿔서 해결하는 방법이 있다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;이번 단원은 첫번째 방법에 대한 내용이다.&lt;/p&gt;

&lt;h2 id=&quot;projected-gradient-descent-1&quot;&gt;Projected Gradient Descent&lt;/h2&gt;

&lt;p&gt;Project 는 &lt;script type=&quot;math/tex&quot;&gt;\Pi_X(y):=\arg\min_{x\in X}\|x-y\|&lt;/script&gt;, 즉 $X$바깥에 있는 $y$와 가장 가까운 $x\in X$에 매칭시켜주는 것이다.&lt;/p&gt;

&lt;p&gt;Projected gradient를 업데이트 하는 방법은 &lt;script type=&quot;math/tex&quot;&gt;x_{t+1}=\Pi_X(x_t-\gamma\triangledown f(x_t))&lt;/script&gt;이다.&lt;/p&gt;

&lt;h2 id=&quot;propconvex-constrained-problem&quot;&gt;(Prop)Convex Constrained Problem&lt;/h2&gt;

&lt;p&gt;$f$가 convex function이고 $X$가 convex set이고, $x^* $가 $X$범위에 대해 $f$의 minimizer라면,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}f(x^* )\le f(y), \forall y\in X \Leftrightarrow \triangledown f(x^* )^T(x-x^* )\ge 0\cdots(* 1)\end{align}&lt;/script&gt;

&lt;p&gt;이 성립한다.&lt;/p&gt;

&lt;h3 id=&quot;proof-leftarrow&quot;&gt;(Proof) $\Leftarrow$&lt;/h3&gt;

&lt;p&gt;$f$가 convex function이라고 했으므로,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(x)\ge f(x^* ) + \triangledown f(x^* )^T(x-x^* )&lt;/script&gt;

&lt;p&gt;를 항상 만족한다. 따라서 $\triangledown f(x^* )^T(x-x^* )\ge 0$이면 $f(x)\ge f(x^* )$이므로 증명은 끝난다.&lt;/p&gt;

&lt;h3 id=&quot;proof-rightarrow&quot;&gt;(Proof) $\Rightarrow$&lt;/h3&gt;

&lt;p&gt;대우명제를 이용한다. $\triangledown f(x^* )^T(x-x^* )\lt 0$이라고 하자. 그러면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\triangledown f(x^* )^T(x-x^* )=\lim_{t\rightarrow 0+}\frac{f(x^* +t(x-x^* ))-f(x^* )}{t(x-x^* )}(x-x^* )\lt 0&lt;/script&gt;

&lt;p&gt;이 성립하므로,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(x^* +t(x-x^* ))-f(x^* )\lt 0&lt;/script&gt;

&lt;p&gt;이 성립하게 된다. 이는 $f(x^* )\le f(y), \forall y\in X$에 부합하지 않으므로, 증명은 끝난다.&lt;/p&gt;

&lt;h2 id=&quot;prop-properties-of-projection&quot;&gt;(Prop) Properties of Projection&lt;/h2&gt;

&lt;p&gt;$X$가 closed 이고 convex라고 하고, $x\in X, y\in \mathbb{R}^d$라고 하자. 그러면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}&amp;(x-\Pi_X(y))^T(y-\Pi_X(y))\le 0\cdots(* 2)\\
&amp;\|x-\Pi_X(y)\|^2+\|y-\Pi_X(y)\|^2\le\|x-y\|^2\cdots(* 3)\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;가 항상 성립한다.&lt;/p&gt;

&lt;h3 id=&quot;proof--2&quot;&gt;(Proof) $(* 2)$&lt;/h3&gt;

&lt;p&gt;$\Pi_X(y)$는 &lt;script type=&quot;math/tex&quot;&gt;d(x)=\|x-y\|^2&lt;/script&gt;의 minimizer이므로, $(* 1)$의 $\Rightarrow$를 이용하면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\triangledown d(\Pi_X(y))^T(x-\Pi_X(y))\ge 0&lt;/script&gt;

&lt;p&gt;이 항상 성립한다. &lt;script type=&quot;math/tex&quot;&gt;d(x) = \|x-y\|^2=(x-y)^T(x-y)&lt;/script&gt;이므로, &lt;script type=&quot;math/tex&quot;&gt;\triangledown d(x) = 2(x-y)&lt;/script&gt;이다. 따라서&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;(\Pi_X(y)-y)^T(x-\Pi_X(y))\ge 0&lt;/script&gt;

&lt;p&gt;이고, 바꿔 말하면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;(x-\Pi_X(y))^T(y-\Pi_X(y))\le 0&lt;/script&gt;

&lt;p&gt;이다.&lt;/p&gt;

&lt;h3 id=&quot;proof--3&quot;&gt;(Proof) $(* 3)$&lt;/h3&gt;

&lt;p&gt;$v:=\Pi_X(y), w:=y-\Pi_X(y)$라고 하자. $(* 2)$에 의해&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;0\le 2v^Tw = \|v\|^2+\|w\|^2-\|v-w\|^2&lt;/script&gt;

&lt;p&gt;이므로,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\|x-\Pi_X(y)\|^2+\|y-\Pi_X(y)\|^2\le\|x-y\|^2&lt;/script&gt;

&lt;p&gt;가 성립한다.&lt;/p&gt;

&lt;h2 id=&quot;projected-gradient--lipschitz-convex&quot;&gt;Projected Gradient : Lipschitz Convex&lt;/h2&gt;

&lt;p&gt;$f:\mathbb{R}^d\rightarrow\mathbb{R}$가 convex function이고 미분가능하다고 하자. $X\subset\mathbb{R}^d$는 closed 이고 convex라고 하고, $x^* \in X$는 $f$에 대한 minimizer라고 하자. &lt;script type=&quot;math/tex&quot;&gt;\|x_0-x^* \|\le R, x_0\in X&lt;/script&gt;이고, &lt;script type=&quot;math/tex&quot;&gt;\|\triangledown f(x)\|\le B, \forall x\in X&lt;/script&gt;라고 하자 (B-Lipschitz continuous). step size $\gamma = \frac{R}{B\sqrt{T}}$라고 지정하면,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(\bar{x})-f(x^* )\le\frac{1}{T}\sum_{t=0}^{T-1}f(x_t)-f(x^* )\le\frac{RB}{\sqrt{T}}&lt;/script&gt;

&lt;p&gt;의 boundary를 갖는다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Lipschitz Continuous에서는 $f(x_{t+1})\le f(x_t)$를 보장하지 못하기 때문에, $f(x_T)\le f(x_t)$라고 장담할 수가 없다. 따라서 $f(\bar{x})-f(x^ *)$를 bound시키는 것이다.&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;vanilla-analysis&quot;&gt;Vanilla Analysis&lt;/h3&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;y_{t+1}=x_t-\gamma\triangledown f(x_t), x_{t+1}=\Pi_X(y_{t+1})&lt;/script&gt;이라고 하자. vanilla anaylsis를 이용하면,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}f(x_t)-f(x^* ) &amp; \le \triangledown f(x_t)^T(x_t-x^* )\\
&amp;=\frac{1}{\gamma}(x_t-y_{t+1})^T(x_t-x^* )\\
&amp;=\frac{1}{2\gamma}(\|x_t-y_{t+1}\|^2+\|x_t-x^* \|^2-\|y_{t+1}-x^* \|^2)\\
&amp;\le\frac{\gamma}{2}\|\triangledown f(x_t)\|^2+\frac{1}{2\gamma}(\|x_t-x^* \|-\|x_{t+1}-x^* \|^2)-\frac{1}{2\gamma}\|y_{t+1}-x_{t+1}\|^2
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;(마지막 줄은 &lt;script type=&quot;math/tex&quot;&gt;(y_{t+1}=x_t-\gamma\triangledown f(x_t))&lt;/script&gt;와 $(3)$을 사용했다). 이 식은 원래의 vanilla anaylsis에서 $-\frac{1}{2\gamma}|y_{t+1}-x_{t+1}|^2$만 추가된 것이다. 모든 $t$에 대해서 다 더하면,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}\sum_{t=0}^{T-1}(f(x_t)-f(x^* ))&amp;\le\frac{1}{2\gamma}(\|x_0-x^* \|^2-\|x_T-x^* \|^2)+\sum_{t=0}^{T-1}\frac{\gamma}{2}\|\triangledown f(x_t)\|^2-\sum_{t=0}^{T-1}\frac{1}{2\gamma}\|y_{t+1}-x_{t+1}\|^2 \\
&amp;\le \frac{1}{2\gamma}(\|x_0-x^* \|^2-\|x_T-x^* \|^2)+\sum_{t=0}^{T-1}\frac{\gamma}{2}\|\triangledown f(x_t)\|^2\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;를 얻게 되는데, 이는 결국 vanilla anaylsis와 같은 결론이다.&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\bar{x}=\frac{x_0+\cdots+x_{T-1}}{T}&lt;/script&gt; 라고 하면,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}f(\bar{x})-f(x^* )&amp;\le\frac{1}{T}\sum_{t=0}^{T-1}(f(x_t)-f(x^* ))\text{(Jensen's inequality)}\\
&amp;\le\frac{1}{2\gamma T}R^2+\frac{1}{T}\sum_{t=0}^{T-1}\frac{\gamma}{2}\|\triangledown f(x_t)\|^2\\
&amp;\le\frac{RB}{2\sqrt{T}}+\frac{RB}{2\sqrt{T}}(\gamma = \frac{R}{B\sqrt{T}})\\
&amp;\le\frac{RB}{\sqrt{T}}\end{align} %]]&gt;&lt;/script&gt;

&lt;h2 id=&quot;projected-gradient--beta-smooth-functions&quot;&gt;Projected Gradient : $\beta$-smooth functions&lt;/h2&gt;

&lt;h3 id=&quot;recall-beta-smooth&quot;&gt;(Recall) $\beta$-smooth&lt;/h3&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}f(x_t)-f(x_{t+1})&amp;\ge\triangledown f(x_t-x_{t+1})-\frac{\beta}{2}\|x_t-x_{t+1}\|^2\\
&amp;=\gamma\|\triangledown f(x_t)\|^2-\frac{\gamma^2\beta}{2}\|\triangledown f(x_t)\|^2\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;unconstrained 에서와는 다르게, constrained에서는&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\gamma\triangledown f(x_t)\neq x_t-x_{t+1}&lt;/script&gt;

&lt;p&gt;이다. 대신 $\gamma = \frac{1}{\beta}$를 대입하면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(x_{t+1})\le f(x_t)-\frac{1}{2\beta}\|\triangledown f(x_t)\|^2 + \frac{\beta}{2}\|y_{t+1}-x_{t+1}\|^2\cdots(* 4)&lt;/script&gt;

&lt;p&gt;를 얻을 수 있다. 주의할 점은, &lt;script type=&quot;math/tex&quot;&gt;\frac{\beta}{2}\|y_{t+1}-x_{t+1}\|^2&lt;/script&gt;때문에 monotone decrese (&lt;script type=&quot;math/tex&quot;&gt;f(x_{t+1}\le f(x_t))&lt;/script&gt;)를 확신할 수 없으므로 이를 먼저 증명해야 한다.&lt;/p&gt;

&lt;h3 id=&quot;proof-fx_t1le-fx_t&quot;&gt;(Proof) &lt;script type=&quot;math/tex&quot;&gt;f(x_{t+1})\le f(x_t)&lt;/script&gt;&lt;/h3&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}f(x_{t+1})&amp;\le f(x_t)+\triangledown f(x_t)^T(x_{t+1}-x_t)+\frac{\beta}{2}\|x_{t+1}-x_t\|^2 (\beta\text{-smooth})\\
&amp;=f(x_t)-\beta(y_{t+1}-x_t)^T(x_{t+1}-x_t)+\frac{\beta}{2}\|x_{t+1}-x_t\|^2(y_{t+1}-x_t=\frac{1}{\beta}\triangledown f(x_t))\\
&amp;=f(x_t)-\frac{\beta}{2}(\|y_{t+1}-x_t\|^2+\|x_{t+1}-x_t\|^2-\|y_{t+1}-x_{t+1}\|^2)+\frac{\beta}{2}\|x_{t+1}-x_t\|^2\\
&amp;=f(x_t)-\frac{\beta}{2}\|y_{t+1}-x_t\|^2+\frac{\beta}{2}\|y_{t+1}-x_{t+1}\|^2\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;$(* 3)$에 의해&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\|x_t-x_{t+1}\|^2+\|y_{t+1}-x_{t+1}\|^2\le\|x_t-y_{t+1}\|^2&lt;/script&gt;

&lt;p&gt;이므로&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;-\frac{\beta}{2}\|y_{t+1}-x_t\|^2+\frac{\beta}{2}\|y_{t+1}-x_{t+1}\|^2\le 0&lt;/script&gt;

&lt;p&gt;이 된다. 따라서&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}f(x_{t+1})-f(x_t)&amp;\le-\frac{\beta}{2}\|y_{t+1}-x_t\|^2+\frac{\beta}{2}\|y_{t+1}-x_{t+1}\|^2\\&amp;\le 0\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;이므로, $f(x_{t+1})\le f(x_t)$를 얻을 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;vanilla-analysis-1&quot;&gt;Vanilla Analysis&lt;/h3&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}\sum_{t=0}^{T-1}(f(x_t)-f(x^* ))&amp;\le\frac{\beta}{2}(\|x_0-x^* \|^2-\|x_T-x^* \|^2)+\sum_{t=0}^{T-1}\left(\frac{1}{2\beta}\|\triangledown f(x_t)\|^2-\frac{\beta}{2}\|y_{t+1}-x_{t+1}\|^2\right)\\
&amp;\le\frac{\beta}{2}\|x_0-x^* \|^2+\sum_{t=0}^{T-1}(f(x_t)-f(x_{t+1}))((* 4)\text{에 의해})\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;우변의 마지막 항을 좌변으로 넘기면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{t=1}^T(f(x_t)-f(x^* ))\le\frac{\beta}{2}\|x_0-x^* \|^2&lt;/script&gt;

&lt;p&gt;를 얻게 되고,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}f(x_T)-f(x^* )&amp;\le\frac{1}{T}\sum_{t=1}^T(f(x_t)-f(x^* ))(f(x_T)\le f(x_t),\forall t\lt T)\\
&amp;\le\frac{\beta}{2T}\|x_0-x^* \|^2\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;로 bound된다.&lt;/p&gt;
</description>
        <pubDate>Mon, 16 Sep 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/lecture/optimization/2019/09/16/OptLecture5.html</link>
        <guid isPermaLink="true">http://localhost:4000/lecture/optimization/2019/09/16/OptLecture5.html</guid>
        
        
        <category>Lecture</category>
        
        <category>Optimization</category>
        
      </item>
    
      <item>
        <title>Optimization Lecture 4</title>
        <description>&lt;h1 id=&quot;convex-optimization&quot;&gt;Convex Optimization&lt;/h1&gt;

&lt;h2 id=&quot;beta-smooth-functions--frac1t-learning-rate&quot;&gt;$\beta$-smooth functions : $\frac{1}{t}$ learning Rate&lt;/h2&gt;

&lt;p&gt;$f$가 $\beta$-smooth하다고 하면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}f(x_t)-f(x_{t+1}) &amp; \ge \triangledown f(x_t)^T(x_t-x_{t+1})-\frac{\beta}{2}\|x_t-x_{t+1}\|^2\\
 &amp; =\gamma\|\triangledown f(x_t)\|^2-\frac{\gamma^2\beta}{2}\|\triangledown f(x_t)\|^2 (x_{t+1}-x_t=-\gamma\triangledown f(x_t), \text{gradient descent})\\
 &amp; =(\gamma-\frac{\gamma^2\beta}{2})\|\triangledown f(x_t)\|^2\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;이다. 이 식에서의 극점은 $\gamma = \frac{1}{\beta}$일 때이므로 이를 대입하면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}f(x_t)-f(x_{t+1})\ge \frac{1}{2\beta}\|\triangledown f(x_t)\|^2\end{align}&lt;/script&gt;

&lt;p&gt;를 얻게 된다. 이 때 $t=0$부터 $t=T-1$까지 다 대입한 후 다 더하면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}f(x_0)-f(x_T) &amp; \ge \sum_{t=0}^{T-1}\frac{1}{2\beta}\|\triangledown f(x_t)\|^2\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;을 얻게 된다. 이전 단원의 Vanilla Analysis 중간에서 얻은 식에 $\gamma=\frac{1}{\beta}$를 대입하면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{t=0}^{T-1}{(f(x_t)-f(x^* ))}\le\frac{\beta}{2}\|x_0-x^* \|^2+\sum_{t=0}^{T-1}\frac{1}{2\beta}\|\triangledown f(x_t)\|^2&lt;/script&gt;

&lt;p&gt;을 얻는데, $(4)$을 이용하면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{t=0}^{T-1}{(f(x_t)-f(x^* ))}\le\frac{\beta}{2}\|x_0-x^* \|^2+\sum_{t=0}^{T-1}\frac{1}{2\beta}\|\triangledown f(x_t)\|^2 \le \frac{\beta}{2}\|x_0-x^* \|^2+ f(x_0)-f(x_T)&lt;/script&gt;

&lt;p&gt;이라고 할 수 있고, 맨 우측 두 항을 좌변으로 넘기면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{t=1}^T(f(x_t)-f(x^* ))\le\frac{\beta}{2}\|x_0-x^* \|^2&lt;/script&gt;

&lt;p&gt;를 얻는다. $f(x_T)\le f(x_t), 0\le \forall t\le T$이므로, 최종적으로&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(x_T)-f(x^* )\le\frac{1}{T}\sum_{t=1}^T(f(x_t)-f(x^* ))\le\frac{\beta}{2T}\|x_0-x^* \|^2&lt;/script&gt;

&lt;p&gt;를 얻는다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;upper bound와 convergence speed에 관한 설명 추가&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;def-linear-convergence&quot;&gt;(Def) Linear Convergence&lt;/h3&gt;

&lt;p&gt;만약&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\lim_{t\rightarrow\infty}{\frac{f(x_{t+1})-f(x^* )}{f(x_t)-f(x^* )}}=c&lt;/script&gt;

&lt;p&gt;를 만족하는 $c\in(0, 1)$가 존재한다면 $f$는 선형적으로 수렴한다고 말할 수 있다. 만약 $\frac{f(x_{t+1})-f(x^* )}{f(x_t)-f(x^* )}\le c, \forall t, \forall c\in (0, 1)$라면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}f(x_t)-f(x^* )\le c^t(f(x_0)-f(x^* ))\end{align}&lt;/script&gt;

&lt;p&gt;가 항상 성립한다.&lt;/p&gt;

&lt;h2 id=&quot;alpha-strongly-convex-and-beta-smooth-function&quot;&gt;$\alpha$-strongly convex and $\beta$-smooth function&lt;/h2&gt;

&lt;p&gt;Vanilla Analysis의 중간 과정에서&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\triangledown f(x_t)^T(x_t-x^* )=\frac{\gamma}{2}\|\triangledown f(x_t)\|^2+\frac{1}{\gamma}(\|x_t-x^* \|^2-\|x_{t+1}-x^* \|^2)&lt;/script&gt;

&lt;p&gt;의 식을 얻을 수 있었다. 그리고 $f$가 $\alpha$-strongly convex라고 하면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\triangledown f(x_t)^T(x_t-x^* )\ge f(x_t)-f(x^* )+\frac{\alpha}{2}\|x_t-x^* \|^2&lt;/script&gt;

&lt;p&gt;를 만족한다. 둘을 합치고 변형하면,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align} f(x_t)-f(x^* )+\frac{\alpha}{2}\|x_t-x^* \|^2&amp;\le\frac{\gamma}{2}\|\triangledown f(x_t)\|^2+\frac{1}{2\gamma}(\|x_t-x^* \|^2-\|x_{t+1}-x^* \|^2)\\
 -\frac{1}{2\gamma}(\|x_t-x^* \|^2-\|x_{t+1}-x^* \|^2)+\frac{\alpha}{2}\|x_t-x^* \|^2 &amp;\le f(x^* )-f(x_t )+ \frac{\gamma}{2}\|\triangledown f(x_t)\|^2\\
 -\|x_t-x^* \|^2+\|x_{t+1}-x^* \|^2+\alpha\gamma\|x_t-x^* \|^2 &amp;\le 2\gamma(f(x^* )-f(x_t ))+ \gamma^2\|\triangledown f(x_t)\|^2\\
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;을 얻을 수 있다. 이 때 $\gamma = \frac{1}{\beta}$를 대입하면 우변은&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}2\gamma(f(x^* )-f(x_t ))+ \gamma^2\|\triangledown f(x_t)\|^2&amp;=\frac{2}{\beta}(f(x^* )-f(x_t))+\frac{1}{\beta^2}\|f(x_t)\|^2\\
&amp;\le\frac{2}{\beta}(f(x_{t+1})-f(x_t))+\frac{1}{\beta^2}\|\triangledown f(x_t)\|^2\\
&amp;\le-\frac{1}{\beta^2}\|\triangledown f(x_t)\|^2+\frac{1}{\beta^2}\|\triangledown f(x_t)\|^2\\
&amp;=0\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;($(11)\rightarrow(12)$은 $(4)$ 때문이다.) $(8)$을 정리하면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\|x_{t+1}-x^* \|^2\le \left(1-\frac{\alpha}{\beta}\right)\|x_t-x^* \|^2&lt;/script&gt;

&lt;p&gt;이라고 할 수 있고, $(6)$에 의해&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\|x_T-x^* \|^2\le\left(1-\frac{\alpha}{\beta}\right)^T\|x_0-x^* \|^2&lt;/script&gt;

&lt;p&gt;가 성립한다.&lt;/p&gt;

&lt;h2 id=&quot;convergence&quot;&gt;Convergence&lt;/h2&gt;

&lt;p&gt;$f$가 $\beta$-smooth라면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}f(x_T)-f(x^* )&amp;\le\triangledown f(x^* )^T(x_T-x^* )+\frac{\beta}{2}\|x_T-x^* \|^2\\
&amp;=\frac{\beta}{2}\|x_T-x^* \|^2\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;이므로,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(x_T)-f(x^* )\le\frac{\beta}{2}\|x_T-x^* \|^2\le\frac{\beta}{2}\left(1-\frac{\alpha}{\beta}\right)^T\|x_0-x^* \|^2&lt;/script&gt;

&lt;p&gt;가 성립한다. 이 때 $\frac{\alpha}{\beta}$를 condition number라고 하는데, 이 값이 $1$이면, 즉 $\alpha = \beta$이면 한 번의 이터레이션으로 최적값을 찾을 수 있다.&lt;/p&gt;
</description>
        <pubDate>Wed, 11 Sep 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/lecture/optimization/2019/09/11/OptLecture4.html</link>
        <guid isPermaLink="true">http://localhost:4000/lecture/optimization/2019/09/11/OptLecture4.html</guid>
        
        
        <category>Lecture</category>
        
        <category>Optimization</category>
        
      </item>
    
      <item>
        <title>Optimization Lecture 3</title>
        <description>&lt;h1 id=&quot;convex-optimization&quot;&gt;Convex Optimization&lt;/h1&gt;
&lt;h2 id=&quot;gradient-descent&quot;&gt;Gradient Descent&lt;/h2&gt;
&lt;h3 id=&quot;gradient-descent-algorithm&quot;&gt;Gradient Descent Algorithm&lt;/h3&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x_t \leftarrow x_t - \gamma\triangledown f(x_t)&lt;/script&gt;

&lt;p&gt;이거나, $\gamma$를 $t$의 함수로 표현하여&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x_t \leftarrow x_t - \gamma_t\triangledown f(x_t)&lt;/script&gt;

&lt;p&gt;로 업데이트 하여 최적점을 찾아가는 방법을 gradient descent 알고리즘이라고 한다. 이 때 $\gamma_t$는 $\frac{1}{t}$, $\frac{1}{\sqrt{t}}$, $\frac{1}{\log{t}}$, $\cos{\theta t}$가 주로 쓰인다.&lt;/p&gt;

&lt;h3 id=&quot;convergence-rate&quot;&gt;Convergence Rate&lt;/h3&gt;

&lt;p&gt;Convergence rate은&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(x_t)-f(x^* )\le\epsilon&lt;/script&gt;

&lt;p&gt;을 만족하는 $t$를 찾기까지의 시행 수를 Big O 표기법으로 나타내는 것이다.&lt;/p&gt;

&lt;h2 id=&quot;strong-convex&quot;&gt;Strong Convex&lt;/h2&gt;
&lt;h3 id=&quot;alpha-strong-convex&quot;&gt;$\alpha$-Strong Convex&lt;/h3&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(x)-f(y)\le\triangledown f(x)^T(x-y)-\frac{\alpha}{2}\|x-y\|^2, \forall x,y&lt;/script&gt;

&lt;p&gt;를 만족할 때의 $\alpha$를 앞에 붙여 $\alpha$-strong convex라고 한다. 만약 $f$가 미분불가능하면 subgradient로 $\alpha$-strongly convexity를 정할 수 있다. 다르게는&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(y)\ge f(x)+\triangledown f(x)^T(y-x)+\frac{\alpha}{2}\|x-y\|^2&lt;/script&gt;

&lt;p&gt;로 쓸 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;smooth&quot;&gt;Smooth&lt;/h2&gt;
&lt;h3 id=&quot;beta-smooth&quot;&gt;$\beta$-Smooth&lt;/h3&gt;
&lt;p&gt;$\triangledown f$가 $\beta$-Lipschitz라고 하면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\|\triangledown f(x)-\triangledown f(y)\|\le\beta\|x-y\|&lt;/script&gt;

&lt;p&gt;를 항상 만족하는데, 이때의 $\beta$를 앞에 붙여 $f$ 를 $\beta$-smooth라고 한다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(x)-f(y) \ge \triangledown f(x)^T(x-y)-\frac{\beta}{2}\|x-y\|^2, \forall x,y&lt;/script&gt;

&lt;p&gt;라고도 쓸 수 있고, 또는&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(y) \le f(x)+\triangledown f(x)^T(y-x)+\frac{\beta}{2}\|x-y\|^2&lt;/script&gt;

&lt;p&gt;라고도 쓸 수 있다.&lt;/p&gt;

&lt;p&gt;$\beta$-smooth 함수에서 gradient descent 알고리즘을 사용해 최적값을 찾을 때에는 &lt;script type=&quot;math/tex&quot;&gt;\|\triangledown f(x)-\triangledown f(x^* )\|\le\beta \|x-x^* \|&lt;/script&gt;이고, $\triangledown f(x^* )=0$이기 때문에 learning rate가 $\gamma = \frac{1}{\beta}$인 것이 좋다. 하지만 보통의 convex optimization에서는 $\beta$를 모르기 때문에 learning rate를 정하는 것이 어렵다.&lt;/p&gt;

&lt;p&gt;어떤 함수가 $\alpha$-strong이고 $\beta$-smooth이면 항상 $\alpha \le \beta$가 성립한다. 또한, convex optimization에서 $\alpha = \beta$일 때 가장 쉽게 해를 찾을 수 있다. 항상 $0 \le \frac{\alpha}{\beta} \le 1$ 이므로, $\frac{\alpha}{\beta}$가 1에 가까울수록 해를 찾기가 쉽다.&lt;/p&gt;

&lt;h2 id=&quot;second-order-characterization-of-convexity&quot;&gt;Second-order Characterization of convexity&lt;/h2&gt;

&lt;p&gt;$f$가 두 번 미분가능하면 $\forall x\in\mathbf{dom}{(f)}$에서 항상 $\triangledown^2f(x)$가 존재한다. 그리고&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f \text{ is convex }\Leftrightarrow \triangledown^2 f(x) \text{ is positive semidefinite }\forall x\in\mathbf{dom}{(f)}&lt;/script&gt;

&lt;p&gt;가 항상 성립한다.&lt;/p&gt;

&lt;h3 id=&quot;def-positive-definite&quot;&gt;(Def) Positive Definite&lt;/h3&gt;

&lt;p&gt;$A\in\mathbb{R}^{d\times d}$는&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x^TAx \gt 0, \forall x\in \mathbb{R}^d&lt;/script&gt;

&lt;p&gt;를 만족할 때 positive definite라고 한다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x^TAx\ge 0 , \forall x\in \mathbb{R}^d&lt;/script&gt;

&lt;p&gt;를 만족하면 positive semidefinite이라고 한다.&lt;/p&gt;

&lt;h2 id=&quot;hessian-smooth-strong-convexity&quot;&gt;Hessian, Smooth, Strong convexity&lt;/h2&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
f :\alpha\text{-strongly convex &amp; } \beta\text{-smooth } \Leftrightarrow \alpha I \le \triangledown^2f(x)\le\beta I %]]&gt;&lt;/script&gt;

&lt;p&gt;를 항상 만족하는데, 행렬 $A, B$에 대하여 $A\le B$는 $B-A$가 positive semidefinite임을 뜻한다. 또한, 이는 $\triangledown^2 f(x)$의 eigenvalue들이 $\alpha$보다 크고 $\beta$보다 작음을 뜻한다.&lt;/p&gt;

&lt;p&gt;hessian은 이러한 성질 때문에 step size를 정하기에 매우 중요한데, 이를 second-order method라고 한다. 하지만 계산량이 많고 저장할 숫자도 많으므로 딥러닝에서는 first-order method만 사용한다.&lt;/p&gt;

&lt;h3 id=&quot;proof-rightarrow&quot;&gt;(Proof) $\Rightarrow$&lt;/h3&gt;

&lt;p&gt;우선 $\alpha$-strongly convex이기 때문에&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;f(y)-f(x)\le\triangledown f(y)^T(y-x)-\frac{\alpha}{2}\|x-y\|^2&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;f(x)-f(y)\le\triangledown f(x)^T(x-y)-\frac{\alpha}{2}\|x-y\|^2&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;가 항상 성립한다. 마찬가지로 $\beta$-smooth이기 때문에&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;f(y)-f(x)\ge\triangledown f(y)^T(y-x)-\frac{\beta}{2}\|x-y\|^2&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;f(x)-f(y)\ge\triangledown f(x)^T(x-y)-\frac{\beta}{2}\|x-y\|^2&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;가 항상 성립한다. $\alpha$에 대한 두 식을 합치면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;0\le(\triangledown f(x)-\triangledown f(y))^T(x-y)-\alpha\|x-y\|^2\cdots(1)&lt;/script&gt;

&lt;p&gt;를 얻게 되고, 마찬가지로 $\beta$에 대한 식을 합치면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;0\ge(\triangledown f(x)-\triangledown f(y))^T(x-y)-\beta\|x-y\|^2\cdots(2)&lt;/script&gt;

&lt;p&gt;를 얻게 된다. $x=y+ht$룰 대입하면 ($h$는 벡터, $t$는 스칼라)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;(\triangledown f(x)-\triangledown f(y))^T(x-y) = \frac{\triangledown f(y+ht)-\triangledown f(y)}{t}ht^2&lt;/script&gt;

&lt;p&gt;를 얻을 수 있는데, 여기에 극한을 취하면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\lim_{t\rightarrow 0}{\frac{\triangledown f(y+ht)-\triangledown f(y)}{t}}ht^2=h^T\triangledown^2f(y)ht^2&lt;/script&gt;

&lt;p&gt;을 얻을 수 있다. 이를 $(1)$과 $(2)$에 대입하면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\alpha t^2\|h\|^2\le h^T\triangledown^2f(y)t^2h\le \beta t^2\|h\|^2&lt;/script&gt;

&lt;p&gt;를 얻게 되고,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;h^T\alpha h\le h^T\triangledown^2 f(y)h\le h^T\beta h, \forall h&lt;/script&gt;

&lt;p&gt;를 얻을 수 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;0\le h^T(\triangledown^2 f(y)-\alpha)h, 0\le h^T(\beta-\triangledown^2 f(y))h , \forall h&lt;/script&gt;

&lt;p&gt;이므로,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\alpha I \le \triangledown^2 f(y)\le \beta I&lt;/script&gt;

&lt;p&gt;와 동치이다.&lt;/p&gt;

&lt;h3 id=&quot;proof-leftarrow&quot;&gt;(Proof) $\Leftarrow$&lt;/h3&gt;

&lt;p&gt;Taylor Thm을 이용하면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(y) = f(x) + \triangledown f(x)^T(y-x) + \frac{1}{2}(y-x)^T\triangledown^2f(c)(y-x), c=x+(y-x)t, \forall t\in[0, 1]&lt;/script&gt;

&lt;p&gt;라고 할 수 있는데, 이 때&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{1}{2}(y-x)T\triangledown f(c)^T(y-x)\ge\frac{1}{2}k\|y-x\|^2&lt;/script&gt;

&lt;p&gt;를 만족하는 $k$를 $\alpha$라고 하고,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{1}{2}(y-x)T\triangledown f(c)^T(y-x)\le\frac{1}{2}k\|y-x\|^2&lt;/script&gt;

&lt;p&gt;를 만족하는 $k$를 $\beta$라고 하면 $f$는 $\alpha$-strongly convex이고 $\beta$-smooth이다.&lt;/p&gt;

&lt;h2 id=&quot;vanila-analysis&quot;&gt;Vanila Analysis&lt;/h2&gt;
&lt;p&gt;만약 $f$가 strongly convex도 아니고, smooth하지도 않지만 $L$-Lipschitz continuous하다고 했을 때, $f(x)-f(x^* )$를 gradient descent를 이용해 어떻게 bound하는지에 대해 알아보자. 여기서는 &lt;script type=&quot;math/tex&quot;&gt;2a^Tb = \|a\|^2+\|b\|^2-\|a-b\|^2 \cdots(* )&lt;/script&gt;임을 이용할 것이다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}f(x_t)-f(x^* )&amp;\le\triangledown f(x_t)^T(x_t-x^* )\\
&amp;=\frac{1}{\gamma}(x_t-x_{t+1})^T(x_t-x^* )(x_{t+1} = x_t-\gamma\triangledown f(x_t)\\
&amp;=\frac{1}{2\gamma}(\|x_t-x_{t+1}\|^2+\|x_t-x^* \|^2-\|x_{t+1}-x^* \|^2)(\text{by }(* ))\\
&amp;=\frac{\gamma}{2}\|\triangledown f(x_t)\|^2+\frac{1}{2\gamma}(\|x_t-x^* \| -\|x_{t+1}-x^* \|^2)\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;이들을 모든 $t$에 대해 다 더하면,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{t=0}^{T-1}\left(f(x_t)-f(x^* )\right)\le\frac{1}{2\gamma}(\|x_0-x^* \|^2-\|x_T-x^* \|^2) + \sum_{t=0}^{T-1}\frac{\gamma}{2}\|\triangledown f(x_t)\|^2&lt;/script&gt;

&lt;p&gt;가 된다. $f$가 Lipschitz continuous라고 했으므로 항상 &lt;script type=&quot;math/tex&quot;&gt;\|\triangledown f(x_t)\|^2 \le L^2&lt;/script&gt;이다. &lt;script type=&quot;math/tex&quot;&gt;\bar{x} = \frac{x_0+\cdots+x_{T-1}}{T}&lt;/script&gt;라고 하면, &lt;script type=&quot;math/tex&quot;&gt;f(\bar{x})\le\frac{1}{T}\sum_{t=0}^{T-1}f(x_t)&lt;/script&gt; 임이 성립하고,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}f(\bar{x})-f(x^* )&amp;\le\frac{1}{T}\sum_{t=0}^{T-1}\left(f(x_t)-f(x^* )\right)\\
&amp;\le \frac{1}{2\gamma T}\|x_0-x^* \|^2 + \frac{\gamma}{2}L^2 (\text{Lipschitz continuous})\\
&amp;=\frac{1}{2\gamma T}R^2 + \frac{\gamma}{2}L^2 (\text{Let }R = x_0-x^* ) \end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;가 된다. 따라서 여기에 적합한 learning rate 를 rough하게 계산해보면 $\gamma = \frac{R}{L\sqrt{T}}$가 적합하다. (마지막 식을 미분해서 0 되는 $\gamma$ 찾음)&lt;/p&gt;

&lt;p&gt;이를 대입해 보면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(\bar{x})-f(x^* )\le\frac{RL}{\sqrt{T}}&lt;/script&gt;

&lt;p&gt;가 성립한다.&lt;/p&gt;

&lt;h3 id=&quot;l-lipschitz-continuous&quot;&gt;$L$-Lipschitz Continuous&lt;/h3&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\|f(x)-f(y)\| \le L\|x-y\|, \forall x,y&lt;/script&gt;

&lt;p&gt;를 만족할 때 $L$-Lipschitz continuous라고 하고, $f$가 미분가능하면 &lt;script type=&quot;math/tex&quot;&gt;\|\triangledown f(x)\|\le L&lt;/script&gt;을 만족할 때 Lipschitz continuous라고 한다.&lt;/p&gt;

&lt;p&gt;이 때 $\gamma = \frac{1}{L}$로 잡으면 diverge는 방지할 수 있지만 fast converge는 보장하지 못한다.&lt;/p&gt;
</description>
        <pubDate>Mon, 09 Sep 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/lecture/optimization/2019/09/09/OptLecture3.html</link>
        <guid isPermaLink="true">http://localhost:4000/lecture/optimization/2019/09/09/OptLecture3.html</guid>
        
        
        <category>Lecture</category>
        
        <category>Optimization</category>
        
      </item>
    
      <item>
        <title>Optimization Lecture 2</title>
        <description>&lt;h1 id=&quot;convex-optimization&quot;&gt;Convex Optimization&lt;/h1&gt;

&lt;h2 id=&quot;convex-optimization-1&quot;&gt;Convex Optimization&lt;/h2&gt;

&lt;p&gt;Convex Optimization이란 $x^* = \text{min}_{x \in C} f(x)$를 찾는 문제이다. 이 때 $f$는 convex function이고, $C$는 convex set이다. 수렴도도 중요하지만, 얼마나 빨리 수렴하는지도 수렴 알고리즘의 선택에서 중요한 요소이다.&lt;/p&gt;

&lt;h3 id=&quot;convergence-rate&quot;&gt;Convergence Rate&lt;/h3&gt;

&lt;p&gt;convergence rate은 $f(x^* )$가 optimal value일 때,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{f(x_t)-f(x^* )}{g(t)} \le c&lt;/script&gt;

&lt;p&gt;에서의 $g(t)$이다. 이 때 $x_t$는 $x$가 $t$번 업데이트 된 값이고, $c$는 상수이다. 주로&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;$g(t) = \frac{1}{t}$&lt;/li&gt;
  &lt;li&gt;$g(t) = \frac{1}{\sqrt{t}}$&lt;/li&gt;
  &lt;li&gt;$g(t) = e^{-t}$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;가 쓰이는데, $g(t) = e^{-t}$가 가장 수렴속도가 빠르다.&lt;/p&gt;

&lt;p&gt;$f(x_t)-f(x^* ) \le \epsilon$으로 만들 때,&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;$g(t) = \frac{1}{t}$일 때는 $\frac{1}{\epsilon}$번의 step이 필요하고,&lt;/li&gt;
  &lt;li&gt;$g(t) = \frac{1}{\sqrt{t}}$일 때는 $\frac{1}{\epsilon^2}$번의 step,&lt;/li&gt;
  &lt;li&gt;$g(t) = e^{-t}$일 때는 $\ln{\frac{1}{\epsilon}}$번의 step이 필요하며, 가장 빠르다.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;convex-functions&quot;&gt;Convex Functions&lt;/h2&gt;

&lt;p&gt;Convex Function의 예시에는 다음과 같은 함수들이 있다.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Linear Functions : $f(x) = a^Tx$&lt;/li&gt;
  &lt;li&gt;Affine Functions : $f(x) = a^Tx + b$&lt;/li&gt;
  &lt;li&gt;Exponential Functions : $f(x) = e^{\alpha x}$&lt;/li&gt;
  &lt;li&gt;Norms&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;norms&quot;&gt;Norms&lt;/h3&gt;

&lt;p&gt;주어진 벡터공간 $V$에서의 norm은 0보다 크거나 같은 scalar function $p$이다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p : V \rightarrow [0, +\infty)&lt;/script&gt;

&lt;p&gt;norm은 다음과 같은 특징을 가진다.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;$p(x) + p(x) \ge p(x+y), \forall x, y\in V$&lt;/li&gt;
  &lt;li&gt;$p(ax) = | a|p(x)$&lt;/li&gt;
  &lt;li&gt;$p(x) = 0 \Leftrightarrow x=0$&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;p-norm&quot;&gt;p-norm&lt;/h3&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\|x\|_ p = \left(\sum_{i=1}^d |x_i|^p\right)^{\frac{1}{p}}&lt;/script&gt;

&lt;p&gt;주로 쓰이는 $p$값은 $1, 2, \infty$이다.&lt;/p&gt;

&lt;h3 id=&quot;example-of-norms&quot;&gt;Example of Norms&lt;/h3&gt;
&lt;p&gt;young’s inequality : $ab\le\frac{a^p}{p}+\frac{b^q}{q}, \forall p, q \ge 1, \frac{1}{p}+\frac{1}{q}=1$&lt;br /&gt;
Hoelder inequality : $|\sum_{i=1}^n u_i\cdot v_i| \le \left(\sum_{i=1}^n|u_i|^p\right)^{\frac{1}{p}}\left(\sum_{i=1}^n|v_i|^q\right)^{\frac{1}{q}}$&lt;br /&gt;
Minkowski inequality : $\left(\sum_{i=1}^n|x_i+y_i|^p\right)^{\frac{1}{p}}\le\left(\sum_{i=1}^n|x_i|^p\right)^{\frac{1}{p}}+\left(\sum_{i=1}^n|y_i|^p\right)^{\frac{1}{p}}$&lt;/p&gt;

&lt;p&gt;Hoelder’s inequality를 증명하는 데에 young’s inequality를 사용하고, Minkowski inequality를 증명하는 데에 Hoelder’s inequality를 사용한다.&lt;/p&gt;

&lt;h3 id=&quot;proof-youngs-inequality&quot;&gt;(Proof) Young’s inequality&lt;/h3&gt;

&lt;h3 id=&quot;proof-hoelders-inequality&quot;&gt;(Proof) Hoelder’s inequality&lt;/h3&gt;

&lt;h3 id=&quot;proof-minkowski-inequality&quot;&gt;(Proof) Minkowski inequality&lt;/h3&gt;

&lt;h2 id=&quot;proof-every-norm-is-convex&quot;&gt;(Proof) Every norm is Convex&lt;/h2&gt;
&lt;p&gt;(Recall) triangle inequality : $p(x) + p(y) \ge p(x+y), \forall x, y \in V$&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}\alpha p(x) + (1-\alpha)p(y) &amp;
= p(\alpha x) + p((1-\alpha)y) \\
&amp; \ge p(\alpha x + (1-\alpha)y)\end{align} %]]&gt;&lt;/script&gt;

&lt;h2 id=&quot;lemma-jensens-inequality&quot;&gt;(Lemma) Jensen’s inequality&lt;/h2&gt;

&lt;p&gt;$f$가 convex이고, $x_1, x_2, \cdots, x_m \in \text{dom}(f)$이고, &lt;script type=&quot;math/tex&quot;&gt;\lambda_1, \lambda_2, \cdots, \lambda_m \in \mathbb{R}_+&lt;/script&gt; s.t. $\sum_{i=1}^m \lambda_i = 1$일 때,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f\left(\sum_{i=1}^m \lambda_ix_i\right) \le \sum_{i=1}^m \lambda_i f(x_i)&lt;/script&gt;

&lt;p&gt;$x$가 random variable이라면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(E(x)) \le E[f(x)]&lt;/script&gt;

&lt;p&gt;이 부등식은 $\lambda_1 + \lambda_2 =1$이고 $\lambda_1, \lambda_2 \ge 0$일 때&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\lambda_1f(x_1) +\lambda_2f(x_2) \ge f(\lambda_1x_1+\lambda_2x_2)&lt;/script&gt;

&lt;p&gt;라는 convex의 정의를 이용해, 수학적 귀납법으로 증명할 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;proof&quot;&gt;(Proof)&lt;/h3&gt;
&lt;p&gt;m일 때 성립한다고 가정한다. 그러면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}f(\sum_{i=1}^{m+1}\lambda_ix_i)
&amp; =f(\sum_{i=1}^m\lambda_ix_i + \lambda_{m+1}x_{m+1})\\
&amp; =f((1-\lambda_{m+1})\sum_{i=1}^m\frac{\lambda_i}{1-\lambda_{m+1}}x_i+\lambda_{m+1}x_{m+1})\\
&amp; \le(1-\lambda_{m+1})f(\sum_{i=1}^m\frac{\lambda_i}{1-\lambda_{m+1}}x_i)+\lambda_{m+1}f(x_{m+1})\\
&amp; \le(1-\lambda_{m+1})\sum_{i=1}^m\frac{\lambda_i}{1-\lambda_{m+1}}f(x_i)+\lambda_{m+1}f(x_{m+1})\\
&amp; =\sum_{i=1}^{m+1}\lambda_if(x_i)\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;따라서,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f\left(\sum_{i=1}^{m+1}\lambda_ix_i\right)\le\sum_{i=1}^{m+1}\lambda_if(x_i)&lt;/script&gt;

&lt;p&gt;가 성립한다.&lt;/p&gt;

&lt;h2 id=&quot;differentiable-functions&quot;&gt;Differentiable Functions&lt;/h2&gt;

&lt;h3 id=&quot;differentiable&quot;&gt;Differentiable&lt;/h3&gt;
&lt;p&gt;$f$가 continuous한 일차미분식을 갖는다면, $f$는 differentiable하다. 이 특징은 매우 중요한데, 대부분의 최적화 iteration은 gradient값을 사용하기 때문이다.&lt;/p&gt;

&lt;h3 id=&quot;tangent-hyperplane&quot;&gt;Tangent Hyperplane&lt;/h3&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;g(x) = f(x_0) + \triangledown f(x_0)^T(x-x_0)&lt;/script&gt;

&lt;p&gt;이 tangent hyperplane이다. 이 직선은 $(x, f(x)$를 지나고 $\triangledown f(x_0)$의 기울기를 갖는다.&lt;/p&gt;

&lt;h2 id=&quot;first-order-characterization-of-convexity&quot;&gt;First-order Characterization of Convexity&lt;/h2&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\triangledown f(x) = \left(\frac{\partial f(x)}{\partial x_1}, \cdots, \frac{\partial f(x)}{\partial x_d}\right)^T&lt;/script&gt;

&lt;p&gt;가 first-order derivative이다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f : \text{ convex function} \Leftrightarrow f(y) \ge f(x) + \triangledown f(x)^T (y-x), \forall x, y \in \text{dom}(f)&lt;/script&gt;

&lt;h3 id=&quot;proof-leftarrow&quot;&gt;(Proof) $\Leftarrow$&lt;/h3&gt;

&lt;p&gt;$z = \theta x + (1-\theta) y, \theta\in [0, 1]$ 라고 하자.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}&amp; (f(x) \ge f(z) + \triangledown f(z)^T(x-z)) \times\theta \\
&amp; (f(y) \ge f(z) + \triangledown f(z)^T(y-z))\times(1-\theta)\\
&amp; \Rightarrow \theta f(x) + (1-\theta)f(y)\ge f(z) + \triangledown f(z)^T(\theta x+(1-\theta)y-z)\\
&amp; \therefore\theta f(x)+(1-\theta)f(y)\ge f(\theta x+(1-\theta)y)\end{align} %]]&gt;&lt;/script&gt;

&lt;h3 id=&quot;proof-rightarrow&quot;&gt;(Proof) $\Rightarrow$&lt;/h3&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}&amp;(1-t)f(x) + tf(y)\ge f(x+(y-x)t)\\
&amp; \Rightarrow f(y)\ge f(x) + \frac{f(x+(y-x)t)-f(x)}{t}\\
&amp; \text{ as } t\rightarrow 0, f(y)\ge f(x)+\triangledown f(x)^T(y-x)\end{align} %]]&gt;&lt;/script&gt;

&lt;h2 id=&quot;nondifferentiable-functions&quot;&gt;Nondifferentiable Functions&lt;/h2&gt;
&lt;p&gt;$f(x) = |x|$같은 함수의 경우, $x=0$부근에서 많은 접점이 있지만 정확한 극한값은 없다. 이 때 그냥 하나를 마음대로 정할 수 있는데, 이것이 subgradient이다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(y)\ge f(x) + g_x^T(y-x)&lt;/script&gt;

&lt;p&gt;에서 $g_x$가 subgradient. 위에서 예를 든 $f(x) = |x|$의 경우, $g\in[-1,1]$은 모두 subgradient가 될 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;second-order-characterization-of-convexity&quot;&gt;Second-order Characterization of Convexity&lt;/h2&gt;

&lt;p&gt;일변수함수에 대해서는 이차미분식이 그냥 하나의 식이지만, 다변수함수에서는 이차미분식이 Hessian이다. Hessian은&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\triangledown^2f(x) = \left(\begin{matrix}
  \frac{\partial^2f}{\partial x_1 \partial x_1} &amp; \frac{\partial^2f}{\partial x_1 \partial x_2} &amp; \cdots &amp; \frac{\partial^2f}{\partial x_1 \partial x_d} \\
  \frac{\partial^2f}{\partial x_2 \partial x_1} &amp; \frac{\partial^2f}{\partial x_2 \partial x_2} &amp; \cdots &amp; \frac{\partial^2f}{\partial x_2 \partial x_d} \\
  \vdots &amp; \vdots &amp; \cdots &amp; \vdots \\
  \frac{\partial^2f}{\partial x_d \partial x_1} &amp; \frac{\partial^2f}{\partial x_d \partial x_2} &amp; \cdots &amp; \frac{\partial^2f}{\partial x_d \partial x_d}
  \end{matrix}\right) %]]&gt;&lt;/script&gt;

&lt;p&gt;으로 정의된다.&lt;/p&gt;

&lt;h3 id=&quot;def-positive-definite&quot;&gt;(Def) Positive Definite&lt;/h3&gt;
&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;A\in \mathbb{R}^{d\times d}&lt;/script&gt;가 symmetric하고 $x^TAx \gt 0, \forall x\in\mathbb{R}^d$일 때 $f$는 positive definite이다.&lt;/p&gt;

&lt;h3 id=&quot;proof-f---convex-function-leftrightarrow-triangledown2fx-is-positive-semidefinite&quot;&gt;(Proof) $f : $ convex function $\Leftrightarrow \triangledown^2f(x)$ is positive semidefinite&lt;/h3&gt;
&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}&amp; g(t) = f(x+(y-x)t)\text{ 는 } t\text{에 대한 convex function이다}\\
&amp; \Leftrightarrow g'(t) = \triangledown f(x+(y-x)t)(y-x)\\
&amp; \Leftrightarrow g''(t) = (y-x)^T\triangledown^2f(x+(y-x)t)(y-x)\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;$g(t)$가 convex function이므로, &lt;script type=&quot;math/tex&quot;&gt;g''(t) \ge 0, \forall t, \forall x, y&lt;/script&gt;를 만족한다. 따라서 $t=0$일 때도 만족하는데, 이를 대입해보면 다음과 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;g''(t)\mid_ {t=0} = (y-x)^T\triangledown^2f(x)(y-x) \ge 0, \forall x, y&lt;/script&gt;

&lt;p&gt;이는 positive semidefinite의 정의이므로, 둘은 동치이다.&lt;/p&gt;

&lt;h2 id=&quot;local-minima-critical-point&quot;&gt;Local Minima, Critical Point&lt;/h2&gt;
&lt;h3 id=&quot;def-local-minima&quot;&gt;(Def) Local Minima&lt;/h3&gt;
&lt;p&gt;$\epsilon \gt0$에 대해, $f(x)\le f(y), \forall y \text{ s.t. }|y-x|\le\epsilon$을 만족하면 $x$는 local minima이다.&lt;/p&gt;

&lt;h3 id=&quot;def-critical-point&quot;&gt;(Def) Critical Point&lt;/h3&gt;
&lt;p&gt;$\triangledown f(x)=0$을 만족하면 $x$는 critical point이다. 만약 $f$가 convex라면, critical point는 global minima이다. critical point는 최적화에서는 bad point인데, tangent line이 기울기가 0이기 때문에 gradient가 존재하지 않아 local minima를 빠져나가기가 어렵다.&lt;/p&gt;

&lt;h2 id=&quot;constrained-minimization&quot;&gt;Constrained Minimization&lt;/h2&gt;
&lt;p&gt;$f$가 convex function이고, $X$가 convex set이면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(x^* )\le f(y), \forall y\in X \Leftrightarrow \triangledown f(x^* )^T(x-x^* ) \ge 0, \forall x\in X&lt;/script&gt;

&lt;h3 id=&quot;proofrightarrow&quot;&gt;(Proof)$\Rightarrow$&lt;/h3&gt;
&lt;p&gt;Let $f(x^* )\le f(y), \forall y \in X$&lt;br /&gt;
Since $f$ is convex,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}&amp;tf(x^* )+(1-t)f(y) \ge f(y+(x^* -y)t)\\
&amp;\Rightarrow f(x^* ) \ge \frac{f(y+(x^* -y)t)-f(y)}{t} + f(y)\\
&amp;\Rightarrow f(x^* )\ge (x^* -y)\triangledown f(y)+f(y) (\text{ as }t\rightarrow0)\\
&amp;\Rightarrow (y-x^* )\triangledown f(y)\ge f(y)-f(x^* ) \ge 0\\
&amp;\therefore (y-x^* )\triangledown f(y) \ge 0, \forall y\in X\end{align} %]]&gt;&lt;/script&gt;

&lt;h3 id=&quot;proof-leftarrow-1&quot;&gt;(Proof) $\Leftarrow$&lt;/h3&gt;
&lt;p&gt;Let $\triangledown f(x^* )^T(y-x^* )\ge 0, \forall y\in X$&lt;br /&gt;
$f$가 convex이기 때문에, $f(y)\ge f(x^* )+\triangledown f(x^* )^T(y-x^* )$이다.&lt;br /&gt;
$\Rightarrow f(y)-f(x^* )\ge\triangledown f(x^* )^T(y-x^* )\ge 0$&lt;br /&gt;
$\therefore f(y)\ge f(x^* ), \forall y\in X$&lt;/p&gt;

&lt;h2 id=&quot;thm-tayler&quot;&gt;(Thm) Tayler&lt;/h2&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}f(x) &amp;= f(0) + f'(0)x + \frac{1}{2}f^{\prime\prime}(0)x^2+\cdots\\
f(b) &amp;= f(a) + f'(a)(b-a) + \frac{1}{2}f^{\prime\prime}(c)(b-a)^2, c\in[a, b]\\
&amp; = f(a) + \triangledown f(a)^T(b-a) + \frac{1}{2}(b-a)^T\triangledown^2f(c)(b-a), c\in[a, a+\theta(b-a)], \theta\in[0, 1]\end{align}\\
 \therefore f(b) \ge f(a)+\triangledown f(a)^T(b-a) %]]&gt;&lt;/script&gt;
</description>
        <pubDate>Wed, 04 Sep 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/lecture/optimization/2019/09/04/OptLecture2.html</link>
        <guid isPermaLink="true">http://localhost:4000/lecture/optimization/2019/09/04/OptLecture2.html</guid>
        
        
        <category>Lecture</category>
        
        <category>Optimization</category>
        
      </item>
    
      <item>
        <title>Optimization Lecture 1</title>
        <description>&lt;p&gt;딥러닝에서 objective function의 설계는 중요하다. 이 때 문제를 더 간단하게 만들기 위해 convex의 개념을 도입한다.&lt;/p&gt;

&lt;h3 id=&quot;convex-set&quot;&gt;Convex set&lt;/h3&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{For } x \in C \text{ and } y \in C \Rightarrow (1-\alpha)x+\alpha y \in C, \forall\alpha\in[0, 1]&lt;/script&gt;

&lt;p&gt;convex set을 쓰면 line 전체가 constraint 안에 있기 때문에 line search를 할 때 쓴다. convex set을 가정한 방법들은 non-convex에서도 사용할 수 있다. local minima 부근에서는 convex이기 때문인데, 이 때 convex 방법들을 적용하면 global minima는 아니다.&lt;/p&gt;

&lt;h3 id=&quot;convex-function&quot;&gt;Convex function&lt;/h3&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\alpha f(x_1)+(1-\alpha)f(x_2) \ge f(\alpha x_1 + (1-\alpha)x_2), \forall x_1, x_2, \forall \alpha \in [0, 1]&lt;/script&gt;

&lt;p&gt;convex function의 가장 큰 특징은 global minima가 하나이고, 2차 미분식(또는 해시안 함수)이 항상 양수라는 점이다. 이또한 분석이 쉽다는 장점을 가지고 있다. 사실, unimodal function도 global minima가 하나지만, 이차미분항이 양수와 음수를 왔다갔다 할 수 있기 때문에 convex function보다는 더 분석이 어렵다.&lt;/p&gt;

&lt;p&gt;이차미분항이 양수라는 점에서 오는 장점은, convergence behavior를 알기가 쉽다는 점이다. 보통 잘 알고 있는 gradient descent 방법의 식은 다음과 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x_{t+1} \leftarrow x_t + \gamma\triangledown f(x)&lt;/script&gt;

&lt;p&gt;이 때 이차미분항이 양수이면, $\triangledown f(x)$의 양상을 더 쉽게 알 수 있다.&lt;/p&gt;
</description>
        <pubDate>Mon, 02 Sep 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/lecture/optimization/2019/09/02/OptLecture1.html</link>
        <guid isPermaLink="true">http://localhost:4000/lecture/optimization/2019/09/02/OptLecture1.html</guid>
        
        
        <category>Lecture</category>
        
        <category>Optimization</category>
        
      </item>
    
      <item>
        <title>CV Lecture 11 - Light</title>
        <description>&lt;h1 id=&quot;light&quot;&gt;Light&lt;/h1&gt;

&lt;h1 id=&quot;color-vector&quot;&gt;Color Vector&lt;/h1&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;terms&quot;&gt;Terms&lt;/h2&gt;
</description>
        <pubDate>Sat, 31 Aug 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/lecture/computervision/2019/08/31/Lecture11.html</link>
        <guid isPermaLink="true">http://localhost:4000/lecture/computervision/2019/08/31/Lecture11.html</guid>
        
        
        <category>Lecture</category>
        
        <category>ComputerVision</category>
        
      </item>
    
  </channel>
</rss>
