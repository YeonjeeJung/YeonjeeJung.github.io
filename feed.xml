<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>YeonjeeJung's Blog</title>
    <description></description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Wed, 18 Sep 2019 21:58:14 +0900</pubDate>
    <lastBuildDate>Wed, 18 Sep 2019 21:58:14 +0900</lastBuildDate>
    <generator>Jekyll v3.8.6</generator>
    
      <item>
        <title>Optimization Lecture 5</title>
        <description>&lt;h1 id=&quot;projected-gradient-descent&quot;&gt;Projected Gradient Descent&lt;/h1&gt;

&lt;h2 id=&quot;constrained-optimization&quot;&gt;Constrained Optimization&lt;/h2&gt;

&lt;p&gt;Constrained Problem은 $f(x)$를 최소화하는 $x$를 찾는 문제인데, $x$의 범위 $X$가 주어져 있다는 점에서 이전과 다르다. 이 문제를 해결하는 방법에는 두 가지가 있는데,&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Projected Gradient Descent를 이용하는 방법과&lt;/li&gt;
  &lt;li&gt;unconstrained problem으로 바꿔서 해결하는 방법이 있다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;이번 단원은 첫번째 방법에 대한 내용이다.&lt;/p&gt;

&lt;h2 id=&quot;projected-gradient-descent-1&quot;&gt;Projected Gradient Descent&lt;/h2&gt;

&lt;p&gt;Project 는 &lt;script type=&quot;math/tex&quot;&gt;\Pi_X(y):=\arg\min_{x\in X}\|x-y\|&lt;/script&gt;, 즉 $X$바깥에 있는 $y$와 가장 가까운 $x\in X$에 매칭시켜주는 것이다.&lt;/p&gt;

&lt;p&gt;Projected gradient를 업데이트 하는 방법은 &lt;script type=&quot;math/tex&quot;&gt;x_{t+1}=\Pi_X(x_t-\gamma\triangledown f(x_t))&lt;/script&gt;이다.&lt;/p&gt;

&lt;h2 id=&quot;propconvex-constrained-problem&quot;&gt;(Prop)Convex Constrained Problem&lt;/h2&gt;

&lt;p&gt;$f$가 convex function이고 $X$가 convex set이고, $x^* $가 $X$범위에 대해 $f$의 minimizer라면,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}f(x^* )\le f(y), \forall y\in X \Leftrightarrow \triangledown f(x^* )^T(x-x^* )\ge 0\cdots(* 1)\end{align}&lt;/script&gt;

&lt;p&gt;이 성립한다.&lt;/p&gt;

&lt;h3 id=&quot;proof-leftarrow&quot;&gt;(Proof) $\Leftarrow$&lt;/h3&gt;

&lt;p&gt;$f$가 convex function이라고 했으므로,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(x)\ge f(x^* ) + \triangledown f(x^* )^T(x-x^* )&lt;/script&gt;

&lt;p&gt;를 항상 만족한다. 따라서 $\triangledown f(x^* )^T(x-x^* )\ge 0$이면 $f(x)\ge f(x^* )$이므로 증명은 끝난다.&lt;/p&gt;

&lt;h3 id=&quot;proof-rightarrow&quot;&gt;(Proof) $\Rightarrow$&lt;/h3&gt;

&lt;p&gt;대우명제를 이용한다. $\triangledown f(x^* )^T(x-x^* )\lt 0$이라고 하자. 그러면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\triangledown f(x^* )^T(x-x^* )=\lim_{t\rightarrow 0+}\frac{f(x^* +t(x-x^* ))-f(x^* )}{t(x-x^* )}(x-x^* )\lt 0&lt;/script&gt;

&lt;p&gt;이 성립하므로,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(x^* +t(x-x^* ))-f(x^* )\lt 0&lt;/script&gt;

&lt;p&gt;이 성립하게 된다. 이는 $f(x^* )\le f(y), \forall y\in X$에 부합하지 않으므로, 증명은 끝난다.&lt;/p&gt;

&lt;h2 id=&quot;prop-properties-of-projection&quot;&gt;(Prop) Properties of Projection&lt;/h2&gt;

&lt;p&gt;$X$가 closed 이고 convex라고 하고, $x\in X, y\in \mathbb{R}^d$라고 하자. 그러면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}&amp;(x-\Pi_X(y))^T(y-\Pi_X(y))\le 0\cdots(* 2)\\
&amp;\|x-\Pi_X(y)\|^2+\|y-\Pi_X(y)\|^2\le\|x-y\|^2\cdots(* 3)\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;가 항상 성립한다.&lt;/p&gt;

&lt;h3 id=&quot;proof--2&quot;&gt;(Proof) $(* 2)$&lt;/h3&gt;

&lt;p&gt;$\Pi_X(y)$는 &lt;script type=&quot;math/tex&quot;&gt;d(x)=\|x-y\|^2&lt;/script&gt;의 minimizer이므로, $(* 1)$의 $\Rightarrow$를 이용하면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\triangledown d(\Pi_X(y))^T(x-\Pi_X(y))\ge 0&lt;/script&gt;

&lt;p&gt;이 항상 성립한다. &lt;script type=&quot;math/tex&quot;&gt;d(x) = \|x-y\|^2=(x-y)^T(x-y)&lt;/script&gt;이므로, &lt;script type=&quot;math/tex&quot;&gt;\triangledown d(x) = 2(x-y)&lt;/script&gt;이다. 따라서&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;(\Pi_X(y)-y)^T(x-\Pi_X(y))\ge 0&lt;/script&gt;

&lt;p&gt;이고, 바꿔 말하면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;(x-\Pi_X(y))^T(y-\Pi_X(y))\le 0&lt;/script&gt;

&lt;p&gt;이다.&lt;/p&gt;

&lt;h3 id=&quot;proof--3&quot;&gt;(Proof) $(* 3)$&lt;/h3&gt;

&lt;p&gt;$v:=\Pi_X(y), w:=y-\Pi_X(y)$라고 하자. $(* 2)$에 의해&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;0\le 2v^Tw = \|v\|^2+\|w\|^2-\|v-w\|^2&lt;/script&gt;

&lt;p&gt;이므로,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\|x-\Pi_X(y)\|^2+\|y-\Pi_X(y)\|^2\le\|x-y\|^2&lt;/script&gt;

&lt;p&gt;가 성립한다.&lt;/p&gt;

&lt;h2 id=&quot;projected-gradient--lipschitz-convex&quot;&gt;Projected Gradient : Lipschitz Convex&lt;/h2&gt;

&lt;p&gt;$f:\mathbb{R}^d\rightarrow\mathbb{R}$가 convex function이고 미분가능하다고 하자. $X\subset\mathbb{R}^d$는 closed 이고 convex라고 하고, $x^* \in X$는 $f$에 대한 minimizer라고 하자. &lt;script type=&quot;math/tex&quot;&gt;\|x_0-x^* \|\le R, x_0\in X&lt;/script&gt;이고, &lt;script type=&quot;math/tex&quot;&gt;\|\triangledown f(x)\|\le B, \forall x\in X&lt;/script&gt;라고 하자 (B-Lipschitz continuous). step size $\gamma = \frac{R}{B\sqrt{T}}$라고 지정하면,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(\bar{x})-f(x^* )\le\frac{1}{T}\sum_{t=0}^{T-1}f(x_t)-f(x^* )\le\frac{RB}{\sqrt{T}}&lt;/script&gt;

&lt;p&gt;의 boundary를 갖는다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Lipschitz Continuous에서는 $f(x_{t+1})\le f(x_t)$를 보장하지 못하기 때문에, $f(x_T)\le f(x_t)$라고 장담할 수가 없다. 따라서 $f(\bar{x})-f(x^ *)$를 bound시키는 것이다.&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;vanilla-analysis&quot;&gt;Vanilla Analysis&lt;/h3&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;y_{t+1}=x_t-\gamma\triangledown f(x_t), x_{t+1}=\Pi_X(y_{t+1})&lt;/script&gt;이라고 하자. vanilla anaylsis를 이용하면,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}f(x_t)-f(x^* ) &amp; \le \triangledown f(x_t)^T(x_t-x^* )\\
&amp;=\frac{1}{\gamma}(x_t-y_{t+1})^T(x_t-x^* )\\
&amp;=\frac{1}{2\gamma}(\|x_t-y_{t+1}\|^2+\|x_t-x^* \|^2-\|y_{t+1}-x^* \|^2)\\
&amp;\le\frac{\gamma}{2}\|\triangledown f(x_t)\|^2+\frac{1}{2\gamma}(\|x_t-x^* \|-\|x_{t+1}-x^* \|^2)-\frac{1}{2\gamma}\|y_{t+1}-x_{t+1}\|^2
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;(마지막 줄은 &lt;script type=&quot;math/tex&quot;&gt;(y_{t+1}=x_t-\gamma\triangledown f(x_t))&lt;/script&gt;와 $(3)$을 사용했다). 이 식은 원래의 vanilla anaylsis에서 $-\frac{1}{2\gamma}|y_{t+1}-x_{t+1}|^2$만 추가된 것이다. 모든 $t$에 대해서 다 더하면,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}\sum_{t=0}^{T-1}(f(x_t)-f(x^* ))&amp;\le\frac{1}{2\gamma}(\|x_0-x^* \|^2-\|x_T-x^* \|^2)+\sum_{t=0}^{T-1}\frac{\gamma}{2}\|\triangledown f(x_t)\|^2-\sum_{t=0}^{T-1}\frac{1}{2\gamma}\|y_{t+1}-x_{t+1}\|^2 \\
&amp;\le \frac{1}{2\gamma}(\|x_0-x^* \|^2-\|x_T-x^* \|^2)+\sum_{t=0}^{T-1}\frac{\gamma}{2}\|\triangledown f(x_t)\|^2\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;를 얻게 되는데, 이는 결국 vanilla anaylsis와 같은 결론이다.&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\bar{x}=\frac{x_0+\cdots+x_{T-1}}{T}&lt;/script&gt; 라고 하면,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}f(\bar{x})-f(x^* )&amp;\le\frac{1}{T}\sum_{t=0}^{T-1}(f(x_t)-f(x^* ))\text{(Jensen's inequality)}\\
&amp;\le\frac{1}{2\gamma T}R^2+\frac{1}{T}\sum_{t=0}^{T-1}\frac{\gamma}{2}\|\triangledown f(x_t)\|^2\\
&amp;\le\frac{RB}{2\sqrt{T}}+\frac{RB}{2\sqrt{T}}(\gamma = \frac{R}{B\sqrt{T}})\\
&amp;\le\frac{RB}{\sqrt{T}}\end{align} %]]&gt;&lt;/script&gt;

&lt;h2 id=&quot;projected-gradient--beta-smooth-functions&quot;&gt;Projected Gradient : $\beta$-smooth functions&lt;/h2&gt;

&lt;h3 id=&quot;recall-beta-smooth&quot;&gt;(Recall) $\beta$-smooth&lt;/h3&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}f(x_t)-f(x_{t+1})&amp;\ge\triangledown f(x_t-x_{t+1})-\frac{\beta}{2}\|x_t-x_{t+1}\|^2\\
&amp;=\gamma\|\triangledown f(x_t)\|^2-\frac{\gamma^2\beta}{2}\|\triangledown f(x_t)\|^2\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;unconstrained 에서와는 다르게, constrained에서는&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\gamma\triangledown f(x_t)\neq x_t-x_{t+1}&lt;/script&gt;

&lt;p&gt;이다. 대신 $\gamma = \frac{1}{\beta}$를 대입하면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(x_{t+1})\le f(x_t)-\frac{1}{2\beta}\|\triangledown f(x_t)\|^2 + \frac{\beta}{2}\|y_{t+1}-x_{t+1}\|^2\cdots(* 4)&lt;/script&gt;

&lt;p&gt;를 얻을 수 있다. 주의할 점은, &lt;script type=&quot;math/tex&quot;&gt;\frac{\beta}{2}\|y_{t+1}-x_{t+1}\|^2&lt;/script&gt;때문에 monotone decrese (&lt;script type=&quot;math/tex&quot;&gt;f(x_{t+1}\le f(x_t))&lt;/script&gt;)를 확신할 수 없으므로 이를 먼저 증명해야 한다.&lt;/p&gt;

&lt;h3 id=&quot;proof-fx_t1le-fx_t&quot;&gt;(Proof) &lt;script type=&quot;math/tex&quot;&gt;f(x_{t+1})\le f(x_t)&lt;/script&gt;&lt;/h3&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}f(x_{t+1})&amp;\le f(x_t)+\triangledown f(x_t)^T(x_{t+1}-x_t)+\frac{\beta}{2}\|x_{t+1}-x_t\|^2 (\beta\text{-smooth})\\
&amp;=f(x_t)-\beta(y_{t+1}-x_t)^T(x_{t+1}-x_t)+\frac{\beta}{2}\|x_{t+1}-x_t\|^2(y_{t+1}-x_t=\frac{1}{\beta}\triangledown f(x_t))\\
&amp;=f(x_t)-\frac{\beta}{2}(\|y_{t+1}-x_t\|^2+\|x_{t+1}-x_t\|^2-\|y_{t+1}-x_{t+1}\|^2)+\frac{\beta}{2}\|x_{t+1}-x_t\|^2\\
&amp;=f(x_t)-\frac{\beta}{2}\|y_{t+1}-x_t\|^2+\frac{\beta}{2}\|y_{t+1}-x_{t+1}\|^2\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;$(* 3)$에 의해&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\|x_t-x_{t+1}\|^2+\|y_{t+1}-x_{t+1}\|^2\le\|x_t-y_{t+1}\|^2&lt;/script&gt;

&lt;p&gt;이므로&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;-\frac{\beta}{2}\|y_{t+1}-x_t\|^2+\frac{\beta}{2}\|y_{t+1}-x_{t+1}\|^2\le 0&lt;/script&gt;

&lt;p&gt;이 된다. 따라서&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}f(x_{t+1})-f(x_t)&amp;\le-\frac{\beta}{2}\|y_{t+1}-x_t\|^2+\frac{\beta}{2}\|y_{t+1}-x_{t+1}\|^2\\&amp;\le 0\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;이므로, $f(x_{t+1})\le f(x_t)$를 얻을 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;vanilla-analysis-1&quot;&gt;Vanilla Analysis&lt;/h3&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}\sum_{t=0}^{T-1}(f(x_t)-f(x^* ))&amp;\le\frac{\beta}{2}(\|x_0-x^* \|^2-\|x_T-x^* \|^2)+\sum_{t=0}^{T-1}\left(\frac{1}{2\beta}\|\triangledown f(x_t)\|^2-\frac{\beta}{2}\|y_{t+1}-x_{t+1}\|^2\right)\\
&amp;\le\frac{\beta}{2}\|x_0-x^* \|^2+\sum_{t=0}^{T-1}(f(x_t)-f(x_{t+1}))((* 4)\text{에 의해})\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;우변의 마지막 항을 좌변으로 넘기면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{t=1}^T(f(x_t)-f(x^* ))\le\frac{\beta}{2}\|x_0-x^* \|^2&lt;/script&gt;

&lt;p&gt;를 얻게 되고,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}f(x_T)-f(x^* )&amp;\le\frac{1}{T}\sum_{t=1}^T(f(x_t)-f(x^* ))(f(x_T)\le f(x_t),\forall t\lt T)\\
&amp;\le\frac{\beta}{2T}\|x_0-x^* \|^2\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;로 bound된다.&lt;/p&gt;
</description>
        <pubDate>Mon, 16 Sep 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/lecture/optimization/2019/09/16/OptLecture5.html</link>
        <guid isPermaLink="true">http://localhost:4000/lecture/optimization/2019/09/16/OptLecture5.html</guid>
        
        
        <category>Lecture</category>
        
        <category>Optimization</category>
        
      </item>
    
      <item>
        <title>Optimization Lecture 4</title>
        <description>&lt;h1 id=&quot;convex-optimization&quot;&gt;Convex Optimization&lt;/h1&gt;

&lt;h2 id=&quot;beta-smooth-functions--frac1t-learning-rate&quot;&gt;$\beta$-smooth functions : $\frac{1}{t}$ learning Rate&lt;/h2&gt;

&lt;p&gt;$f$가 $\beta$-smooth하다고 하면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}f(x_t)-f(x_{t+1}) &amp; \ge \triangledown f(x_t)^T(x_t-x_{t+1})-\frac{\beta}{2}\|x_t-x_{t+1}\|^2\\
 &amp; =\gamma\|\triangledown f(x_t)\|^2-\frac{\gamma^2\beta}{2}\|\triangledown f(x_t)\|^2 (x_{t+1}-x_t=-\gamma\triangledown f(x_t), \text{gradient descent})\\
 &amp; =(\gamma-\frac{\gamma^2\beta}{2})\|\triangledown f(x_t)\|^2\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;이다. 이 식에서의 극점은 $\gamma = \frac{1}{\beta}$일 때이므로 이를 대입하면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}f(x_t)-f(x_{t+1})\ge \frac{1}{2\beta}\|\triangledown f(x_t)\|^2\end{align}&lt;/script&gt;

&lt;p&gt;를 얻게 된다. 이 때 $t=0$부터 $t=T-1$까지 다 대입한 후 다 더하면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}f(x_0)-f(x_T) &amp; \ge \sum_{t=0}^{T-1}\frac{1}{2\beta}\|\triangledown f(x_t)\|^2\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;을 얻게 된다. 이전 단원의 Vanilla Analysis 중간에서 얻은 식에 $\gamma=\frac{1}{\beta}$를 대입하면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{t=0}^{T-1}{(f(x_t)-f(x^* ))}\le\frac{\beta}{2}\|x_0-x^* \|^2+\sum_{t=0}^{T-1}\frac{1}{2\beta}\|\triangledown f(x_t)\|^2&lt;/script&gt;

&lt;p&gt;을 얻는데, $(4)$을 이용하면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{t=0}^{T-1}{(f(x_t)-f(x^* ))}\le\frac{\beta}{2}\|x_0-x^* \|^2+\sum_{t=0}^{T-1}\frac{1}{2\beta}\|\triangledown f(x_t)\|^2 \le \frac{\beta}{2}\|x_0-x^* \|^2+ f(x_0)-f(x_T)&lt;/script&gt;

&lt;p&gt;이라고 할 수 있고, 맨 우측 두 항을 좌변으로 넘기면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{t=1}^T(f(x_t)-f(x^* ))\le\frac{\beta}{2}\|x_0-x^* \|^2&lt;/script&gt;

&lt;p&gt;를 얻는다. $f(x_T)\le f(x_t), 0\le \forall t\le T$이므로, 최종적으로&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(x_T)-f(x^* )\le\frac{1}{T}\sum_{t=1}^T(f(x_t)-f(x^* ))\le\frac{\beta}{2T}\|x_0-x^* \|^2&lt;/script&gt;

&lt;p&gt;를 얻는다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;upper bound와 convergence speed에 관한 설명 추가&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;def-linear-convergence&quot;&gt;(Def) Linear Convergence&lt;/h3&gt;

&lt;p&gt;만약&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\lim_{t\rightarrow\infty}{\frac{f(x_{t+1})-f(x^* )}{f(x_t)-f(x^* )}}=c&lt;/script&gt;

&lt;p&gt;를 만족하는 $c\in(0, 1)$가 존재한다면 $f$는 선형적으로 수렴한다고 말할 수 있다. 만약 $\frac{f(x_{t+1})-f(x^* )}{f(x_t)-f(x^* )}\le c, \forall t, \forall c\in (0, 1)$라면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}f(x_t)-f(x^* )\le c^t(f(x_0)-f(x^* ))\end{align}&lt;/script&gt;

&lt;p&gt;가 항상 성립한다.&lt;/p&gt;

&lt;h2 id=&quot;alpha-strongly-convex-and-beta-smooth-function&quot;&gt;$\alpha$-strongly convex and $\beta$-smooth function&lt;/h2&gt;

&lt;p&gt;Vanilla Analysis의 중간 과정에서&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\triangledown f(x_t)^T(x_t-x^* )=\frac{\gamma}{2}\|\triangledown f(x_t)\|^2+\frac{1}{\gamma}(\|x_t-x^* \|^2-\|x_{t+1}-x^* \|^2)&lt;/script&gt;

&lt;p&gt;의 식을 얻을 수 있었다. 그리고 $f$가 $\alpha$-strongly convex라고 하면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\triangledown f(x_t)^T(x_t-x^* )\ge f(x_t)-f(x^* )+\frac{\alpha}{2}\|x_t-x^* \|^2&lt;/script&gt;

&lt;p&gt;를 만족한다. 둘을 합치고 변형하면,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align} f(x_t)-f(x^* )+\frac{\alpha}{2}\|x_t-x^* \|^2&amp;\le\frac{\gamma}{2}\|\triangledown f(x_t)\|^2+\frac{1}{2\gamma}(\|x_t-x^* \|^2-\|x_{t+1}-x^* \|^2)\\
 -\frac{1}{2\gamma}(\|x_t-x^* \|^2-\|x_{t+1}-x^* \|^2)+\frac{\alpha}{2}\|x_t-x^* \|^2 &amp;\le f(x^* )-f(x_t )+ \frac{\gamma}{2}\|\triangledown f(x_t)\|^2\\
 -\|x_t-x^* \|^2+\|x_{t+1}-x^* \|^2+\alpha\gamma\|x_t-x^* \|^2 &amp;\le 2\gamma(f(x^* )-f(x_t ))+ \gamma^2\|\triangledown f(x_t)\|^2\\
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;을 얻을 수 있다. 이 때 $\gamma = \frac{1}{\beta}$를 대입하면 우변은&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}2\gamma(f(x^* )-f(x_t ))+ \gamma^2\|\triangledown f(x_t)\|^2&amp;=\frac{2}{\beta}(f(x^* )-f(x_t))+\frac{1}{\beta^2}\|f(x_t)\|^2\\
&amp;\le\frac{2}{\beta}(f(x_{t+1})-f(x_t))+\frac{1}{\beta^2}\|\triangledown f(x_t)\|^2\\
&amp;\le-\frac{1}{\beta^2}\|\triangledown f(x_t)\|^2+\frac{1}{\beta^2}\|\triangledown f(x_t)\|^2\\
&amp;=0\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;($(11)\rightarrow(12)$은 $(4)$ 때문이다.) $(8)$을 정리하면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\|x_{t+1}-x^* \|^2\le \left(1-\frac{\alpha}{\beta}\right)\|x_t-x^* \|^2&lt;/script&gt;

&lt;p&gt;이라고 할 수 있고, $(6)$에 의해&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\|x_T-x^* \|^2\le\left(1-\frac{\alpha}{\beta}\right)^T\|x_0-x^* \|^2&lt;/script&gt;

&lt;p&gt;가 성립한다.&lt;/p&gt;

&lt;h2 id=&quot;convergence&quot;&gt;Convergence&lt;/h2&gt;

&lt;p&gt;$f$가 $\beta$-smooth라면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}f(x_T)-f(x^* )&amp;\le\triangledown f(x^* )^T(x_T-x^* )+\frac{\beta}{2}\|x_T-x^* \|^2\\
&amp;=\frac{\beta}{2}\|x_T-x^* \|^2\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;이므로,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(x_T)-f(x^* )\le\frac{\beta}{2}\|x_T-x^* \|^2\le\frac{\beta}{2}\left(1-\frac{\alpha}{\beta}\right)^T\|x_0-x^* \|^2&lt;/script&gt;

&lt;p&gt;가 성립한다. 이 때 $\frac{\alpha}{\beta}$를 condition number라고 하는데, 이 값이 $1$이면, 즉 $\alpha = \beta$이면 한 번의 이터레이션으로 최적값을 찾을 수 있다.&lt;/p&gt;
</description>
        <pubDate>Wed, 11 Sep 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/lecture/optimization/2019/09/11/OptLecture4.html</link>
        <guid isPermaLink="true">http://localhost:4000/lecture/optimization/2019/09/11/OptLecture4.html</guid>
        
        
        <category>Lecture</category>
        
        <category>Optimization</category>
        
      </item>
    
      <item>
        <title>Robot Lecture 3</title>
        <description>&lt;h1 id=&quot;kinematics&quot;&gt;Kinematics&lt;/h1&gt;
</description>
        <pubDate>Mon, 09 Sep 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/lecture/robot/2019/09/09/RobLecture3.html</link>
        <guid isPermaLink="true">http://localhost:4000/lecture/robot/2019/09/09/RobLecture3.html</guid>
        
        
        <category>Lecture</category>
        
        <category>Robot</category>
        
      </item>
    
      <item>
        <title>Optimization Lecture 3</title>
        <description>&lt;h1 id=&quot;convex-optimization&quot;&gt;Convex Optimization&lt;/h1&gt;
&lt;h2 id=&quot;gradient-descent&quot;&gt;Gradient Descent&lt;/h2&gt;
&lt;h3 id=&quot;gradient-descent-algorithm&quot;&gt;Gradient Descent Algorithm&lt;/h3&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x_t \leftarrow x_t - \gamma\triangledown f(x_t)&lt;/script&gt;

&lt;p&gt;이거나, $\gamma$를 $t$의 함수로 표현하여&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x_t \leftarrow x_t - \gamma_t\triangledown f(x_t)&lt;/script&gt;

&lt;p&gt;로 업데이트 하여 최적점을 찾아가는 방법을 gradient descent 알고리즘이라고 한다. 이 때 $\gamma_t$는 $\frac{1}{t}$, $\frac{1}{\sqrt{t}}$, $\frac{1}{\log{t}}$, $\cos{\theta t}$가 주로 쓰인다.&lt;/p&gt;

&lt;h3 id=&quot;convergence-rate&quot;&gt;Convergence Rate&lt;/h3&gt;

&lt;p&gt;Convergence rate은&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(x_t)-f(x^* )\le\epsilon&lt;/script&gt;

&lt;p&gt;을 만족하는 $t$를 찾기까지의 시행 수를 Big O 표기법으로 나타내는 것이다.&lt;/p&gt;

&lt;h2 id=&quot;strong-convex&quot;&gt;Strong Convex&lt;/h2&gt;
&lt;h3 id=&quot;alpha-strong-convex&quot;&gt;$\alpha$-Strong Convex&lt;/h3&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(x)-f(y)\le\triangledown f(x)^T(x-y)-\frac{\alpha}{2}\|x-y\|^2, \forall x,y&lt;/script&gt;

&lt;p&gt;를 만족할 때의 $\alpha$를 앞에 붙여 $\alpha$-strong convex라고 한다. 만약 $f$가 미분불가능하면 subgradient로 $\alpha$-strongly convexity를 정할 수 있다.&lt;/p&gt;

&lt;p&gt;다르게는
&lt;script type=&quot;math/tex&quot;&gt;f(y)\ge f(x)+\triangledown f(x)^T(y-x)+\frac{\alpha}{2}\|x-y\|^2&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;로 쓸 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;smooth&quot;&gt;Smooth&lt;/h2&gt;
&lt;h3 id=&quot;beta-smooth&quot;&gt;$\beta$-Smooth&lt;/h3&gt;
&lt;p&gt;$\triangledown f$가 $\beta$-Lipschitz이고&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\|\triangledown f(x)-\triangledown f(y)\|\le\beta\|x-y\|&lt;/script&gt;

&lt;p&gt;를 만족할 때의 $\beta$를 앞에 붙여 $\beta$-smooth라고 한다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(x)-f(y) \ge \triangledown f(x)^T(x-y)-\frac{\beta}{2}\|x-y\|^2, \forall x,y&lt;/script&gt;

&lt;p&gt;라고도 쓸 수 있고, 또는&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(y) \le f(x)+\triangledown f(x)^T(y-x)+\frac{\beta}{2}\|x-y\|^2&lt;/script&gt;

&lt;p&gt;라고도 쓸 수 있다.&lt;/p&gt;

&lt;p&gt;$\beta$-smooth 함수에서 gradient descent 알고리즘을 사용해 최적값을 찾을 때에는 &lt;script type=&quot;math/tex&quot;&gt;\|\triangledown f(x)-\triangledown f(x^* )\|\le\beta \|x-x^* \|&lt;/script&gt;이고, $\triangledown f(x^* )=0$이기 때문에 learning rate가 $\gamma = \frac{1}{\beta}$인 것이 좋다. 하지만 보통의 convex optimization에서는 $\beta$를 모르기 때문에 learning rate를 정하는 것이 어렵다.&lt;/p&gt;

&lt;p&gt;어떤 함수가 $\alpha$-strong이고 $\beta$-smooth이면 항상 $\alpha \le \beta$가 성립한다. 또한, convex optimization에서 $\alpha = \beta$일 때 가장 쉽게 해를 찾을 수 있다. 항상 $0 \le \frac{\alpha}{\beta} \le 1$ 이므로, $\frac{\alpha}{\beta}$가 1에 가까울수록 해를 찾기가 쉽다.&lt;/p&gt;

&lt;h2 id=&quot;second-order-characterization-of-convexity&quot;&gt;Second-order Characterization of convexity&lt;/h2&gt;

&lt;p&gt;$f$가 두 번 미분가능하면 $\forall x\in\mathbf{dom}{(f)}$에서 항상 $\triangledown^2f(x)$가 존재한다. 그리고&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f \text{ is convex }\Leftrightarrow \triangledown^2 f(x) \text{ is positive semidefinite }\forall x\in\mathbf{dom}{(f)}&lt;/script&gt;

&lt;p&gt;가 항상 성립한다.&lt;/p&gt;

&lt;h3 id=&quot;def-positive-definite&quot;&gt;(Def) Positive Definite&lt;/h3&gt;

&lt;p&gt;$A\in\mathbb{R}^{d\times d}$는&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x^TAx \gt 0, \forall x\in \mathbb{R}^d&lt;/script&gt;

&lt;p&gt;를 만족할 때 positive definite라고 한다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x^TAx\ge 0 , \forall x\in \mathbb{R}^d&lt;/script&gt;

&lt;p&gt;를 만족하면 positive semidefinite이라고 한다.&lt;/p&gt;

&lt;h2 id=&quot;hessian-smooth-strong-convexity&quot;&gt;Hessian, Smooth, Strong convexity&lt;/h2&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f \text{ 가 }\alpha\text{-strongly convex 이고 } \beta\text{-smooth 이다} \Leftrightarrow \alpha I \le \triangledown^2f(x)\le\beta I&lt;/script&gt;

&lt;p&gt;를 항상 만족하는데, 행렬 $A, B$에 대하여 $A\le B$는 $B-A$가 positive semidefinite임을 뜻한다. 또한, 이는 $\triangledown^2 f(x)$의 eigenvalue들이 $\alpha$보다 크고 $\beta$보다 작음을 뜻한다.&lt;/p&gt;

&lt;p&gt;hessian은 이러한 성질 때문에 step size를 정하기에 매우 중요한데, 이를 second-order method라고 한다. 하지만 계산량이 많고 저장할 숫자도 많으므로 딥러닝에서는 first-order method만 사용한다.&lt;/p&gt;

&lt;h3 id=&quot;proof-rightarrow&quot;&gt;(Proof) $\Rightarrow$&lt;/h3&gt;

&lt;p&gt;우선 $\alpha$-strongly convex이기 때문에&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;f(y)-f(x)\le\triangledown f(y)^T(y-x)-\frac{\alpha}{2}\|x-y\|^2&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;f(x)-f(y)\le\triangledown f(x)^T(x-y)-\frac{\alpha}{2}\|x-y\|^2&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;가 항상 성립한다. 마찬가지로 $\beta$-smooth이기 때문에&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;f(y)-f(x)\ge\triangledown f(y)^T(y-x)-\frac{\beta}{2}\|x-y\|^2&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;f(x)-f(y)\ge\triangledown f(x)^T(x-y)-\frac{\beta}{2}\|x-y\|^2&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;가 항상 성립한다.&lt;/p&gt;

&lt;p&gt;$\alpha$에 대한 두 식을 합치면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;0\le(\triangledown f(x)-\triangledown f(y))^T(x-y)-\alpha\|x-y\|^2\cdots(1)&lt;/script&gt;

&lt;p&gt;를 얻게 되고, 마찬가지로 $\beta$에 대한 식을 합치면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;0\ge(\triangledown f(x)-\triangledown f(y))^T(x-y)-\beta\|x-y\|^2\cdots(2)&lt;/script&gt;

&lt;p&gt;를 얻게 된다.&lt;/p&gt;

&lt;p&gt;$x=y+ht$룰 대입하면 ($h$는 벡터, $t$는 스칼라)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;(\triangledown f(x)-\triangledown f(y))^T(x-y) = \frac{\triangledown f(y+ht)-\triangledown f(y)}{t}ht^2&lt;/script&gt;

&lt;p&gt;를 얻을 수 있는데, 여기에 극한을 취하면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\lim_{t\rightarrow 0}{\frac{\triangledown f(y+ht)-\triangledown f(y)}{t}}ht^2=h^T\triangledown^2f(y)ht^2&lt;/script&gt;

&lt;p&gt;을 얻을 수 있다.&lt;/p&gt;

&lt;p&gt;이를 $(1)$과 $(2)$에 대입하면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\alpha t^2\|h\|^2\le h^T\triangledown^2f(y)t^2h\le \beta t^2\|h\|^2&lt;/script&gt;

&lt;p&gt;를 얻게 되고,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;h^T\alpha h\le h^T\triangledown^2 f(y)h\le h^T\beta h, \forall h&lt;/script&gt;

&lt;p&gt;를 얻을 수 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;0\le h^T(\triangledown^2 f(y)-\alpha)h, 0\le h^T(\beta-\triangledown^2 f(y))h , \forall h&lt;/script&gt;

&lt;p&gt;이므로,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\alpha I \le \triangledown^2 f(y)\le \beta I&lt;/script&gt;

&lt;p&gt;와 동치이다.&lt;/p&gt;

&lt;h3 id=&quot;proof-leftarrow&quot;&gt;(Proof) $\Leftarrow$&lt;/h3&gt;

&lt;p&gt;Taylor Thm을 이용하면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(y) = f(x) + \triangledown f(x)^T(y-x) + \frac{1}{2}(y-x)^T\triangledown^2f(c)(y-x), c=x+(y-x)t, \forall t\in[0, 1]&lt;/script&gt;

&lt;p&gt;라고 할 수 있는데, 이 때&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{1}{2}(y-x)T\triangledown f(c)^T(y-x)\ge\frac{1}{2}k\|y-x\|^2&lt;/script&gt;

&lt;p&gt;를 만족하는 $k$를 $\alpha$라고 하고,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{1}{2}(y-x)T\triangledown f(c)^T(y-x)\le\frac{1}{2}k\|y-x\|^2&lt;/script&gt;

&lt;p&gt;를 만족하는 $k$를 $\beta$라고 하면 $f$는 $\alpha$-strongly convex이고 $\beta$-smooth이다.&lt;/p&gt;

&lt;h2 id=&quot;vanila-analysis&quot;&gt;Vanila Analysis&lt;/h2&gt;
&lt;p&gt;만약 $f$가 strongly convex도 아니고, smooth하지도 않지만 $L$-Lipschitz continuous하다고 했을 때, $f(x)-f(x^* )$를 gradient descent를 이용해 어떻게 bound하는지에 대해 알아보자. 여기서는 &lt;script type=&quot;math/tex&quot;&gt;2a^Tb = \|a\|^2+\|b\|^2-\|a-b\|^2 \cdots(* )&lt;/script&gt;임을 이용할 것이다.&lt;/p&gt;

&lt;p&gt;$f(x_t)-f(x^* )$&lt;/p&gt;

&lt;p&gt;$\le\triangledown f(x_t)^T(x_t-x^* )$&lt;/p&gt;

&lt;p&gt;$=\frac{1}{\gamma}(x_t-x_{t+1})^T(x_t-x^* )$ ($x_{t+1} = x_t-\gamma\triangledown f(x_t)$, gradient descent)&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;=\frac{1}{2\gamma}(\|x_t-x_{t+1}\|^2+\|x_t-x^* \|^2-\|x_{t+1}-x^* \|^2)&lt;/script&gt; (위 $(* )$에 의해)&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;=\frac{\gamma}{2}\|\triangledown f(x_t)\|^2+\frac{1}{2\gamma}(\|x_t-x^* \| -\|x_{t+1}-x^* \|^2)&lt;/script&gt; 이다.&lt;/p&gt;

&lt;p&gt;이들을 모든 $t$에 대해 다 더하면,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{t=0}^{T-1}\left(f(x_t)-f(x^* )\right)\le\frac{1}{2\gamma}(\|x_0-x^* \|^2-\|x_T-x^* \|^2) + \sum_{t=0}^{T-1}\frac{\gamma}{2}\|\triangledown f(x_t)\|^2&lt;/script&gt;

&lt;p&gt;가 된다. $f$가 Lipschitz continuous라고 했으므로 항상 &lt;script type=&quot;math/tex&quot;&gt;\|\triangledown f(x_t)\|^2 \le L^2&lt;/script&gt;이다. &lt;script type=&quot;math/tex&quot;&gt;\bar{x} = \frac{x_0+\cdots+x_{T-1}}{T}&lt;/script&gt;라고 하면, &lt;script type=&quot;math/tex&quot;&gt;f(\bar{x})\le\frac{1}{T}\sum_{t=0}^{T-1}f(x_t)&lt;/script&gt; 임이 성립하고,&lt;/p&gt;

&lt;p&gt;$f(\bar{x})-f(x^* )$&lt;/p&gt;

&lt;p&gt;$\le\frac{1}{T}\sum_{t=0}^{T-1}\left(f(x_t)-f(x^* )\right)$&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\le \frac{1}{2\gamma T}\|x_0-x^* \|^2 + \frac{\gamma}{2}L^2&lt;/script&gt; (Lipschitz continuous에 의해)&lt;/p&gt;

&lt;p&gt;$=\frac{1}{2\gamma T}R^2 + \frac{\gamma}{2}L^2$ ($R = x_0-x^* $라고 하자)&lt;/p&gt;

&lt;p&gt;가 된다. 따라서 여기에 적합한 learning rate 를 rough하게 계산해보면 $\gamma = \frac{R}{L\sqrt{T}}$가 적합하다. (마지막 식을 미분해서 0 되는 $\gamma$ 찾음)&lt;/p&gt;

&lt;p&gt;이를 대입해 보면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(\bar{x})-f(x^* )\le\frac{RL}{\sqrt{T}}&lt;/script&gt;

&lt;p&gt;가 성립한다.&lt;/p&gt;

&lt;h3 id=&quot;l-lipschitz-continuous&quot;&gt;$L$-Lipschitz Continuous&lt;/h3&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\|f(x)-f(y)\| \le L\|x-y\|, \forall x,y&lt;/script&gt;

&lt;p&gt;를 만족할 때 $L$-Lipschitz continuous라고 하고, $f$가 미분가능하면 &lt;script type=&quot;math/tex&quot;&gt;\|\triangledown f(x)\|\le L&lt;/script&gt;을 만족할 때 Lipschitz continuous라고 한다.&lt;/p&gt;

&lt;p&gt;이 때 $\gamma = \frac{1}{L}$로 잡으면 diverge는 방지할 수 있지만 fast converge는 보장하지 못한다.&lt;/p&gt;
</description>
        <pubDate>Mon, 09 Sep 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/lecture/optimization/2019/09/09/OptLecture3.html</link>
        <guid isPermaLink="true">http://localhost:4000/lecture/optimization/2019/09/09/OptLecture3.html</guid>
        
        
        <category>Lecture</category>
        
        <category>Optimization</category>
        
      </item>
    
      <item>
        <title>Optimization Lecture 2</title>
        <description>&lt;h1 id=&quot;convex-optimization&quot;&gt;Convex Optimization&lt;/h1&gt;

&lt;h2 id=&quot;convex-optimization-1&quot;&gt;Convex Optimization&lt;/h2&gt;

&lt;p&gt;Convex Optimization이란 $x^* = \text{min}_{x \in C} f(x)$를 찾는 문제이다. 이 때 $f$는 convex function이고, $C$는 convex set이다. 수렴도도 중요하지만, 얼마나 빨리 수렴하는지도 수렴 알고리즘의 선택에서 중요한 요소이다.&lt;/p&gt;

&lt;h3 id=&quot;convergence-rate&quot;&gt;Convergence Rate&lt;/h3&gt;

&lt;p&gt;convergence rate은 $f(x^* )$가 optimal value일 때,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{f(x_t)-f(x^* )}{g(t)} \le c&lt;/script&gt;

&lt;p&gt;에서의 $g(t)$이다. 이 때 $x_t$는 $x$가 $t$번 업데이트 된 값이고, $c$는 상수이다. 주로&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;$g(t) = \frac{1}{t}$&lt;/li&gt;
  &lt;li&gt;$g(t) = \frac{1}{\sqrt{t}}$&lt;/li&gt;
  &lt;li&gt;$g(t) = e^{-t}$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;가 쓰이는데, $g(t) = e^{-t}$가 가장 수렴속도가 빠르다.&lt;/p&gt;

&lt;p&gt;$f(x_t)-f(x^* ) \le \epsilon$으로 만들 때,&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;$g(t) = \frac{1}{t}$일 때는 $\frac{1}{\epsilon}$번의 step이 필요하고,&lt;/li&gt;
  &lt;li&gt;$g(t) = \frac{1}{\sqrt{t}}$일 때는 $\frac{1}{\epsilon^2}$번의 step,&lt;/li&gt;
  &lt;li&gt;$g(t) = e^{-t}$일 때는 $\ln{\frac{1}{\epsilon}}$번의 step이 필요하며, 가장 빠르다.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;convex-functions&quot;&gt;Convex Functions&lt;/h2&gt;

&lt;p&gt;Convex Function의 예시에는 다음과 같은 함수들이 있다.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Linear Functions : $f(x) = a^Tx$&lt;/li&gt;
  &lt;li&gt;Affine Functions : $f(x) = a^Tx + b$&lt;/li&gt;
  &lt;li&gt;Exponential Functions : $f(x) = e^{\alpha x}$&lt;/li&gt;
  &lt;li&gt;Norms&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;norms&quot;&gt;Norms&lt;/h3&gt;

&lt;p&gt;주어진 벡터공간 $V$에서의 norm은 0보다 크거나 같은 scalar function $p$이다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p : V \rightarrow [0, +\infty)&lt;/script&gt;

&lt;p&gt;norm은 다음과 같은 특징을 가진다.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;$p(x) + p(x) \ge p(x+y), \forall x, y\in V$&lt;/li&gt;
  &lt;li&gt;$p(ax) = | a|p(x)$&lt;/li&gt;
  &lt;li&gt;$p(x) = 0 \Leftrightarrow x=0$&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;p-norm&quot;&gt;p-norm&lt;/h3&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\|x\|_ p = \left(\sum_{i=1}^d |x_i|^p\right)^{\frac{1}{p}}&lt;/script&gt;

&lt;p&gt;주로 쓰이는 $p$값은 $1, 2, \infty$이다.&lt;/p&gt;

&lt;h3 id=&quot;example-of-norms&quot;&gt;Example of Norms&lt;/h3&gt;
&lt;p&gt;young’s inequality : $ab\le\frac{a^p}{p}+\frac{b^q}{q}, \forall p, q \ge 1, \frac{1}{p}+\frac{1}{q}=1$&lt;br /&gt;
Hoelder inequality : $|\sum_{i=1}^n u_i\cdot v_i| \le \left(\sum_{i=1}^n|u_i|^p\right)^{\frac{1}{p}}\left(\sum_{i=1}^n|v_i|^q\right)^{\frac{1}{q}}$&lt;br /&gt;
Minkowski inequality : $\left(\sum_{i=1}^n|x_i+y_i|^p\right)^{\frac{1}{p}}\le\left(\sum_{i=1}^n|x_i|^p\right)^{\frac{1}{p}}+\left(\sum_{i=1}^n|y_i|^p\right)^{\frac{1}{p}}$&lt;/p&gt;

&lt;p&gt;Hoelder’s inequality를 증명하는 데에 young’s inequality를 사용하고, Minkowski inequality를 증명하는 데에 Hoelder’s inequality를 사용한다.&lt;/p&gt;

&lt;h3 id=&quot;proof-youngs-inequality&quot;&gt;(Proof) Young’s inequality&lt;/h3&gt;

&lt;h3 id=&quot;proof-hoelders-inequality&quot;&gt;(Proof) Hoelder’s inequality&lt;/h3&gt;

&lt;h3 id=&quot;proof-minkowski-inequality&quot;&gt;(Proof) Minkowski inequality&lt;/h3&gt;

&lt;h2 id=&quot;proof-every-norm-is-convex&quot;&gt;(Proof) Every norm is Convex&lt;/h2&gt;
&lt;p&gt;(Recall) triangle inequality : $p(x) + p(y) \ge p(x+y), \forall x, y \in V$&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}\alpha p(x) + (1-\alpha)p(y) &amp;
= p(\alpha x) + p((1-\alpha)y) \\
&amp; \ge p(\alpha x + (1-\alpha)y)\end{align} %]]&gt;&lt;/script&gt;

&lt;h2 id=&quot;lemma-jensens-inequality&quot;&gt;(Lemma) Jensen’s inequality&lt;/h2&gt;

&lt;p&gt;$f$가 convex이고, $x_1, x_2, \cdots, x_m \in \text{dom}(f)$이고, &lt;script type=&quot;math/tex&quot;&gt;\lambda_1, \lambda_2, \cdots, \lambda_m \in \mathbb{R}_+&lt;/script&gt; s.t. $\sum_{i=1}^m \lambda_i = 1$일 때,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f\left(\sum_{i=1}^m \lambda_ix_i\right) \le \sum_{i=1}^m \lambda_i f(x_i)&lt;/script&gt;

&lt;p&gt;$x$가 random variable이라면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(E(x)) \le E[f(x)]&lt;/script&gt;

&lt;p&gt;이 부등식은 $\lambda_1 + \lambda_2 =1$이고 $\lambda_1, \lambda_2 \ge 0$일 때&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\lambda_1f(x_1) +\lambda_2f(x_2) \ge f(\lambda_1x_1+\lambda_2x_2)&lt;/script&gt;

&lt;p&gt;라는 convex의 정의를 이용해, 수학적 귀납법으로 증명할 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;proof&quot;&gt;(Proof)&lt;/h3&gt;
&lt;p&gt;m일 때 성립한다고 가정한다. 그러면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}f(\sum_{i=1}^{m+1}\lambda_ix_i)
&amp; =f(\sum_{i=1}^m\lambda_ix_i + \lambda_{m+1}x_{m+1})\\
&amp; =f((1-\lambda_{m+1})\sum_{i=1}^m\frac{\lambda_i}{1-\lambda_{m+1}}x_i+\lambda_{m+1}x_{m+1})\\
&amp; \le(1-\lambda_{m+1})f(\sum_{i=1}^m\frac{\lambda_i}{1-\lambda_{m+1}}x_i)+\lambda_{m+1}f(x_{m+1})\\
&amp; \le(1-\lambda_{m+1})\sum_{i=1}^m\frac{\lambda_i}{1-\lambda_{m+1}}f(x_i)+\lambda_{m+1}f(x_{m+1})\\
&amp; =\sum_{i=1}^{m+1}\lambda_if(x_i)\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;따라서,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f\left(\sum_{i=1}^{m+1}\lambda_ix_i\right)\le\sum_{i=1}^{m+1}\lambda_if(x_i)&lt;/script&gt;

&lt;p&gt;가 성립한다.&lt;/p&gt;

&lt;h2 id=&quot;differentiable-functions&quot;&gt;Differentiable Functions&lt;/h2&gt;

&lt;h3 id=&quot;differentiable&quot;&gt;Differentiable&lt;/h3&gt;
&lt;p&gt;$f$가 continuous한 일차미분식을 갖는다면, $f$는 differentiable하다. 이 특징은 매우 중요한데, 대부분의 최적화 iteration은 gradient값을 사용하기 때문이다.&lt;/p&gt;

&lt;h3 id=&quot;tangent-hyperplane&quot;&gt;Tangent Hyperplane&lt;/h3&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;g(x) = f(x_0) + \triangledown f(x_0)^T(x-x_0)&lt;/script&gt;

&lt;p&gt;이 tangent hyperplane이다. 이 직선은 $(x, f(x)$를 지나고 $\triangledown f(x_0)$의 기울기를 갖는다.&lt;/p&gt;

&lt;h2 id=&quot;first-order-characterization-of-convexity&quot;&gt;First-order Characterization of Convexity&lt;/h2&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\triangledown f(x) = \left(\frac{\partial f(x)}{\partial x_1}, \cdots, \frac{\partial f(x)}{\partial x_d}\right)^T&lt;/script&gt;

&lt;p&gt;가 first-order derivative이다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f : \text{ convex function} \Leftrightarrow f(y) \ge f(x) + \triangledown f(x)^T (y-x), \forall x, y \in \text{dom}(f)&lt;/script&gt;

&lt;h3 id=&quot;proof-leftarrow&quot;&gt;(Proof) $\Leftarrow$&lt;/h3&gt;

&lt;p&gt;$z = \theta x + (1-\theta) y, \theta\in [0, 1]$ 라고 하자.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}&amp; (f(x) \ge f(z) + \triangledown f(z)^T(x-z)) \times\theta \\
&amp; (f(y) \ge f(z) + \triangledown f(z)^T(y-z))\times(1-\theta)\\
&amp; \Rightarrow \theta f(x) + (1-\theta)f(y)\ge f(z) + \triangledown f(z)^T(\theta x+(1-\theta)y-z)\\
&amp; \therefore\theta f(x)+(1-\theta)f(y)\ge f(\theta x+(1-\theta)y)\end{align} %]]&gt;&lt;/script&gt;

&lt;h3 id=&quot;proof-rightarrow&quot;&gt;(Proof) $\Rightarrow$&lt;/h3&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}&amp;(1-t)f(x) + tf(y)\ge f(x+(y-x)t)\\
&amp; \Rightarrow f(y)\ge f(x) + \frac{f(x+(y-x)t)-f(x)}{t}\\
&amp; \text{ as } t\rightarrow 0, f(y)\ge f(x)+\triangledown f(x)^T(y-x)\end{align} %]]&gt;&lt;/script&gt;

&lt;h2 id=&quot;nondifferentiable-functions&quot;&gt;Nondifferentiable Functions&lt;/h2&gt;
&lt;p&gt;$f(x) = |x|$같은 함수의 경우, $x=0$부근에서 많은 접점이 있지만 정확한 극한값은 없다. 이 때 그냥 하나를 마음대로 정할 수 있는데, 이것이 subgradient이다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(y)\ge f(x) + g_x^T(y-x)&lt;/script&gt;

&lt;p&gt;에서 $g_x$가 subgradient. 위에서 예를 든 $f(x) = |x|$의 경우, $g\in[-1,1]$은 모두 subgradient가 될 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;second-order-characterization-of-convexity&quot;&gt;Second-order Characterization of Convexity&lt;/h2&gt;

&lt;p&gt;일변수함수에 대해서는 이차미분식이 그냥 하나의 식이지만, 다변수함수에서는 이차미분식이 Hessian이다. Hessian은&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\triangledown^2f(x) = \left(\begin{matrix}
  \frac{\partial^2f}{\partial x_1 \partial x_1} &amp; \frac{\partial^2f}{\partial x_1 \partial x_2} &amp; \cdots &amp; \frac{\partial^2f}{\partial x_1 \partial x_d} \\
  \frac{\partial^2f}{\partial x_2 \partial x_1} &amp; \frac{\partial^2f}{\partial x_2 \partial x_2} &amp; \cdots &amp; \frac{\partial^2f}{\partial x_2 \partial x_d} \\
  \vdots &amp; \vdots &amp; \cdots &amp; \vdots \\
  \frac{\partial^2f}{\partial x_d \partial x_1} &amp; \frac{\partial^2f}{\partial x_d \partial x_2} &amp; \cdots &amp; \frac{\partial^2f}{\partial x_d \partial x_d}
  \end{matrix}\right) %]]&gt;&lt;/script&gt;

&lt;p&gt;으로 정의된다.&lt;/p&gt;

&lt;h3 id=&quot;def-positive-definite&quot;&gt;(Def) Positive Definite&lt;/h3&gt;
&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;A\in \mathbb{R}^{d\times d}&lt;/script&gt;가 symmetric하고 $x^TAx \gt 0, \forall x\in\mathbb{R}^d$일 때 $f$는 positive definite이다.&lt;/p&gt;

&lt;h3 id=&quot;proof-f---convex-function-leftrightarrow-triangledown2fx-is-positive-semidefinite&quot;&gt;(Proof) $f : $ convex function $\Leftrightarrow \triangledown^2f(x)$ is positive semidefinite&lt;/h3&gt;
&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}&amp; g(t) = f(x+(y-x)t)\text{ 는 } t\text{에 대한 convex function이다}\\
&amp; \Leftrightarrow g'(t) = \triangledown f(x+(y-x)t)(y-x)\\
&amp; \Leftrightarrow g''(t) = (y-x)^T\triangledown^2f(x+(y-x)t)(y-x)\end{align} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;$g(t)$가 convex function이므로, &lt;script type=&quot;math/tex&quot;&gt;g''(t) \ge 0, \forall t, \forall x, y&lt;/script&gt;를 만족한다. 따라서 $t=0$일 때도 만족하는데, 이를 대입해보면 다음과 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;g''(t)\mid_ {t=0} = (y-x)^T\triangledown^2f(x)(y-x) \ge 0, \forall x, y&lt;/script&gt;

&lt;p&gt;이는 positive semidefinite의 정의이므로, 둘은 동치이다.&lt;/p&gt;

&lt;h2 id=&quot;local-minima-critical-point&quot;&gt;Local Minima, Critical Point&lt;/h2&gt;
&lt;h3 id=&quot;def-local-minima&quot;&gt;(Def) Local Minima&lt;/h3&gt;
&lt;p&gt;$\epsilon \gt0$에 대해, $f(x)\le f(y), \forall y \text{ s.t. }|y-x|\le\epsilon$을 만족하면 $x$는 local minima이다.&lt;/p&gt;

&lt;h3 id=&quot;def-critical-point&quot;&gt;(Def) Critical Point&lt;/h3&gt;
&lt;p&gt;$\triangledown f(x)=0$을 만족하면 $x$는 critical point이다. 만약 $f$가 convex라면, critical point는 global minima이다. critical point는 최적화에서는 bad point인데, tangent line이 기울기가 0이기 때문에 gradient가 존재하지 않아 local minima를 빠져나가기가 어렵다.&lt;/p&gt;

&lt;h2 id=&quot;constrained-minimization&quot;&gt;Constrained Minimization&lt;/h2&gt;
&lt;p&gt;$f$가 convex function이고, $X$가 convex set이면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(x^* )\le f(y), \forall y\in X \Leftrightarrow \triangledown f(x^* )^T(x-x^* ) \ge 0, \forall x\in X&lt;/script&gt;

&lt;h3 id=&quot;proofrightarrow&quot;&gt;(Proof)$\Rightarrow$&lt;/h3&gt;
&lt;p&gt;Let $f(x^* )\le f(y), \forall y \in X$&lt;br /&gt;
Since $f$ is convex,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}&amp;tf(x^* )+(1-t)f(y) \ge f(y+(x^* -y)t)\\
&amp;\Rightarrow f(x^* ) \ge \frac{f(y+(x^* -y)t)-f(y)}{t} + f(y)\\
&amp;\Rightarrow f(x^* )\ge (x^* -y)\triangledown f(y)+f(y) (\text{ as }t\rightarrow0)\\
&amp;\Rightarrow (y-x^* )\triangledown f(y)\ge f(y)-f(x^* ) \ge 0\\
&amp;\therefore (y-x^* )\triangledown f(y) \ge 0, \forall y\in X\end{align} %]]&gt;&lt;/script&gt;

&lt;h3 id=&quot;proof-leftarrow-1&quot;&gt;(Proof) $\Leftarrow$&lt;/h3&gt;
&lt;p&gt;Let $\triangledown f(x^* )^T(y-x^* )\ge 0, \forall y\in X$&lt;br /&gt;
$f$가 convex이기 때문에, $f(y)\ge f(x^* )+\triangledown f(x^* )^T(y-x^* )$이다.&lt;br /&gt;
$\Rightarrow f(y)-f(x^* )\ge\triangledown f(x^* )^T(y-x^* )\ge 0$&lt;br /&gt;
$\therefore f(y)\ge f(x^* ), \forall y\in X$&lt;/p&gt;

&lt;h2 id=&quot;thm-tayler&quot;&gt;(Thm) Tayler&lt;/h2&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}f(x) &amp;= f(0) + f'(0)x + \frac{1}{2}f^{\prime\prime}(0)x^2+\cdots\\
f(b) &amp;= f(a) + f'(a)(b-a) + \frac{1}{2}f^{\prime\prime}(c)(b-a)^2, c\in[a, b]\\
&amp; = f(a) + \triangledown f(a)^T(b-a) + \frac{1}{2}(b-a)^T\triangledown^2f(c)(b-a), c\in[a, a+\theta(b-a)], \theta\in[0, 1]\end{align}\\
 \therefore f(b) \ge f(a)+\triangledown f(a)^T(b-a) %]]&gt;&lt;/script&gt;
</description>
        <pubDate>Wed, 04 Sep 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/lecture/optimization/2019/09/04/OptLecture2.html</link>
        <guid isPermaLink="true">http://localhost:4000/lecture/optimization/2019/09/04/OptLecture2.html</guid>
        
        
        <category>Lecture</category>
        
        <category>Optimization</category>
        
      </item>
    
      <item>
        <title>Optimization Lecture 1</title>
        <description>&lt;p&gt;딥러닝에서 objective function의 설계는 중요하다. 이 때 문제를 더 간단하게 만들기 위해 convex의 개념을 도입한다.&lt;/p&gt;

&lt;h3 id=&quot;convex-set&quot;&gt;Convex set&lt;/h3&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{For } x \in C \text{ and } y \in C \Rightarrow (1-\alpha)x+\alpha y \in C, \forall\alpha\in[0, 1]&lt;/script&gt;

&lt;p&gt;convex set을 쓰면 line 전체가 constraint 안에 있기 때문에 line search를 할 때 쓴다. convex set을 가정한 방법들은 non-convex에서도 사용할 수 있다. local minima 부근에서는 convex이기 때문인데, 이 때 convex 방법들을 적용하면 global minima는 아니다.&lt;/p&gt;

&lt;h3 id=&quot;convex-function&quot;&gt;Convex function&lt;/h3&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\alpha f(x_1)+(1-\alpha)f(x_2) \ge f(\alpha x_1 + (1-\alpha)x_2), \forall x_1, x_2, \forall \alpha \in [0, 1]&lt;/script&gt;

&lt;p&gt;convex function의 가장 큰 특징은 global minima가 하나이고, 2차 미분식(또는 해시안 함수)이 항상 양수라는 점이다. 이또한 분석이 쉽다는 장점을 가지고 있다. 사실, unimodal function도 global minima가 하나지만, 이차미분항이 양수와 음수를 왔다갔다 할 수 있기 때문에 convex function보다는 더 분석이 어렵다.&lt;/p&gt;

&lt;p&gt;이차미분항이 양수라는 점에서 오는 장점은, convergence behavior를 알기가 쉽다는 점이다. 보통 잘 알고 있는 gradient descent 방법의 식은 다음과 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x_{t+1} \leftarrow x_t + \gamma\triangledown f(x)&lt;/script&gt;

&lt;p&gt;이 때 이차미분항이 양수이면, $\triangledown f(x)$의 양상을 더 쉽게 알 수 있다.&lt;/p&gt;
</description>
        <pubDate>Mon, 02 Sep 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/lecture/optimization/2019/09/02/OptLecture1.html</link>
        <guid isPermaLink="true">http://localhost:4000/lecture/optimization/2019/09/02/OptLecture1.html</guid>
        
        
        <category>Lecture</category>
        
        <category>Optimization</category>
        
      </item>
    
      <item>
        <title>CV Lecture 11 - Light</title>
        <description>&lt;h1 id=&quot;light&quot;&gt;Light&lt;/h1&gt;

&lt;h1 id=&quot;color-vector&quot;&gt;Color Vector&lt;/h1&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;terms&quot;&gt;Terms&lt;/h2&gt;
</description>
        <pubDate>Sat, 31 Aug 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/lecture/computervision/2019/08/31/Lecture11.html</link>
        <guid isPermaLink="true">http://localhost:4000/lecture/computervision/2019/08/31/Lecture11.html</guid>
        
        
        <category>Lecture</category>
        
        <category>ComputerVision</category>
        
      </item>
    
      <item>
        <title>Peeking Inside the Black-Box ; A Survey on Explainable Artificial Intelligence (XAI) - 2</title>
        <description>&lt;h1 id=&quot;axis-1-xai-methods-taxonomy--explainability-strategies&quot;&gt;AXIS 1. XAI Methods Taxonomy : Explainability Strategies&lt;/h1&gt;

&lt;p&gt;현재 있는 설명 방법들의 오버뷰를 제시할 것이다. 세 가지의 기준을 가지고 나눌 수 있는데,&lt;br /&gt;
A. 해석의 복잡도&lt;br /&gt;
B. 해석의 범위&lt;br /&gt;
C. 사용된 ML 모델에 대한 의존성&lt;br /&gt;
이다.&lt;/p&gt;

&lt;h2 id=&quot;a-complexity-related-methods&quot;&gt;A. Complexity Related Methods&lt;/h2&gt;

&lt;p&gt;모델의 복잡도는 직접적으로 해석력과 연관되어 있다. 일반적으로 더 복잡하면 더 설명하기 어렵다. 따라서 설명가능한 모델을 얻는 직접적인 방법은 본질적으로 설명가능한 알고리즘을 디자인하는 것이다.&lt;/p&gt;

&lt;p&gt;BRL 모델은 결정 트리 기반의 모델인데, 저자는 초기의 설명가능한 모델이 XAI에 더 적합하다고 주장한다. 시각화를 통해 결과를 설명할 수 있는 attention model과, SLIM이라고 불리는 데이터 기반의 점수 시스템을 제안하는 연구도 있다. 최근에는 해석력과 정확도의 trade-off를 강조하며 ‘설명가능한 모델은 정확도 하락이라는 cost를 갖는다’를 주장하는 연구들도 활발하다.&lt;/p&gt;

&lt;p&gt;설명가능한 모델을 만드는 대신, 복잡하고 높은 정확도를 갖는 블랙박스 모델을 만들고 역공학을 통해 설명을 제공하는 연구도 있다. 이 방법은 매우 복잡하고 비싸지만, 최근 XAI에서는 자연어, 시각화, 예시를 통한 설명이라는 분야들이 활발히 연구되고 있다. 만약 높은 정확도가 필수적이라면, 이 연구가 꼭 필요하다.&lt;/p&gt;

&lt;h2 id=&quot;b-scoop-related-methods&quot;&gt;B. Scoop Related Methods&lt;/h2&gt;

&lt;p&gt;모델의 해석력은 두가지의 범위가 있다.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;모델 전체의 행동을 이해 (Global interpretability)&lt;/li&gt;
  &lt;li&gt;하나의 예측을 이해 (Local interpretability)&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;1-global-interpretability&quot;&gt;1. Global Interpretability&lt;/h3&gt;
&lt;p&gt;전체 해석력은 모델의 전체 논리를 이해하고 전체 모든 가능한 결과에 대해 추론하게 해준다. 많은 대상에 대한 추론에 사용한다.&lt;/p&gt;

&lt;p&gt;GRIP이라고 불리는, local explanation을 이용해 모델의 global 설명 트리를 만들 수 있는 방법이 제안되었다. 이 방법으로 모델이 정당하게 작동하고 있는지, 아니면 오버핏되었는지를 알 수 있다. global 정보 추출을 위한 감독 방법도 제안되었다. 이 연구는 DNN이 해석 가능한 패턴 기반의 모델과 결합되어 해석될 수 있다는 아이디어를 돕는다. Activation maximization(선호되는 인풋을 종합하는 것) 기반의 접근법도 제안되었는데, 이미지 인식을 위해 global 해석가능한 모델을 만들어준다.&lt;/p&gt;

&lt;p&gt;global 해석력은 파라미터가 너무 많은 경우에 얻기 어렵다. 사람은 전체를 이해하기 위해 부분에 집중하는데, 따라서 local 해석력이 더 적용하기 좋다.&lt;/p&gt;

&lt;h3 id=&quot;2-local-interpretability&quot;&gt;2. Local Interpretability&lt;/h3&gt;
&lt;p&gt;특정한 결정에 대한 설명이나 하나의 예측을 하는 것은 해석력이 local에서 작용한다는 것을 말한다. 이러한 해석의 범위는 특정한 예를 들어서 모델이 왜 이런 선택을 했는지를 정의하는 데에 사용된다.&lt;/p&gt;

&lt;p&gt;LIME은 블랙박스 모델을 관심있는 특정 예측 근처에서 근사할 수 있다. LOCO는 local variable의 중요도를 알려주는, local explanation을 할 수 있는 또다른 유명한 방법이다. local gradient를 이용해 local decision을 설명할 수 있는 방법도 있는데, 이 방법을 이미지 분류에도 쓸 수 있다. 이미지 분류 시스템에서 최종 클래스를 찾을 때 중요한 region을 찾는 것은 일반적인 방법론인데, 이 region은 sensitivity map, saliency map, pixel attribution map 등으로 불린다.&lt;/p&gt;

&lt;p&gt;모델의 예측을 각각의 특징에 대한 개별적인 기여로의 분해에 기반하여 예시를 통해 모델을 설명하는 방법을 제시한 연구도 있다. 원래의 예측과, 특정 특징들을 무시하고 예측했을 때의 차이를 측정한다. local 방법들을 결합해주는 Shapely Explanation이라고 불리는 방법을 제안한 연구도 있다.&lt;/p&gt;

&lt;p&gt;유망한 연구들은 local과 global explanation의 장점들을 결합하는 연구들이다. 4가지의 가능한 결합들은&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;일반적인 global explanation&lt;/li&gt;
  &lt;li&gt;모델의 각 부분들이 어떻게 전체 결과에 영향을 주는지 (global explanation을 부분 수준에 적용)&lt;/li&gt;
  &lt;li&gt;왜 모델이 예시 그룹에 대해 이런 결정을 했는지 (그룹 수준에 local explanation 적용)&lt;/li&gt;
  &lt;li&gt;일반적인 local explanation
이다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;local 설명은 보통 DNN에서 쓰이지만, 학자들은 자신의 방법이 DNN이 아니라 어떤 모델이든 적용될 수 있다고 말한다.&lt;/p&gt;

&lt;h2 id=&quot;c-model-related-methods&quot;&gt;C. Model Related Methods&lt;/h2&gt;

&lt;p&gt;위에서 설명한 것처럼 local 설명은 어떤 모델이든 적용될 수 있다고 했다. 이런 방법들은 Model-agnostic 설명력이라고 하고, 특정 모델에만 쓰일 수 있는 방법들은 Model-specific 설명력이라고 한다.&lt;/p&gt;

&lt;h3 id=&quot;1-model-specific-interpretability&quot;&gt;1. Model-specific Interpretability&lt;/h3&gt;
&lt;p&gt;이 설명력의 단점은, 우리가 특정한 타입의 설명이 필요할 때 그 설명이 가능한 모델만 사용할 수 있고, 더 정확도가 높거나 표현력이 좋은 모델을 사용할 수 없다는 비용이 든다는 점이다. 따라서 최근에는 model-agnostic 방법을 쓰는 것이 유행이다.&lt;/p&gt;

&lt;h3 id=&quot;2-model-agnostic-interpretability&quot;&gt;2. Model-Agnostic Interpretability&lt;/h3&gt;
&lt;p&gt;이 종류에 속하는 방법들은 설명과 예측을 분리시킨다. 이들은 주로 사후 분석(post-hoc)이다. 4가지의 종류로 나눠서 설명할 것이다.&lt;/p&gt;

&lt;p&gt;a) Visualization&lt;br /&gt;
b) Knowledge Extraction&lt;br /&gt;
c) Influence Methods&lt;br /&gt;
d) Example-based Explanation&lt;/p&gt;

&lt;h4 id=&quot;a-visualization&quot;&gt;a) Visualization&lt;/h4&gt;
&lt;p&gt;DNN의 내부를 이해하기 위한 가장 일반적인 아이디어이다. 시각화는 반드시 감독학습에 적용되어야 한다. 대표적인 시각화 방법은&lt;br /&gt;
(1) Surrogate models&lt;br /&gt;
(2) Partial Dependence Plot(PDP)&lt;br /&gt;
(3) Individual Conditional Expectation (ICE)&lt;br /&gt;
이다.&lt;/p&gt;

&lt;p&gt;(1) 근사 모델은 복잡한 모델을 설명하는 데에 사용되는 간단한 모델이다. 나중의 블랙박스 모델 해석을 위하여 예측 단계에 학습된다. 그러나 간단한 근사 모델이 복잡한 것보다 더 표현력이 뛰어나다는 연구 결과는 없다. LIME 방법은 하나의 관찰 근처에서 local 근사 모델을 만드는 방법이다. 모델 행동을 표현해주는 결정트리를 뽑아주는 근사 방법 연구가 있다. 또한 근사 모델을 가지고 계층 시각화를 하는 방법도 제안되었다.&lt;/p&gt;

&lt;p&gt;(2) PDP는 하나 또는 그 이상의 변수와 블랙박스 모델의 예측 간의 평균적인 부분 관계를 시각화하는 데 도와주는 그래픽 표현이다. 감독학습 모델을 이해하기 위해서 PDP를 사용하는 연구들은 다음과 같다. Bayesian Additive Regression 예측기와 conditional average treatment effect 사이의 관계를 이해하기 위해 PDP를 사용한 연구가 있다. 생물학과 관련된 연구에서는, PDP를 stochastic gradient boosting에 사용했고, 비대칭 분류에서 랜덤포레스트와 PDP를 사용해 결정자와 반응 관계를 알아보는 것이 의미있음을 증명한 연구도 있다. 최근에는 랜덤포레스트를 시각화하기 위한 Forest Floor라는 방법이 제안되었는데, 이 방법은 PDP보다 특징 기여에 더 의존한다. 이 방법이 더 좋은 이유는, PDP의 평균에 의해서는 상호작용을 알 수 없다는 점이다.&lt;/p&gt;

&lt;p&gt;(3) ICE는 PDP를 확장한 것이다. PD는 대략적인 모델의 작동만을 보여주는데, ICE는 PDP 결과물을 흩트려서 상호작용과 개별 분산을 보여준다. 최근 연구들은 PDP보다는 ICE를 사용한다. 부분 중요도(partial importance, PI)와 개별 조건 중요도(individual conditional importance, ICI) 둘 다를 시각 툴로 사용한 local 특징 중요도 기반 방법을 제안한 연구가 있다.&lt;/p&gt;

&lt;h4 id=&quot;b-knowledge-extraction&quot;&gt;b) Knowledge Extraction&lt;/h4&gt;

&lt;p&gt;특히 ANN에 기반을 둔 모델이라면, ML 모델이 어떻게 동작하는지 아는 것은 어렵다. 모델은 학습하면서 내부 변수를 변경한다. 따라서 ANN에서 설명을 추출한다는 것은, 학습으로부터 얻은 정보와 내부 표현을 해석한다는 뜻이 된다. 두 가지의 주된 방법이 있는데,&lt;br /&gt;
(1) Rule Extraction&lt;br /&gt;
(2) Model Distillation&lt;br /&gt;
이다.&lt;/p&gt;

&lt;p&gt;(1) 복잡한 모델에서 인사이트를 얻을 수 있는 방법은 규칙 추출이다. 인풋과 아웃풋을 이용해서 어떻게 ANN이 결정을 내렸는지를 근사하는 규칙을 설명해주는 방법들의 연구가 많다. 이것은 전통적인 인공지능 전문가 시스템에서 사용하던 방법이다. 세 가지의 주된 방법들이 있다.&lt;br /&gt;
(a) Pedagogical(가르치는) rule extraction&lt;br /&gt;
(b) Decompositional rule extraction&lt;br /&gt;
(c) Eclectic(절충) rule extraction&lt;/p&gt;

&lt;p&gt;(b)는 ANN으로부터 학습된 각 유닛 수준에서 규칙을 추출하며 투명하게 보는 반면 (a)는 ANN을 블랙박스로 취급한다. (a)는 OSRE(Orthogonal Search-based Rule Extraction)에서 적용되었다. (c)는 (a)와 (b)를 융합한 방법이다.&lt;/p&gt;

&lt;p&gt;(2) 다른 방법은 모델 압축이다. 이 방법은 모델의 압축된 정보를 선생님 역할의 딥 네트워크에서 학생 역할의 얕은 네트워크로 전달하는 방법이다. 모델 압축은 계산량을 줄이기 위해 제안되었으나 이후에는 해석을 위해 사용되었다. Interpretable Mimic Learning은 깊은 망의 성능까지 따라하면서 robust한 결정을 내릴 수 있는 특징을 학습하는 방법이다. DarkSight는 dark knowledge의 개념에서 감명받아 만든 블랙박스 분류기의 예측을 해석하는 시각화 방법이다. 이는 지식 압축, 차원 축소, 시각화 방법들을 결합한 것이다.&lt;/p&gt;

&lt;h4 id=&quot;c-influence-methods&quot;&gt;c) Influence Methods&lt;/h4&gt;

&lt;p&gt;이 부류의 방법들은 인풋이나 내부 변수를 바꾼 뒤 모델 결과에 얼마나 영향을 미치는지를 알아내어 특징들의 중요도나 관련성을 예측한다. 이 방법들은 시각화될 수도 있다. 세 가지의 주된 방법이 있다.&lt;br /&gt;
(1) Sensitivity Analysis (SA)&lt;br /&gt;
(2) Layer-wise Relevance Propagation (LRP)&lt;br /&gt;
(3) Feature Importance&lt;br /&gt;
이다.&lt;/p&gt;

&lt;p&gt;(1) 민감도는 모델이 인풋이나 내부 변수들의 변화로부터 그 결과가 얼마나 영향을 받는지를 의미한다. SA는 데이터가 바뀌어도 내부 변수나 결과가 얼마나 안정되게 나오는지 검증하기 위해 사용된다. 모델의 데이터가 바뀌어도 모델이 안정적이라는 것은 믿음을 주기에 좋다. SA는 결과값 그 자체 대신 분산이 모델을 설명해 준다. 따라서 SA는 관계를 설명해 주는 것이 아니라 모델의 안정성을 테스트 하기 위해 사용된다. 또한 중요하지 않은 인풋 특징이나 시작점을 찾아서 없애는 도구로써 사용되고, 이후에 더 강력한 설명 방법을 사용할 수 있다.&lt;/p&gt;

&lt;p&gt;(2) LRP는 말단 노드에서 입력 노드쪽으로 역전파한다. 중요한 특징은 관계 보존이고, SA와는 반대로 어떤 특징이 결과에 가장 중요한지 알려준다.&lt;/p&gt;

&lt;p&gt;(3) 변수 중요도는 각 인풋 특징들이 얼마나 결과에 영향을 미치는지를 정량화한다. 모델 예측 에러의 증가는 특징의 중요도를 측정하기 위해 특징이 치환된 후에 계산된다. 중요한 특징을 치환한 뒤에는 모델 에러가 증가하고, 중요하지 않은 특징이 치환된다면 모델의 에러는 변하지 않는다. MCR(Model Class Reliance)은 모델에 상관없이 특징의 중요도를 계산할 수 있는 방법이다. SFIMP라는 특징 중요도를 계산하는 local 방법도 제안되었고, LOCO도 지역 변수 중요도를 사용했다.&lt;/p&gt;

&lt;h4 id=&quot;d-example-based-explanation&quot;&gt;d) Example-based Explanation&lt;/h4&gt;

&lt;p&gt;데이터셋에서 특정 예시들을 선택하여 모델의 행동을 설명한다. 이들은 어떤 모델이든 더 해석가능하게 만들기 때문에 대부분 모델에 상관없는 방법이다. model-agnostic과 다른 점은 내부 변수를 변경하거나 모델을 바꾸는 일 없이 예시들만 선택해서 모델을 설명한다는 점이다. 두 가지의 방법이 있는데,&lt;br /&gt;
(1) prototype and criticism&lt;br /&gt;
(2) Counterfactuals explanations&lt;br /&gt;
이다.&lt;/p&gt;

&lt;p&gt;(1) 프로토타입은 데이터들을 대표하는 예시들을 고르는 것이다. 각각이 어디에 속하는지는 프로토타입과 얼마나 비슷한지에 따라 결정되는데, 이것은 과일반화로 연결되기도 한다. 이를 피하기 위해 비평이라고 불리는, 프로토타입으로 잘 대표되지 않는 데이터들을 따로 뽑는다. MMD-critic은 프로토타임과 비평을 자동으로 찾아주는 비지도학습 방법이다.&lt;/p&gt;

&lt;p&gt;(2) 반사실 설명은 전체 논리를 설명할 필요 없이 반대 결론으로 이끌 수 있는 최소 조건만을 알려준다. 중요한 점은 설명해주는 것이 아니라 반대 예시를 찾는 것이다.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;model-agnostic의 최대 장점은 설명과 표현 수준에서 볼 때, 모델의 융통성이 높다는 점이다. 여러 다른 모델을 비교할 수 있다. 그렇지만 이들은 여러 근사 방법을 사용하여 표현되는데, 이 때 정확도가 떨어질 수도 있다. model-specific방법들이 더 직접적으로 표현되기 때문에 더 정확한 설명을 해줄 수도 있다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ML모델은 완전히 투명하거나/사후에 해석될 수 있다. 또한 local에서 해석되거나/global하게 해석될 수 있다. local 방법은 데이터에 초점을 맞추고, global 방법은 모델에 초점을 맞춘다. 신뢰 관점에서는, local 방법이 더 충실하다고 할 수 있다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Sat, 17 Aug 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/thesis/2019/08/17/XAI2.html</link>
        <guid isPermaLink="true">http://localhost:4000/thesis/2019/08/17/XAI2.html</guid>
        
        <category>XAI</category>
        
        
        <category>Thesis</category>
        
      </item>
    
      <item>
        <title>왜 XAI가 필요한가?</title>
        <description>&lt;h1 id=&quot;사람-vs-인공지능&quot;&gt;사람 vs. 인공지능&lt;/h1&gt;

&lt;h3 id=&quot;사람과-인공지능의-의사결정-과정&quot;&gt;사람과 인공지능의 의사결정 과정&lt;/h3&gt;

&lt;p&gt;사람 : 사진 10장정도로 학습 가능&lt;br /&gt;
인공지능 : 엄청난 수의 이미지 필요&lt;/p&gt;

&lt;h3 id=&quot;1-사람이-발달과정-중-얻는-직관&quot;&gt;1. 사람이 발달과정 중 얻는 직관&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;물리적 직관&lt;/li&gt;
  &lt;li&gt;심리적 직관&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;2-사람은-학습모델을-빠르게-구축&quot;&gt;2. 사람은 학습모델을 빠르게 구축&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;더 기초적인 문제로 조합&lt;/li&gt;
  &lt;li&gt;인과관계 이용&lt;/li&gt;
  &lt;li&gt;기존지식을 새로운 분야에 적용&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;3-사람은-빠르게-생각할-수-있다&quot;&gt;3. 사람은 빠르게 생각할 수 있다.&lt;/h3&gt;

&lt;h1 id=&quot;darpa&quot;&gt;DARPA&lt;/h1&gt;

&lt;h3 id=&quot;설명가능한-모델-구축&quot;&gt;설명가능한 모델 구축&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;심층 설명 학습 : DNN의 각 레이어에서 의미있는 특징을 학습하도록 함&lt;/li&gt;
  &lt;li&gt;해석 가능한 모델 : 해석 가능한 인과관계 모델 구축 ( 모델을 작은 조각들의 조합으로 표현)&lt;/li&gt;
  &lt;li&gt;모델 귀납 : 원래 있던 모델을 해석&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;이유-설명-인터페이스&quot;&gt;이유 설명 인터페이스&lt;/h3&gt;

&lt;h3 id=&quot;인과관계-모델--딥러닝--mrf&quot;&gt;인과관계 모델 : 딥러닝 + MRF&lt;/h3&gt;

&lt;h3 id=&quot;lrp--계층적-상관성-전파-입력의-어느-부분이-출력에-영향을-미쳤는지-알려줌&quot;&gt;LRP : 계층적 상관성 전파. 입력의 어느 부분이 출력에 영향을 미쳤는지 알려줌&lt;/h3&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;future-study&quot;&gt;Future Study&lt;/h1&gt;
&lt;p&gt;LRP, MRF, BPL&lt;/p&gt;
</description>
        <pubDate>Mon, 12 Aug 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/note/2019/08/12/WHYXAI.html</link>
        <guid isPermaLink="true">http://localhost:4000/note/2019/08/12/WHYXAI.html</guid>
        
        <category>XAI</category>
        
        
        <category>Note</category>
        
      </item>
    
      <item>
        <title>Peeking Inside the Black-Box ; A Survey on Explainable Artificial Intelligence (XAI) - 1</title>
        <description>&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;4차 산업혁명의 여명으로, 우리의 일상에서 더 알고리즘적인 사회에 다가가갈 수 있게 하는 인공지능이 빠르게 적용되고 있다. 그러나, 전례 없는 발전에도, AI시스템의 가장 큰 문제점은 투명성이 없다는 것이다. 당연히 이 시스템의 블랙박스인 특성이 강력한 예측을 하게 해주지만, 직접적으로는 설명될 수 없다. 이 문제는 XAI에 대한 논쟁을 야기했다. 이 연구분야는 AI시스템의 믿음과 투명성을 향상시킬 것이라는 희망을 갖고 있다. AI가 계속해서 이어나가려면 XAI는 필수요소이다. 이 서베이는 관심있는 연구자들과 연습자들이 초기 단계이지만 빠르게 성장하는 XAI의 주요 양상들을 배울 수 있도록 하는 입구가 될 것이다. 주제, 논쟁 트렌드, 주 연구 궤적을 문헌들을 통해서 현존하는 접근법을 리뷰할 것이다.&lt;/p&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;h3 id=&quot;context&quot;&gt;Context&lt;/h3&gt;

&lt;p&gt;현재 인공지능은 덜 중요한 일에는 매우 많이 쓰이고 있지만, 매우 중요한 일에는 설명이 되지 않기 때문에 쓰이지 않고 있다. 이것이 XAI가 중요한 이유이다. XAI는 더 투명한 AI를 만들기 위한 발걸음이다. XAI는 좋은 성능은 잃지 않으면서도 설명 가능한 모델을 만들기 위한 연구이다.&lt;/p&gt;

&lt;h3 id=&quot;xais-landscape-dynamic&quot;&gt;XAI’s Landscape Dynamic&lt;/h3&gt;

&lt;p&gt;최근 많은 컨퍼런스에서 XAI를 비중있게 다루었는데, 그 중 눈에 띠는 연구 그룹은 FAT와 DARPA이다. FAT(Fairness, Accountability, Transparency) 는 많은 참가자와 논문을 포함했다. DARPA는 2017년에 XAI프로그램을 설립했고, 2021년까지 11개의 프로젝트를 진행한다. DARPA는 미 국방부의 지원을 받지만, 다양한 학문 기관에서 온 연구자들과 함께 다양한 팀을 꾸린다.&lt;/p&gt;

&lt;h3 id=&quot;contribution-and-organization&quot;&gt;Contribution and Organization&lt;/h3&gt;

&lt;p&gt;이 논문은 XAI의 기반을 확실히 해 놓아야 다음 연구로 진전이 될 수 있을 것을 주장한다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;XAI가 왜 필요한지&lt;/li&gt;
  &lt;li&gt;XAI 접근법 오버뷰&lt;/li&gt;
  &lt;li&gt;미래 연구 분야&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;를 이 논문이 알려준다.&lt;/p&gt;

&lt;h1 id=&quot;background&quot;&gt;Background&lt;/h1&gt;

&lt;h3 id=&quot;understanding-xai--a-contextual-definition&quot;&gt;Understanding XAI : A Contextual Definition&lt;/h3&gt;

&lt;p&gt;XAI의 필요성은 1970년대부터 있어왔다. 그러나 AI에 대한 관심이 줄어들었는데, 그 이후 AI의 방향이 모델 구현하는 쪽으로 변하면서 예측 능력에 가려 덜 중요하다고 생각되었다. 2004년에 XAI라는 용어를 처음 썼다. AI가 발전하자, 그에 따라 XAI라는 용어도 다시 급부상했다.&lt;/p&gt;

&lt;p&gt;하지만 XAI에 대한 정확한 정의는 없다. 기관에 따라 다른 정의를 갖고 있다. DARPA에 따르면, “정확도를 유지하면서 설명 가능한 모델을 만드는 것, 유저에게 설명할 수 있어야 하며, 그로 인해 AI를 제어할 수 있어야 한다.” 이다.&lt;/p&gt;

&lt;p&gt;사실 과학 문헌에서는 explainable보다는 interpretable이 더 많이 쓰이지만, 퍼블릭에서는 반대이므로 XAI라는 이름이 붙었다.&lt;/p&gt;

&lt;h3 id=&quot;using-xai-the-need-and-the-application-opportunities&quot;&gt;USING XAI: THE NEED AND THE APPLICATION OPPORTUNITIES&lt;/h3&gt;

&lt;p&gt;XAI가 필요한 4가지 이유&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;결과가 옳다고 주장하기 위해&lt;/li&gt;
  &lt;li&gt;결과를 조절하기 위해&lt;/li&gt;
  &lt;li&gt;결과를 개선하기 위해&lt;/li&gt;
  &lt;li&gt;새로운 것을 발견하기 위해&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;그러나 구글 연구원 노빅은 사람도 자신의 선택을 설명하는 것을 잘 못한다고 했다. 설명력은 매우 중요하지만, 필수적이지는 않다. 설명을 잘 하는 모델이 정확도는 더 떨어질 수도 있기 때문이다. 게다가 비싸다. 그래서 특정 상황일 때만 XAI를 쓰는 것이 적절하다. AI 알고리즘의 복잡도로부터 발생되는 불투명도의 정도가 높을 때 (알고리즘이 복잡해서 우리가 못알아들을 때), 그리고 응용 분야의 에러 저항력이 낮을 때 (조금이라도 틀리면 치명적일 때) 사용해야 한다.&lt;/p&gt;

&lt;p&gt;XAI는 꼭 필요하지만, 아직 기술적으로 발전되지 않았다. 기존 설명 가능한 rule-based 모델이 있었지만 이를 DNN에 적용하기는 어렵다.&lt;/p&gt;

&lt;p&gt;80년도에 개발된 전문가 시스템은 설명가능하지만 성능이 별로였다. 그 뒤에 제안된 DNN은 엄청난 성능을 자랑하지만 복잡도가 높아 설명이 어렵다.&lt;/p&gt;

&lt;p&gt;다만, 모든 모델들이 같은 레벨의 불투명도를 가지는 것은 아니다. 보통은 정확도와 설명가능도의 trade off를 가지며, 대체로 설명가능도가 높으면 정확도가 낮다.&lt;/p&gt;

&lt;p&gt;XAI의 기초가 되는 분야는 Data Science, AI/ML, Human Science (사람은 어떻게 설명하나?), HCI(사람이 기계를 이해하고 믿으려면 어떻게 해야하나?) 이다.&lt;/p&gt;

&lt;h1 id=&quot;review&quot;&gt;Review&lt;/h1&gt;

&lt;h3 id=&quot;related-surveys&quot;&gt;Related Surveys&lt;/h3&gt;

&lt;p&gt;XAI에 대한 연구는 늘어나고 있지만, 총정리 서베이와 종류를 구별한 서베이는 별로 없다. 이 논문은 이전 연구들과는 다르게 여러 다른 관점에서 XAI 연구 성과에 대한 오버뷰를 제시할 것이다. 전체론과 투명성에 집중할 것이다.&lt;/p&gt;

&lt;p&gt;이후에는 4개의 주축으로 나누어 설명할 것인데, 다음과 같다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;axis 1 : XAI Methods Taxonomy - Explainability Strategies&lt;/li&gt;
  &lt;li&gt;axis 2 : XAI Measurement - Evaluating Explanations&lt;/li&gt;
  &lt;li&gt;axis 3 : XAI Perception - Human in the Loop&lt;/li&gt;
  &lt;li&gt;axis 4 : XAI Antithesis - Explain or Predict&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Sun, 11 Aug 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/thesis/2019/08/11/XAI1.html</link>
        <guid isPermaLink="true">http://localhost:4000/thesis/2019/08/11/XAI1.html</guid>
        
        <category>XAI</category>
        
        
        <category>Thesis</category>
        
      </item>
    
  </channel>
</rss>
