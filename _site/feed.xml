<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>YeonjeeJung's Blog</title>
    <description></description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Wed, 15 Jan 2020 22:49:08 +0900</pubDate>
    <lastBuildDate>Wed, 15 Jan 2020 22:49:08 +0900</lastBuildDate>
    <generator>Jekyll v3.8.6</generator>
    
      <item>
        <title>Cyclical Learning Rates for Training Neural Networks</title>
        <description>&lt;p&gt;2017년&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;abstract&quot;&gt;[Abstract]&lt;/h2&gt;

&lt;p&gt;이 논문에서는 cyclical learning rate를 제시하는데, 이 방법은 적당한 boundary값 사이를 왔다갔다 한다. 이 적당한 boundary를 찾는 방법 또한 제시하는데, learning rate을 몇 epoch동안 계속 늘려보는 방법으로 찾을 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;1-introduction&quot;&gt;[1] Introduction&lt;/h2&gt;

&lt;p&gt;이 논문에서는 훈련 중에 learning rate을 계속 바꾸는 것이 훈련에 이득이라는 것을 증명했다. 또한 CLR을 이용하면 좋은 성능을 위해서 lr을 튜닝하는 작업도 없앨 수 있다. 그리고 adaptive lr을 사용하면 들여야 하는 추가 계산도 필요하지 않다.&lt;/p&gt;

&lt;h2 id=&quot;2-related-work&quot;&gt;[2] Related work&lt;/h2&gt;

&lt;h3 id=&quot;adaptive-learning-rates&quot;&gt;Adaptive learning rates&lt;/h3&gt;

&lt;p&gt;adaptive lr은 CLR의 경쟁자로 볼 수 있다. adaptive lr은 global lr대신 local adaptive lr에 의존하는 대신, 많은 계산량이 필요하다.&lt;/p&gt;

&lt;p&gt;AdaGrad는 gradient로부터 lr을 예측하는 초기 adaptive lr 방법이다. RMSProp은 해당 weight에 사용되었던 최근 gradient를 평균내서 local adaptive lr을 계산하는 방법이다. 이후에는 gradient의 Hessian의 대각성분을 추정하는 방법이 제안되었으며, 해당 논문에서는 또한 lr을 크게 하는 것이 유의미하다는 것을 보였다. AdaDelta는 AdaGrad를 개선시킨 방법이다. 더 최근에는 AdaSecant방법이 제안되었는데, 이는 gradient의 root mean square 통계와 분산을 사용한 방법이다. ESGD는 RMSProp이 편향된 추정을 제공한다는 점을 개선한 방법이다. 이후 AdaGrad와 RMSProp을 합친 방법인 Adam이 나왔다.&lt;/p&gt;

&lt;p&gt;CLR과 adaptive lr은 완전히 다른 정책을 사용하고, 또한 둘은 합쳐질 수도 있다. CLR은 SGDR과 비슷하기도 하다.&lt;/p&gt;

&lt;h2 id=&quot;3-optimal-learning-rates&quot;&gt;[3] Optimal Learning Rates&lt;/h2&gt;

&lt;h3 id=&quot;31-cyclical-learning-rates&quot;&gt;[3.1] Cyclical Learning Rates&lt;/h3&gt;

&lt;p&gt;CLR의 핵심은 lr을 크게 하는 것이 단기로 보면 좋지 않지만 장기적으로는 이득이 된다는 것이다. 최댓값과 최솟값만 정해놓고 그 사이를 왔다갔다 하는 여러 함수로 실험해 보았는데 최종적으로는 삼각형 모양의 linear schedule을 선택했다. 이유는 이 아이디어를 따르는 가장 단순한 모양이기 때문이다.&lt;/p&gt;

&lt;p&gt;이전의 한 논문에서 훈련을 어렵게 하는 이유 중 하나가 안장점 때문이라는 연구가 있었는데, 안장점에서는 gradient가 작으므로 lr을 높임으로써 학습을 빠르게 할 수 있다. 따라서 CLR이 잘 작동하는 것이다. 또한, 최적의 lr은 bound 안에 있을 것이고 최적에 가까운 lr이 학습 중에 사용되기 때문에 잘 작동하는 것이라는 이유도 있다.&lt;/p&gt;

&lt;p&gt;triangular 방법 말고도 다른 두가지의 방법이 이 논문에서 논의된다. 하나는 triangular2 방법으로, triangular과 거의 비슷하지만 각 cycle이 끝날 때마다 lr의 차이가 반으로 줄어드는 방법이다. exp_range는 각 boundary가 $\gamma^{\text{iteration}}$만큼씩 줄어드는 방법이다.&lt;/p&gt;

&lt;h3 id=&quot;32-how-can-one-estimate-a-good-value-for-the-cycle-length&quot;&gt;[3.2] How can one estimate a good value for the cycle length?&lt;/h3&gt;

&lt;p&gt;실험상 stepsize의 2~8epoch로 하면 좋았다.&lt;/p&gt;

&lt;h3 id=&quot;33-how-can-one-estimate-reasonable-minimum-and-maximum-boundary-values&quot;&gt;[3.3] How can one estimate reasonable minimum and maximum boundary values?&lt;/h3&gt;

&lt;p&gt;최적의 bound를 찾으려면 몇 epoch를 lr을 크게 하면서 돌려보면 된다. 정확도가 증가하는 lr과 감소하는 lr을 각각 하한과 상한으로 잡으면 된다. 이 방법은 최적의 LR과 최적의 LR범위를 알려준다. &lt;em&gt;왜그런지는 모르겠음&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;4-experiments&quot;&gt;[4] Experiments&lt;/h2&gt;

&lt;h3 id=&quot;41-cifar-10-and-cifar-100&quot;&gt;[4.1] CIFAR-10 and CIFAR-100&lt;/h3&gt;

&lt;p&gt;우선 CIFAR-10 데이터셋을 가지고 실험을 했는데, triangular2 방법을 가지고 baseline의 정확도와 같은 정확도(81.4%)를 달성하는 데에 반복이 훨씬 덜 소요됐다.&lt;/p&gt;

&lt;p&gt;CLR이 잘되는 이유를 lr이 감소하는 부분 때문이라고 생각하고 감소하기만 하는 schedule로 실험을 해봤는데, 결과는 증가하는 부분과 감소하는 부분 둘 다가 있어야 더 좋은 성능을 갖는다는 것을 보였다. 마찬가지로, Caffe에 구현되어 있는 exp와 exp_range를 비교했는데 exp_range의 성능이 더 좋았다. 이 또한 진동하는 lr이 더 효과가 있음을 보여준다. CLR은 또한 batch normalization과도 결합될 수 있는데, 이때도 CLR을 적용한 것이 결과가 더 좋았다.&lt;/p&gt;

&lt;p&gt;이후에는 여러 다른 모델을 사용해서 CIFAR-10과 CIFAR-100에 실험했는데, ResNet, Stochastic Depth network (SD), DenseNet을 비교했다. 이들 모두 CLR을 사용해서 결과가 더 좋아지거나 적어도 비슷한 결과가 나왔다. 따라서 CLR을 사용하는 것이 효과가 있다는 것을 보여준다.&lt;/p&gt;

&lt;h3 id=&quot;42-imagenet&quot;&gt;[4.2] ImageNet&lt;/h3&gt;

&lt;p&gt;이 실험에서는 Caffe에서 제공하는 AlexNet을 baseline으로 잡고 실험했다. CLR을 사용한 것과 사용하지 않은 것이 별로 차이가 없ㅇ었는데, 이를 통해 원래의 baseline lr이 잘 정해졌다는 것을 알 수 있다. GoogleNet에서도 마찬가지로 CLR을 사용한 것이 조금 더 성능이 좋았다.&lt;/p&gt;
</description>
        <pubDate>Tue, 14 Jan 2020 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/thesis/2020/01/14/CyclicLR.html</link>
        <guid isPermaLink="true">http://localhost:4000/thesis/2020/01/14/CyclicLR.html</guid>
        
        <category>Optimization</category>
        
        
        <category>Thesis</category>
        
      </item>
    
      <item>
        <title>Automatically Inferring Data Quality for spatiotemporal Forecasting</title>
        <description>&lt;h2 id=&quot;abstract&quot;&gt;[Abstract]&lt;/h2&gt;

&lt;p&gt;시공간 데이터가 많이 쓰이고 있는데, 이때 데이터의 질이 다양할 수 있다. 이는 잘못하면 신뢰도가 떨어지는 예측으로 이어질 수 있고, 이는 블랙박스인 딥러닝에서는 치명적일 수 있다. 따라서 이 논문에서는 데이터의 질을 자동으로 알려주는 해결책을 제시한다.&lt;/p&gt;

&lt;h2 id=&quot;1-introduction&quot;&gt;[1] Introduction&lt;/h2&gt;

&lt;p&gt;여러 다른 센서의 사용으로 데이터에 다양한 노이즈가 낄 수 있고, 따라서 네트워크의 성능 저하가 생길 수 있다. 이 논문에서는 데이터의 질을 예측하는 DQ-LSTM 모델을 제시하는데, 이 모델은 data quality level에 대한 공간적 구조를 잘 탐색할 수 있고, 각 시계열에 대한 의존도도 파악할 수 있다. 관련 연구는 서로 다른 출처에서의 데이터의 질에 대한 연구와, 그래프의 신호를 학습하는 연구가 있었다. 데이터의 질에 관한 연구들은 대부분 label이 필요했는데, 이 논문의 방법에서는 label을 사용하지 않고 서로 다른 출처의 데이터를 구분할 수 있다. 그래프의 신호를 학습하는 연구에서는 특별히 CNN에 관한 연구가 있었는데 이는 지역화된 패턴들을 뽑을 수 있는 구조이다. 이 논문에서는 graph convolution layer를 사용하여 시공간 특징을 data quality에 맵핑하는 모델을 제안한다.&lt;/p&gt;

&lt;h2 id=&quot;2-preliminaries&quot;&gt;[2] Preliminaries&lt;/h2&gt;

&lt;h3 id=&quot;21-local-variation&quot;&gt;[2.1] Local Variation&lt;/h3&gt;

&lt;p&gt;local variation은 원래 $\triangledown_ix=(\frac{\partial x}{\partial e}|_ {e=(i,j)}|j\in \mathcal{N}_i)$라고 정의한다. 이때 $\mathcal{N}_i$는 $i$번째 vertex에 연결된 vertex들인데, vertex마다 연결된 개수가 다르면 차원이 일정하지 않을 수 있다. 따라서 최종적으로&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\|\triangledown_ix \|_ 2^2=\sum_{j\in\mathcal{N}_i}W_{ij}(x(j)-x(i))^2&lt;/script&gt;

&lt;p&gt;라고 정의된다. 이 정의에 따르면, 연결된 vertex의 $x$값 (시그널)들이 현재 vertex의 $x$값과 비슷하면 local variation은 작아진다. 또한, local variation은 $W$와 $x$의 함수라는 것도 알 수 있다.&lt;/p&gt;

&lt;p&gt;또한 $M$개의 서로 다른 센서들에서 온 신호가 있다고 할 때, 한 vertex의 local variation을&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L_i=(\|\triangledown_ix^1\|_ 2^2,\cdots,\|\triangledown_ix^m\|_ 2^2, \cdots, \|\triangledown_ix^M\|_ 2^2)&lt;/script&gt;

&lt;p&gt;로 나타낼 수 있다. 최종적으로,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L(W, X)=(D+W)(X\odot X)-2(X\odot WX)&lt;/script&gt;

&lt;p&gt;라고 할 수 있다. ($\odot$은 element-wise multiplication)&lt;/p&gt;

&lt;h3 id=&quot;22-data-quality-level&quot;&gt;[2.2] Data Quality Level&lt;/h3&gt;

&lt;p&gt;data quality level을 정할 때는 주위 신호와 현재 node의 신호가 큰 차이가 없는 것이 바람직하다고 가정한다. data quality level은 $s_i=q(L_i)$라고 할 수 있는데, 함수 $q$는 임의로 정할 수 있다. GCN이라는 그래프 파라미터를 학습할 수 있는 CNN 모델이 제안되었는데, 이 모델을 여러겹 쌓은 모델을 만들어 통과시킨 후 data quality level을 결정할 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;Z=\sigma_K(\hat{A}\sigma_{K-1}(\hat{A}\cdots\sigma_1(\hat{A}X\Theta_1)\cdots\Theta_{K-1})\Theta_K)&lt;/script&gt;&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;s=\sigma_L(L(W,Z)\Phi)&lt;/script&gt;&lt;/p&gt;

&lt;h2 id=&quot;3-model&quot;&gt;[3] Model&lt;/h2&gt;

&lt;h3 id=&quot;data-quality-network&quot;&gt;Data quality network&lt;/h3&gt;

&lt;p&gt;다층의 GCN이 그래프 신호와 DQL 사이에서 연관 신호를 추출하고 필터 크기를 늘릴 수 있다. 이 DQN으로 단일층 뉴럴 네트워크 $\Phi$와 활성함수 $\sigma_L$을 사용한다.&lt;/p&gt;

&lt;h3 id=&quot;long-short-term-memory&quot;&gt;Long short term memory&lt;/h3&gt;

&lt;p&gt;순간적인 신호를 처리하기 위해 이 논문에서는 LSTM을 사용했다. 길이가 $k$인 순차 신호를 LSTM에 넣고 다음 신호를 예측한다. 예측된 신호는 실제 신호와 비교되며 LSTM의 파라미터가 업데이트된다.&lt;/p&gt;

&lt;h3 id=&quot;dq-lstm&quot;&gt;DQ-LSTM&lt;/h3&gt;

&lt;p&gt;LSTM에 신호를 넣기 위해서는 전체 신호를 길이가 $k$인 신호로 세그먼트를 만들어야 한다. 그리고 모든 vertex에 대한 마지막 신호는 GCN의 입력이 된다. loss function은 다음과 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{L}_i=\frac{1}{n_i}\sum_{j=1}^{n_i}s_i\|\hat{\mathcal{X}}(i,:,k+j-1)-\mathcal{X}(i,:,k+j-1)\|_2^2+\beta\|\Phi\|_2^2&lt;/script&gt;

&lt;p&gt;$i$는 vertex의 인덱스이고, $j$는 세그먼트의 시작점, $k$는 세그먼트의 길이, $\hat{\mathcal{X}}$는 예측신호값, $\mathcal{X}$는 실제 신호, $s_i$는 노드의 quality level이다.&lt;/p&gt;

&lt;h2 id=&quot;4-experiments&quot;&gt;[4] Experiments&lt;/h2&gt;

&lt;p&gt;이 논문에서는 평가 척도로 mean absolute error (MAE) $\frac{\sum_{i=1}^n|y_i-x_i|}{n}$을 사용했다.&lt;/p&gt;

&lt;h3 id=&quot;42-graph-generation&quot;&gt;[4.2] Graph Generation&lt;/h3&gt;

&lt;p&gt;weather station에 대해서 그래프를 만들어야 하는데, 문제는 어디를 연결해야 하는지 정하는 것이다. 처음에는 거리 기반으로 연결을 정의하려 했으나, 데이터에 거리 외에도 많은 특징들이 있기 때문에 거리가 가까워도 다른 특징들이 많이 차이가 나는 현상이 생길 수 있다. 따라서 이 논문에서는 모든 특징에 똑같은 중요도를 두고 연결을 정의했다.&lt;/p&gt;

&lt;h3 id=&quot;43-baselines&quot;&gt;[4.3] Baselines&lt;/h3&gt;

&lt;p&gt;이 논문에서는 우선 확률론적 방법인 autoregressive와 비교하고, simple LSTM과 비교하고, 마지막으로 GCN과 비교한다.&lt;/p&gt;

&lt;h2 id=&quot;5-results-and-discussion&quot;&gt;[5] Results and Discussion&lt;/h2&gt;

&lt;h3 id=&quot;51-forecasting-experiments&quot;&gt;[5.1] Forecasting Experiments&lt;/h3&gt;

&lt;p&gt;이 논문에서의 결과에 따르면, $K$가 더 크면 더 큰 주변의 신호를 알게 되므로 결과가 더 좋아진다. GCN과 DQ-LSTM의 차이점은, GCN은 주어진 신호에서 data quality를 바로 예측하지만 DQ-LSTM은 local variation을 계산한 후 data quality를 예측한다. DQ-LSTM의 성능이 더 좋은 것으로 볼 때, local variation이 data quality 예측에 유용하게 쓰인다는 것을 알 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;52-node-embedding-and-low-quality-detection&quot;&gt;[5.2] Node Embedding and Low-Quality Detection&lt;/h3&gt;

&lt;p&gt;DQ-LSTM이 GCN과 연결되어 있으므로 GCN에서 나온 output을 가지고 embedding으로 표현할 수 있다. embedding은 낮은 차원을 갖고 있기 때문에 t-SNE와 같은 방법으로 시각화할 수 있다. 이 논문에서는 이 방법을 사용해 시각화했는데, 맨 처음에는 기준 노드에 연결된 노드들이 근처에 있다가, 한 노드가 자신과 연결된 다른 노드에 의해 영향을 더 받게 되면 기준 노드로부터 멀어진다.&lt;/p&gt;

&lt;p&gt;위에서 기준 노드로부터 멀어진 노드를 $F$라고 하고, 그와 연결된 다른 노드를 $C$라고 하자. $F$가 기준 노드와 멀어진 이유는 두 가지로 생각해볼 수 있는데, 하나는 $F$의 신호가 기준 노드에 연결된 다른 노드들의 신호와 매우 다른 것이고, 다른 하나는 $C$의 신호에 노이즈가 많아서 $F$에 영향을 준 것이다. 첫번째 이유는 이전의 가정에 위배되므로 사실이 아니고, 따라서 $C$와 $F$가 low-quality node의 후보가 될 수 있다.&lt;/p&gt;
</description>
        <pubDate>Fri, 10 Jan 2020 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/thesis/2020/01/10/DataQual.html</link>
        <guid isPermaLink="true">http://localhost:4000/thesis/2020/01/10/DataQual.html</guid>
        
        <category>Optimization</category>
        
        
        <category>Thesis</category>
        
      </item>
    
      <item>
        <title>Visualizing the Loss Landscape of Neural Nets</title>
        <description>&lt;h2 id=&quot;abstract&quot;&gt;[Abstract]&lt;/h2&gt;

&lt;p&gt;이 논문에서는 왜 특정 네트워크 구조가 학습이 잘되고 일반화가 잘되는지를 알기 위해, loss function의 구조와 지형을 시각화해보고자 한다.&lt;/p&gt;

&lt;h2 id=&quot;1-introduction&quot;&gt;[1] Introduction&lt;/h2&gt;

&lt;p&gt;이 논문에서 알아보고자 하는 것은&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;loss function의 특징&lt;/li&gt;
  &lt;li&gt;네트워크 구조가 loss지형에 미치는 영향&lt;/li&gt;
  &lt;li&gt;non-convex 구조가 학습에 미치는 영향&lt;/li&gt;
  &lt;li&gt;loss function의 지형이 일반화 특성에 미치는 영향&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;이다. 그리고 이들은 filter normalization이라는 방법을 사용했다.&lt;/p&gt;

&lt;h2 id=&quot;2-theoretical-background&quot;&gt;[2] Theoretical Background&lt;/h2&gt;

&lt;p&gt;연구자들은 sharpness나 flatness가 일반화 능력에 관련있다고 했다. 그래서 flatness를 최소값 근처의 면적, hessian의 eigenvalue 또는 sharpness를 local entropy를 이용해서 나타내기도 했으나, &lt;a href=&quot;https://arxiv.org/abs/1706.08947&quot;&gt;다른 연구&lt;/a&gt;에서는 일반화 능력은 이런 flatness의 측정값과는 관련이 없다고도 했다.&lt;/p&gt;

&lt;h2 id=&quot;3-the-basics-of-loss-function-visualization&quot;&gt;[3] The Basics of Loss Function Visualization&lt;/h2&gt;

&lt;p&gt;loss function을 시각화하는 데에는 1차원 방법과 2차원 방법이 있다. 1차원 방법은 두 점을 찍어서 그 두 점을 잇는 점들의 loss 값을 그래프로 그리는 방법인데, non-convexity를 시각화하기 어렵고, batch normalization이나 invariance symmetry(또는 scale invariance)를 고려하지 않는다. 2차원 방법은 한 점과 특정 두 방향을 정해서 그 길을 따라가며 2차원으로 그래프를 그리는 방법인데, 계산량이 많아서 resolution이 낮아 역시 non-convexity를 시각화하기 어렵다.&lt;/p&gt;

&lt;h2 id=&quot;4-proposed-visualization--filter-wise-normalization&quot;&gt;[4] Proposed Visualization : Filter-Wise Normalization&lt;/h2&gt;

&lt;p&gt;network weight에는 scale invariance라는 속성이 있는데, 이는 weight에 특정 수를 곱하거나 나누어도 전체 네트워크의 행동이 변하지 않는다는 것이다. 이런 특성 때문에 weight값이 한 unit이 바뀌어도, 원래의 scale이 컸으면 영향이 거의 없고, 원래의 scale이 작았으면 엄청난 영향이 된다. 따라서 여러 다른 minimizer나 다른 네트워크를 비교할 수 없게 된다. 이런 문제를 해결하기 위해 이 논문에서는 filter-wise normalization을 제안하는데, 위의 2차원 방법을 사용하되 두 방향을 고른 뒤 &lt;script type=&quot;math/tex&quot;&gt;d_{i,j}\leftarrow \frac{d_{i,j}}{\|d_{i,j}\|}\|\theta_{i,j}\|&lt;/script&gt;로 normalize하는 방법이다. 이렇게 하면 본래의 거리 scale을 유지할 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;5-the-sharp-vs-flat-dilemma&quot;&gt;[5] The Sharp vs. Flat Dilemma&lt;/h2&gt;

&lt;p&gt;1차원 방법으로 small batch의 minimizer $\theta^s$와 large batch의 minimizer $\theta^l$을 그려본 결과, $\theta^s$가 더 flat했고, 실제로 small batch를 사용하면 일반화가 더 잘된다. 하지만 weight decay를 적용하였더니 $\theta^l$이 더 flat했다. 사실 $\theta^s$의 weight들이 크기가 더 크다. 따라서 scale variance때문에 더 flat해보이기만 한 것이다. 마찬가지로 weight decay에서는 shrinkage 효과때문에 $\theta^s$의 weight 크기가 작아졌고, 따라서 $\theta^l$이 더 flat해보인 것이다. 따라서 sharpness와 일반화는 관련이 없다고 결론지을 수 있다.&lt;/p&gt;

&lt;p&gt;이 실험을 filter-wise normalization을 이용해서 다시 해봤는데, 비교해본 결과 weight decay를 사용한 것과 사용하지 않은 것에서 둘다 $\theta^s$이 더 flat했다. 따라서, 사실은 일반화는 sharpness와 관련이 있었지만, 여태까지의 시각화가 잘못된 것임을 알 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;6-what-makes-neural-networks-trainable-insights-on-the-nonconvexity-structure-of-loss-surfaces&quot;&gt;[6] What Makes Neural Networks Trainable? Insights on the (Non)Convexity Structure of Loss Surfaces&lt;/h2&gt;

&lt;p&gt;ResNet과, ResNet과 같은 구조인데 skip connection이 없는 네트워크를 filter-wise normalization을 이용해 비교해본 결과 관찰된 특성이 몇가지 있다.&lt;/p&gt;

&lt;h3 id=&quot;1-네트워크-깊이의-효과&quot;&gt;1. 네트워크 깊이의 효과&lt;/h3&gt;

&lt;p&gt;ResNet-20 with no skip connection은 convex한 모습을 띠는데, 이는 VGG-19와 비슷한 구조로, 원래 학습이 잘되는 구조이다. 그런데 ResNet-56와 ResNet-110 with no skip connection은 복잡하고 non-convex한 모양을 띤다. 반대로 그냥 ResNet-56과 ResNet-110은 여전히 convex한 모양을 띤다. 이것으로, 얕은 네트워크에서는 skip connection의 효용이 두드러지지 않지만, 깊은 네트워크에서는 매우 중요한 역할을 한다고 볼 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;2-wide-model-vs-thin-model&quot;&gt;2. Wide model vs. Thin model&lt;/h3&gt;

&lt;p&gt;WRN의 k를 늘려가며 실험했는데, 더 wide한 네트워크가 더 flat했다. skip connection이 없는 모델에서도 더 넓은 모델이 더 flat한 지형을 갖는다. 또한, 더 wide한 모델의 테스트 정확도가 높았으므로, flatness와 일반화의 관계를 다시한번 확인할 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;3-네트워크-초기화&quot;&gt;3. 네트워크 초기화&lt;/h3&gt;

&lt;p&gt;convex에 가까운 ResNet 네트워크나 VGG 네트워크의 경우, 랜덤하게 초기화해도 잘 최적화될 가능성이 높지만, 깊은 ResNet with no skip connection 네트워크의 경우 랜덤 초기화는 gradient가 전혀 정보를 주지 않는 곳에 있을 확률이 높아 학습하기 어렵다.&lt;/p&gt;

&lt;h3 id=&quot;4-convexity&quot;&gt;4. Convexity&lt;/h3&gt;

&lt;p&gt;우리가 본 컨투어는 차원을 엄청나게 낮춘 것이기 때문에, non-convex처럼 보이면 실제 차원에서도 non-convex하지만, convex처럼 보인다고 해서 실제 차원에서 convex인 것은 아니고 양의 curvature들이 우세하다는 정도로만 보아야 한다. 이것을 보이기 위해 이전에 그린 그래프를 따라 &lt;script type=&quot;math/tex&quot;&gt;\|\frac{\lambda_{min}}{\lambda_{max}}\|&lt;/script&gt;를 그려보았는데, convex같아보이는 그래프에서는 이 값들이 작았고, 아닌 그래프에서는 non-convex로 보이는 부분들에 이 값이 크게 나타났다.&lt;/p&gt;
</description>
        <pubDate>Tue, 07 Jan 2020 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/thesis/2020/01/07/Visual.html</link>
        <guid isPermaLink="true">http://localhost:4000/thesis/2020/01/07/Visual.html</guid>
        
        <category>Optimization</category>
        
        
        <category>Thesis</category>
        
      </item>
    
      <item>
        <title>On the Variance of the Adaptive Learning Rate and Beyond</title>
        <description>&lt;h2 id=&quot;abstract&quot;&gt;[Abstract]&lt;/h2&gt;

&lt;p&gt;learning rate warmup은 학습을 안정화시키고, 수렴을 가속화하고 adaptive SGD의 일반화를 개선하는 데에 좋다. 이 논문에서는 그 매커니즘을 자세히 알아본다. adaptive lr은 초기 단계에 분산이 큰데, warmup이 분산 감소에 효과적이라는 이론을 제안하고 이를 검증한다. 또한 이 논문에서는 RAdam을 제시한다.&lt;/p&gt;

&lt;h2 id=&quot;1-introduction&quot;&gt;[1] Introduction&lt;/h2&gt;

&lt;p&gt;최근 adaptive lr이 빠른 수렴도때문에 많이 쓰이는데, 나쁜 local optima에 빠지지 않게 하고 학습을 안정화시키기 위해 warmup을 사용한다. 하지만 이 warmup에 대한 이론적인 토대는 충분하지 않기 때문에, 이를 스케줄링하는데 많은 노력이 들어간다.&lt;/p&gt;

&lt;p&gt;이 논문에서는 warmup을 이론적, 실험적으로 해석한다. 정확하게는, adaptive learning이 제한된 training sample로 학습되기 때문에 초기에 큰 분산을 가지게 된다는 것을 보여주려 한다. 따라서 초기에 작은 lr을 이용해 warmup을 하면 더 작은 분산을 갖게되며 학습이 잘 된다는 것을 보여준다. 더해서, Adam의 변형인 RAdam을 제안하는데, 이는 Adam에서 adaptive lr의 분산 항을 수정한 것이다.&lt;/p&gt;

&lt;h2 id=&quot;2-preliminaries-and-motivations&quot;&gt;[2] Preliminaries and Motivations&lt;/h2&gt;

&lt;h3 id=&quot;generic-adaptive-methods&quot;&gt;Generic adaptive methods&lt;/h3&gt;

&lt;p&gt;모든 알고리즘은 $\phi$와 $\psi$의 선택에 따라 달라진다.&lt;/p&gt;

&lt;h3 id=&quot;learning-rate-warmup&quot;&gt;Learning rate warmup&lt;/h3&gt;

&lt;p&gt;warmup이 왜 잘되는지 알기 위해 gradient의 절댓값을 log scale로 그려보았는데, warmup이 없을 때에는 큰 gradient가 많았지만 있는 경우에는 작은 gradient가 많았다. 이 말은 처음 몇 단계에서는 나쁜 local optima에 빠진다는 것을 의미한다.&lt;/p&gt;

&lt;h2 id=&quot;3-variance-of-adaptive-rate&quot;&gt;[3] Variance of Adaptive Rate&lt;/h2&gt;

&lt;p&gt;“학습 초기에는 샘플이 별로 없어서 adaptive learning rate의 분산이 커져서 나쁜 local optima로 향한다” 가 이논문이 주장하는 가설이다.&lt;/p&gt;

&lt;h3 id=&quot;31-warmup-as-variance-reduction&quot;&gt;[3.1] Warmup as Variance Reduction&lt;/h3&gt;

&lt;p&gt;Adam-2k와 Adam-eps라는 Adam의 변형들이 실험에 사용되었는데, 이 논문에서는 실험을 위해 IWSLT’14 German to English 데이터셋을 사용했다. Adam-2k에서는 초기 2k 반복동안에 adaptive learning rate $\psi$만 업데이트되고 momentum $\phi$와 파라미터 $\theta$는 고정시켰는데, 이 방법은 vanilla Adam의 수렴 문제를 해결했고 초기 단계에서의 샘플의 부족이 gradient의 분포를 왜곡시킨다는 점을 알 수 있다. &lt;em&gt;이게 왜 샘플의 개수 때문이지..?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;또한, 이 논문에서는 adaptive learning rate의 분산을 작게 만듦으로써 이 수렴 문제를 해결할 수 있다는 점도 증명했다. Adam-eps에서는 adaptive lr의 분산을 작게 하기 위해 $\hat{\psi}(.)$에서의 $\epsilon$을 크게 설정했다. 이것은 또한 vanilla Adam의 수렴 문제를 해결했지만, 결과가 좀 나빴다. 이 논문에서는 $\epsilon$을 크게 설정하는 것이 bias를 야기해서 최적화 과정을 느리게 만들기 때문이라고 추측한다.&lt;/p&gt;

&lt;h3 id=&quot;32-analysis-of-adaptive-learning-rate-variance&quot;&gt;[3.2] Analysis of Adaptive Learning Rate Variance&lt;/h3&gt;

&lt;p&gt;[Thm1] 만약 $\psi^2(.)$이 $\text{Scaled-inv-}\chi^2(\rho, \frac{1}{\sigma^2})$을 따른다면, $\rho$가 증가하면 $\text{Var}(\psi(.))$은 감소한다. 이 이론은 초기 단계의 샘플 부족으로 안해 $\text{Var}(\psi(.))$가 커진다는 것을 보여준다. &lt;em&gt;이것도..왜 샘플의 개수 때문이지?&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;4-rectified-adaptive-learning-rate&quot;&gt;[4] Rectified Adaptive Learning Rate&lt;/h2&gt;

&lt;h3 id=&quot;41-estimate-of-rho&quot;&gt;[4.1] Estimate of $\rho$&lt;/h3&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(\frac{(1-\beta_2)\sum_{i=1}^t\beta_2^{t-i}g_i^2}{1-\beta_2^t})\approx p(\frac{\sum_{i=1}^{f(t, \beta_2)}g_{t+1-i}}{f(t,\beta_2)})&lt;/script&gt;

&lt;p&gt;로 주로 근사된다. $f(t, \beta_2)$는 SMA의 length인데, 이는 SMA와 EMA가 같은 무게중심을 갖게 해야 한다. 따라서 $f(t,\beta_2)=\frac{2}{1-\beta_2}-1-\frac{2t\beta_2^t}{1-\beta_2^t}$가 되어야 한다.&lt;/p&gt;

&lt;p&gt;또한 우리는 $f(t,\beta_2)$로 $\rho$를 근사할 수 있으므로 $f(t,\beta_2)=\rho_t$라고 쓸 것이고, $\frac{2}{1-\beta_2}-1=\rho_{\infty}$라고 쓸 것이다.&lt;/p&gt;

&lt;h3 id=&quot;42-variance-estimation-and-rectification&quot;&gt;[4.2] Variance Estimation and Rectification&lt;/h3&gt;

&lt;p&gt;adaptive learning rate $\psi(.)$가 일관되는 분산을 가지게 하기 위해 이 논문에서는 rectification을 이용했다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{Var}[r_t\psi(g_1, \cdots, g_t)]=C_{\text{Var}}&lt;/script&gt;

&lt;p&gt;이때 $r_t=\sqrt{\frac{C_{\text{Var}}}{\text{Var}[\psi(g_1, \cdots, g_t)]}}$이고 $C_{\text{Var}}=\text{Var}[\psi(.)]|_  {\rho_t=\rho_{\infty}}$이다.&lt;/p&gt;

&lt;p&gt;$\text{Var}[\psi(.)]$는 수치적으로 stable하지 않으므로 rectified term을 1차 근사로 계산한다. 결론적으로 구한 $r_t$는&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;r_t=\sqrt{\frac{(\rho_t-4)(\rho_t-2)}{(\rho_\infty-4)(\rho_\infty-2)}}&lt;/script&gt;

&lt;p&gt;이다. 이것을 적용한 변형된 Adam을 RAdam이라고 한다.&lt;/p&gt;

&lt;h3 id=&quot;43-in-comparison-with-warmup&quot;&gt;[4.3] In Comparison with Warmup&lt;/h3&gt;

&lt;p&gt;$r_t$는 warmup과 비슷한 효과를 내며, warmup이 분산을 감소시키는 방식으로 작동된다는 것을 알았다. RAdam은 분산이 커질 때 adaptive lr을 비활성화시켜서 초기 불안정성을 막고, 하이퍼파라미터도 필요하지 않다.&lt;/p&gt;

&lt;h2 id=&quot;5-experiments&quot;&gt;[5] Experiments&lt;/h2&gt;

&lt;h3 id=&quot;51-comparing-to-vanilla-adam&quot;&gt;[5.1] Comparing to Vanilla Adam&lt;/h3&gt;

&lt;p&gt;adaptive learning rate은 초기에 큰 분산을 가지고 있다. 이를 보완하기 위해 이 논문에서는 RAdam을 만들었는데, 이는 Adam보다 결과가 좋을 뿐만 아니라 learning rate에도 민감하지 않다. (라는데 learning rate에 민감하지 않은지는 그래프를 봐도 뚜렷하게 나타나지는 않는다.)&lt;/p&gt;

&lt;p&gt;이 논문에서는 One billion word dataset (Language Modeling), CIFAR10, ImageNet (Image Classification)에 실험을 했는데, 이때의 결과로는 초기 분산 감소가 빠르고 정확도도 더 높은 결과를 가져오는데 좋은 역할을 했음을 알 수 있다. SGD와도 비교했는데, test accuracy는 SGD보다 좋지 않지만 train accuracy는 SGD보다 더 좋았다. (그런데 이말은 곧 overfitting이 더 된다는 말인 것 같다.)&lt;/p&gt;

&lt;p&gt;또, RAdam, vanilla Adam, Adam with warmup을 여러 범위의 lr로 비교한 결과, RAdam이 가장 민감하지 않았다고 하는데, 이 부분은 그래프에서 그리 뚜렷한 차이가 보이지는 않는다.&lt;/p&gt;

&lt;h3 id=&quot;52-comparing-to-heuristic-warmup&quot;&gt;[5.2] Comparing to Heuristic Warmup&lt;/h3&gt;

&lt;p&gt;IWSLT 데이터셋에 실험해본 결과, RAdam은 Adam에 warmup을 사용한 것과 비슷한 양상을 보인다. 또한 RAdam은 더 적은 하이퍼파라미터 튜닝을 요한다. 이 결과 또한 RAdam이 분산이 큰 상황을 없애기 때문이라는 주장에 보탬이 된다.&lt;/p&gt;

&lt;h3 id=&quot;53-simulated-verification&quot;&gt;[5.3] Simulated Verification&lt;/h3&gt;

&lt;p&gt;실험으로 1차 근사한 rectification term도 잘 근사되었다는 것을 확인했고, 그 결과 rectification 된 $\psi(.)$의 분산이 일정하다는 점도 확인했다.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;이 논문에서는 optimizer로 Adam을 사용한다.&lt;/p&gt;
</description>
        <pubDate>Fri, 27 Dec 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/thesis/2019/12/27/Adaptivelr.html</link>
        <guid isPermaLink="true">http://localhost:4000/thesis/2019/12/27/Adaptivelr.html</guid>
        
        <category>Optimization</category>
        
        
        <category>Thesis</category>
        
      </item>
    
      <item>
        <title>Deep Residual Learning for Image Recognition</title>
        <description>&lt;h2 id=&quot;abstract&quot;&gt;[Abstract]&lt;/h2&gt;

&lt;p&gt;residual learning 이라는 framework을 만들었는데, 이 방법은 layer input에 대한 reference가 된다. 이 방법을 이용해서 더 깊은 네트워크를 더 쉽고 정확하게 학습시킬 수 있다. 여기다 앙상블을 쓴 결과는 ImageNet에서 3.57%의 에러를 갖는다.&lt;/p&gt;

&lt;h2 id=&quot;1-introduction&quot;&gt;[1] Introduction&lt;/h2&gt;

&lt;p&gt;네트워크의 깊이가 적당히 깊으면 학습이 잘되는데, 계속 깊게하면 성능이 저하된다. 이는 일반화에 관한 것이 아니고 train error 또한 크다. 네트워크를 더 깊게 만드기 위해 shortcut connection이라는 것을 만들었다. 만약 identity mapping이 최적이라면 이 네트워크는 0으로 학습된다.&lt;/p&gt;

&lt;h2 id=&quot;3-deep-residual-learning&quot;&gt;[3] Deep Residual Learning&lt;/h2&gt;

&lt;h3 id=&quot;31-residual-learning&quot;&gt;[3.1] Residual Learning&lt;/h3&gt;

&lt;p&gt;원래의 learning되는 함수가 $\mathcal{H}(x)$라고 하면, shortcut connection을 이용해 학습되는 함수는 $\mathcal{F}(x)+x$가 된다. 이렇게 학습되면 더이상 학습이 필요하지 않은 경우는 identity mapping이 되기 때문에 더 얕은 네트워크보다 성능이 저하되지 않는다. 이렇게 하면 새로 네트워크를 학습시키는 것보다 identity를 참조해서 복잡한 부분을 더 쉽게 찾을 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;32-identity-mapping-by-shortcuts&quot;&gt;[3.2] Identity Mapping by Shortcuts&lt;/h3&gt;

&lt;p&gt;building block은 다음과 같은 모양이다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y = \mathcal{F}(x, \{W_i\})+x&lt;/script&gt;

&lt;p&gt;plain network와 residual network는 같은 수의 파라미터, 깊이, 넓이, 계산복잡도를 갖는다. 만약 input과 output이 같은 차원을 갖지 않으면 그를 맞추기 위해 다음과 같이 설정한다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y = \mathcal{F}(x, \{W_i\})+W_sx&lt;/script&gt;

&lt;p&gt;그러나 identity mapping이 성능 저하를 줄이는 데에 좋으므로, 이런 형태는 차원 맞추기에만 사용된다. $\mathcal{F}$는 주로 그 안에 여러 층을 갖는다. 만약 한 층만 갖게 되면 선형 모양이 되기 때문에 plain network와 비교했을 때 별다른 이득을 얻지 못한다.&lt;/p&gt;

&lt;h3 id=&quot;33-network-architectures&quot;&gt;[3.3] Network Architectures&lt;/h3&gt;

&lt;p&gt;이 논문에서의 baseline은 VGGnet에서 영감을 받았다.&lt;/p&gt;

&lt;h2 id=&quot;4-experiments&quot;&gt;[4] Experiments&lt;/h2&gt;

&lt;h3 id=&quot;41-imagenet-classification&quot;&gt;[4.1] ImageNet Classification&lt;/h3&gt;

&lt;p&gt;34layer net 안에 18layer net의 모든 경우의 수가 들어갈 수 있음에도 불구하고 34layer net의 train error가 더 작았다. 따라서 degradation 현상을 목격했다고 할 수 있다. 이 논문에서는 실험에 BN을 사용했기 때문에 gradient vanishing 현상에 의해 나타났다고 보기 어렵다. &lt;em&gt;이 논문에서는 deep plain net이 수렴도가 매우 낮기 때문이라고 생각한다.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;ResNet에서는 더 깊은 네트워크가 train error가 더 작았고, 일반화도 더 잘 되었다. 또한 ResNet이 더 수렴속도가 빨랐다. 세 가지 구조가 있는데, (A) 차원을 늘리기 위해 zero-padding shortcut을 사용하고, 모든 shortcut은 parameter-free (B) 차원을 늘리기 위해 projection shortcut을 사용하고 다른 shortcut은 identity (C) 모든 shortcut은 projection. 그러나 이 결과들이 아주 조금의 차이만 있기 때문에, 이 논문에서는 projection shortcut이 residual learning에서 필수적인 요소는 아니라고 결론짓는다.&lt;/p&gt;

&lt;p&gt;이 논문에서는 3층의 bottleneck 구조를 사용하는데, 이 구조는 2층의 구조와 시간복잡도는 비슷하고, convolution에서의 input과 output의 크기는 줄여준다. 만약 여기서 parameter-free가 아닌 projection으로 대체된다면, 시간복잡도가 배가 될 것이므로 identity shortcut이 더 효율적이다. 이 구조를 이용해 더 깊은 50layer, 101layer, 152layer를 만들어냈는데, 이들은 degradation 문제가 없으며, 더 깊을수록 결과가 더 좋았다.&lt;/p&gt;

&lt;h3 id=&quot;42-cifar-10-and-analysis&quot;&gt;[4.2] CIFAR-10 and Analysis&lt;/h3&gt;

&lt;p&gt;CIFAR-10에서는 warmup을 위해 초기 learning rate을 0.01로 두고 training error가 80%이하가 된 이후에 0.1로 높였고, 그 이후의 learning rate schedule은 이전과 같이 정했다. 이 논문에서는 layer마다 표준편차를 계산했는데, ResNet은 plain net보다 더 표준편차가 적은 것을 볼 수 있었다. 그리고 더 깊은 network일수록 표준편차가 더 작았다.&lt;/p&gt;

&lt;p&gt;그러나 1000layer가 넘는 network에서는 그보다 적은 layer의 network와 같은 train error를 가져도 test error가 더 높았다. &lt;em&gt;이 논문에서는 이가 overfitting 때문이라고 생각한다. 그러나 이 논문에서는 dropout같은 정규화 방법을 쓰지 않았기 때문에 이 방법들을 사용하면 결과가 더 나아질 것이라고 본다.&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;10-crop test : 이미지의 각 코너와 center crop을 하고, flip된 이미지도 마찬가지로 처리하면 10개이다.&lt;br /&gt;
Pytorch에 ResNet이 구현되어 있는데, 이는 ImageNet을 위한 구조이고, CIFAR-10을 위한 구조는 다르다.&lt;/p&gt;
</description>
        <pubDate>Mon, 23 Dec 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/thesis/2019/12/23/ResNet.html</link>
        <guid isPermaLink="true">http://localhost:4000/thesis/2019/12/23/ResNet.html</guid>
        
        <category>Optimization</category>
        
        
        <category>Thesis</category>
        
      </item>
    
      <item>
        <title>QSGD - Communication-Efficient SGD via Gradient Quantization and Encoding</title>
        <description>&lt;h2 id=&quot;abstract&quot;&gt;[Abstract]&lt;/h2&gt;

&lt;p&gt;SGD는 병렬처리를 할 수 있어 좋지만, 통신에 cost가 많이 들어간다. 이에 대응하기 위해 양자화된 gradient만 통신하는 방법이 제안되었는데, 이는 항상 수렴하지는 않는다. 이 논문에서 제안된 QSGD는 수렴을 보장하고 좋은 성능을 가진다. 이 방법을 이용하면 통신 대역과 수렴 시간에 대한 trade-off를 마음대로 할 수 있다. 각 노드들은 반복 당 몇 비트를 보낼지 조절할 수 있는데, 그러면 분산과의 trade-off가 있다. 이 방법은 딥러닝 학습에서의 시간을 줄여준다.&lt;/p&gt;

&lt;h2 id=&quot;1-introduction&quot;&gt;[1] Introduction&lt;/h2&gt;

&lt;p&gt;SGD에서의 gradient를 보내는 작업은 상당한 bottleneck이 되는데, 이를 줄이기 위한 precision을 줄이는 방법과 gradient의 부호만 보내는 방법이 있었는데, 이들은 특정 조건에서만 수렴했다.&lt;/p&gt;

&lt;h2 id=&quot;3-quantized-stochastic-gradient-descent-qsgd&quot;&gt;[3] Quantized Stochastic Gradient Descent (QSGD)&lt;/h2&gt;

&lt;h3 id=&quot;31-generalized-stochastic-quantization-and-coding&quot;&gt;[3.1] Generalized Stochastic Quantization and Coding&lt;/h3&gt;

&lt;h4 id=&quot;stochastic-quantization&quot;&gt;Stochastic Quantization&lt;/h4&gt;

&lt;p&gt;양자화 함수는 $Q_s(v)$로 나타내며, $s$는 $0$부터 $1$까지를 $s$개로 나눌 것이라는 파라미터이다. 모든 벡터 $v$에 대하여 $Q_s(v)$는 unbiasedness, variance bound, sparsity가 증명되었다.&lt;/p&gt;

&lt;h4 id=&quot;efficient-coding-of-gradients&quot;&gt;Efficient Coding of Gradients&lt;/h4&gt;

&lt;p&gt;$Q_s(v)$는 튜플 &lt;script type=&quot;math/tex&quot;&gt;(\|v\|_ 2, \sigma, \zeta)&lt;/script&gt;로 표현될 수 있다. 그리고 Elias coding을 사용하는데, $k$를 elias coding한다고 하면 우선 $k$를 이진수로 표현하고, length를 그 뒤에 붙인다. 그리고 바로 앞에 있는 것을 반복적으로 encoding하는데, 매우 효율적이다. 양자화 레벨 $s$로 양자화된 &lt;script type=&quot;math/tex&quot;&gt;(\|v\|_ 2, \sigma, \zeta)&lt;/script&gt;를 하나의 string $S$로 encoding한다면, 처음 32비트로 &lt;script type=&quot;math/tex&quot;&gt;\|v\|_ 2&lt;/script&gt;를 encode하고, $\sigma_i$, $\text{Elias}(\zeta_i)$를 반복적으로 붙인다. 이렇게 하면 $Q_s(\tilde{g}(x))$를 encoding하는 데에 필요한 비트수는 &lt;script type=&quot;math/tex&quot;&gt;(3+(\frac{3}{2}+o(1))\log(\frac{2(s^2+n)}{s(s+\sqrt{n})}))s(s+\sqrt{n})+32&lt;/script&gt;를 넘지 않는다. dense한 gradient에 대해서는 $s=\sqrt(n)$으로 encoding하는데, 이 때 필요한 비트수는 최대 $2.8n+32$이다.&lt;/p&gt;

&lt;h3 id=&quot;32-qsgd-guarantees&quot;&gt;[3.2] QSGD Guarantees&lt;/h3&gt;

&lt;p&gt;QSGD를 $K$개의 프로세서에서 실행한다고 하면, 한 회당 $2.8n+32$번의 통신만을 사용한다. 이것은 매우 효율적이고, non-convex SGD에도 사용할 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;33-quantized-variance-reduced-sgd&quot;&gt;[3.3] Quantized Variance-Reduced SGD&lt;/h3&gt;

&lt;p&gt;SVRG를 업데이트할 때 QSGD를 사용해도 같은 수렴 한계를 얻을 수 있다. 적절한 수를 선택하면, 각 epoch당 $O(\kappa(\frac{\log 1}{\epsilon}+n))$의 통신만 사용하면 된다.&lt;/p&gt;

&lt;h2 id=&quot;5-experiments&quot;&gt;[5] Experiments&lt;/h2&gt;

&lt;h4 id=&quot;communication-vs-computation&quot;&gt;Communication vs. Computation&lt;/h4&gt;

&lt;p&gt;먼저, 네트워크들을 communication-intensive(AlexNet, VGG, LSTM)과 computation-intensive(Inception, ResNet)로 나누었는데, 두 부류 다 통신을 줄이자 시간에서의 이득이 보였다.&lt;/p&gt;

&lt;h4 id=&quot;accuracy&quot;&gt;Accuracy&lt;/h4&gt;

&lt;p&gt;4bit나 8bit QSGD는 충분히 높은 정확도를 보인다. 최근 발표된 논문에 따르면 gradient에 noise를 추가하는 것이 도움이 되는데, 이 논문의 방법도 zero-mean noise의 일종일 수 있다. 그러나 엄청나게 비트수를 줄이는 것은 정확도에 해가 될 수 있다. 한가지 이슈는, 특정 layer는 양자화에 특히 민감할 수 있다는 것이다. convolution layer의 비트수를 많이 줄이면(2bit QSGD) 정확도 손실을 가져온다.&lt;/p&gt;
</description>
        <pubDate>Tue, 03 Dec 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/thesis/2019/12/03/QSGD.html</link>
        <guid isPermaLink="true">http://localhost:4000/thesis/2019/12/03/QSGD.html</guid>
        
        <category>Optimization</category>
        
        
        <category>Thesis</category>
        
      </item>
    
      <item>
        <title>A Closer Look at Deep Learning Heuristics - Learning Rate Restarts, Warmup and Distillation</title>
        <description>&lt;h2 id=&quot;abstract&quot;&gt;[Abstract]&lt;/h2&gt;

&lt;p&gt;현재의 딥러닝 모델들은 heuristic들로부터 좋은 결과를 냈다. 현재 이 경험적인 방법들은 linear interpolation이나 차원 감소를 통한 시각화들로 분석되는데, 이들은 각자의 단점을 가지고 있다. 이 논문에서는 mode connectivity, Canonical correlation analysis(CCA)를 가지고 knowledge distillation과 cosine restart, warmup을 재분석한다. 분석으로 얻은 결과는 다음과 같다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;cosine annealing이 잘 작동되는 이유로 설명된 것들이 실험적으로 아님을 보였다.&lt;/li&gt;
  &lt;li&gt;warmup은 깊은 층의 가중치 변화를 하지 않게 만들며, 그냥 그 가중치들을 고정시켜도 warmup과 같은 효과를 얻는다.&lt;/li&gt;
  &lt;li&gt;knowledge distillation에서 teacher가 공유한 latent knowledge는 더 깊은 층으로 분배된다.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;1-introduction&quot;&gt;[1] Introduction&lt;/h2&gt;

&lt;p&gt;knowledge distillation은 teacher model을 먼저 학습하고, 더 작은 데이터셋을 가지고 student model을 학습하는데, loss function을 사용하는 것이 아니고 teacher를 따라하는 방식으로 학습한다.&lt;/p&gt;

&lt;p&gt;Mode connectivity(MC)를 이용하면 네트워크의 local minima들을 piecewise-linear curve로 연결할 수 있는데, 이는 서로 다른 방법으로 얻은 local minima와 특성들이 연결되어 있음을 알 수 있다.&lt;/p&gt;

&lt;p&gt;CCA를 전처리 단계에서 사용하여 네트워크의 활성화를 분석할 수 있다는 연구도 있었다.&lt;/p&gt;

&lt;h2 id=&quot;2-empirical-tools&quot;&gt;[2] Empirical Tools&lt;/h2&gt;

&lt;h3 id=&quot;21-mode-connectivity&quot;&gt;[2.1] Mode Connectivity&lt;/h3&gt;

&lt;p&gt;local minima들을 연결하면, 그들을 연결한 사이에도 비교적 작은 loss의 점들이 있다.&lt;/p&gt;

&lt;h4 id=&quot;211-resilience-of-mode-connectivity&quot;&gt;[2.1.1] Resilience of Mode Connectivity&lt;/h4&gt;

&lt;p&gt;보통은 서로 다른 훈련 방법(batch size가 다르거나, optimizer가 다르거나, 초기화가 다르거나, 정규화 텀이 다르거나)을 사용하면 서로 다른 점으로 수렴한다고 알고 있다. 이 논문에서는 기준 mode $G$를포함해, 서로 다른 훈련 방법을 사용하여 ${A, B, C, D, E, F, G}$의 mode를 얻었다. 그리고 mode connectivity 알고리즘을 사용하여 $A$부터 $F$까지를 $G$와 짝지어 각각의 curve를 만들어 내었다. 그리고 그 curve의 점마다의 validation accuracy를 봤는데, 마지막 epoch에서는 다들 비슷한 정확도를 보였다. 따라서 결국 각 minima를 잇는 선들도 정확도가 비슷하다.&lt;/p&gt;

&lt;h3 id=&quot;22-cca-for-measuring-representational-similarity&quot;&gt;[2.2] CCA for Measuring Representational Similarity&lt;/h3&gt;

&lt;p&gt;CCA는 다변량 통계에서의 두 RV 집합간의 관계를 연구하는 전통적인 툴이다. 이전 연구에서는 CCA를 SVD나 DFT와 함께 이용해서 두 layer를 비교하는 방법이 제안되었다. 각 layer의 activation에서 나오는 값을 행렬로 만들고, 그들과 SVD를 이용해 layer간의 correlation을 계산한다. convolution layer인 경우에는 DFT를 사용한다. 이 방법은 서로 다른 네트워크를 비교할 뿐만 아니라 한 네트워크가 훈련 중 어떻게 변화하는지를 보고싶을 때 사용하기도 한다.&lt;/p&gt;

&lt;h2 id=&quot;3-stochastic-gradient-descent-with-restarts-sgdr&quot;&gt;[3] Stochastic Gradient Descent with Restarts (SGDR)&lt;/h2&gt;

&lt;p&gt;cosine annealing이 잘 되는 이유는 잘 모르는데, multi-modal 함수에서는 restart가 다른 지역을 더 탐색할 수 있게 해준다는 것이 하나의 설명이다. 실험 결과로는, restart가 일어나기 직전 minima들을 line segment로 연결한 점들은 두 점보다 높은 loss를 보였지만, learning rate를 줄인 minima들은 line segment로 연결해도 loss가 증가하지 않았다. 결과를 보니, 사실 SGDR이 local minima를 탈출하게 해준다는 결과는 보이지 않았다. 그리고 training loss와 validation loss가 달라서, train loss 입장에서는 loss가 높아져도, validation loss의 입장에서는 낮아져서 일반화가 잘 될 때도 있다.&lt;/p&gt;

&lt;h2 id=&quot;4-warmup-learning-rate-scheme&quot;&gt;[4] Warmup Learning rate Scheme&lt;/h2&gt;

&lt;p&gt;이론적으로, SGD의 learning dynamics는 batch size와 learning rate의 비율에 의존한다. warmup은 학습 불안정성을 야기하지 않고 큰 learning rate를 사용할 수 있는 방법으로 제안되었다. 이 논문에서 연구하고자 한 것은 “어떻게 learning rate warmup이 서로 다른 layer에 영향을 미칠까?”이다. 이것을 연구하기 위해 CCA를 사용하였고, 세 가지의 scheme을 사용한 네트워크들의 차이점과 유사점을 조사하였다. LB+warmup과 SB에서 유사점이 나타났는데, 이것은 큰 batch size와 큰 learning rate을 사용할 때, warmup이 불안정한 큰 변화를 없애준다는 것을 시사한다. 이것을 더 알아보기 위해 warmup을 사용하지 않고 대신 처음 20 epoch동안 fully connected layer들을 고정시켜 봤는데, 또한 유사도가 높았고 validation 정확도가 warmup을 사용한 것보다 더 높았다.&lt;/p&gt;

&lt;h2 id=&quot;5-knowledge-distillation&quot;&gt;[5] Knowledge Distillation&lt;/h2&gt;

&lt;p&gt;knowledge distillation 또한 CCA를 사용하여 연구하였다. 이전에는 dark knowledge의 전이가, 최근에는 중요도의 해석이 knowledge distillation을 잘 되게 한다고 설명되어 왔다. 이 논문에서는 knowledge transfer가 네트워크의 특정 부분에 국한되는지, student and teacher 모델의 layer사이의 유사도와 student가 이 질문에 대한 해답을 줄 수 있는지에 대해 조사하였다. student model을 학습하고, 같은 구조를 가진 모델을 hard label을 가지고 학습하였다. 그리고 그들의 layer간의 유사도를 teacher model과 비교하였다. 결과는, 깊은 층에 비해 얕은 층에 대해서는 base model과 teacher간의 유사도와 student와 teacher간의 유사도가 비슷했다. 따라서 깊은 층에 대해서만 knowledge transfer가 일어난다고 할 수 있다. &lt;em&gt;얕은 층에서도 knowledge transfer가 일어나지만, 그게 알아내기 쉬운 knowledge라서 transfer가 되지 않은 것처럼 보이는 것은 아닐까?&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;6-discussion-and-conclusion&quot;&gt;[6] Discussion and Conclusion&lt;/h2&gt;

&lt;p&gt;이 연구는 learning heuristic을 설명했고, 또한 우연히 새로운 heuristic을 발견하였다.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;처음부터 끝까지 실험, 실험, 실험…. 논문은 이렇게 쓰는거구나&lt;/p&gt;
</description>
        <pubDate>Sun, 01 Dec 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/thesis/2019/12/01/HEUR.html</link>
        <guid isPermaLink="true">http://localhost:4000/thesis/2019/12/01/HEUR.html</guid>
        
        <category>Optimization</category>
        
        
        <category>Thesis</category>
        
      </item>
    
      <item>
        <title>Don't Decay the Learning Rate, Increase the Batch Size</title>
        <description>&lt;h2 id=&quot;abstract&quot;&gt;[Abstract]&lt;/h2&gt;

&lt;p&gt;이 논문에서는 learning rate를 줄이는 것 대신 batch size를 훈련 동안 증가시키면서 원래와 비슷한 learning curve를 얻는 방법에 대해 소개한다. 이 방법은 SGD, SGD with momentum, Nesterov momentum, and Adam. 같은 epoch 수를 사용하면 같은 테스트 정확도가 나오지만, 파라미터 업데이트를 덜 하면 더 병렬화가 가능하고 학습 시간이 덜 걸린다. 따라서 learning rate $\epsilon$을 크게 하고, 그에 비례하게 batch size $B$도 늘일 수 있다. 결론적으로 momentum coefficient $m$을 늘리고 $B\propto\frac{1}{1-m}$로 batch size를 증가시킬 수 있다. 그러나 이것은 약간의 테스트 정확도 손실을 초래한다. 이 논문의 방법은 30분 안에 ImageNet을 ResNet-50을 이용해 76.1%의 검증 정확도가 나오게 학습했다.&lt;/p&gt;

&lt;h2 id=&quot;1-introduction&quot;&gt;[1] Introduction&lt;/h2&gt;

&lt;h2 id=&quot;2-stochastic-gradient-descent-and-convex-optimization&quot;&gt;[2] Stochastic Gradient Descent and Convex Optimization&lt;/h2&gt;

&lt;p&gt;SGD에는 noise가 들어가기 때문에 $\frac{dw}{dt}=\frac{dC}{dw}+\eta(t)$로 모델링된다. $\eta{(t)}$가 Gaussian random noise이다. noise scale $g=\epsilon(\frac{N}{B}-1)$로 정의되는데, 이는 학습동안의 noise를 제어한다. learning rate $\epsilon$을 줄이면 이 noise scale을 줄일 수 있는데, 줄이지 않고 $B$를 늘려도 같은 효과를 얻을 수 있다. 이 논문의 방법은 $B$를 $\frac{N}{10}$까지 증가시킨 이후에 $\epsilon$을 줄이는 방법을 사용한다.&lt;/p&gt;

&lt;h2 id=&quot;3-simulated-annealing-and-the-generalization-gap&quot;&gt;[3] Simulated Annealing and the Generalization GAP&lt;/h2&gt;

&lt;p&gt;simulated annealing은 처음 noise는 넓은 범위를 탐색할 수 있게 해주고, 좋은 지점에 다다르면 더 깊숙히 들어갈 수 있게 한다. 아마 이것이 SGD의 learning rate decay가 잘 되는 이유일 것이다.&lt;/p&gt;

&lt;h2 id=&quot;4-the-effective-learning-rate-and-the-accumulation-variable&quot;&gt;[4] The Effective Learning rate and the Accumulation Variable&lt;/h2&gt;

&lt;p&gt;최근 연구자들은 그냥 SGD대신 momentum을 사용한 SGD를 많이 사용한다. momentum 계수 $m$이 $0$으로 가면 noise scale이 작아진다. 한 연구에서 제안된 learning rate를 늘리고 동시에 $B\propto \frac{1}{1-m}$를 늘이는 것은 효과가 있었지만, momentum coefficient를 늘리는 것은 테스트 정확도의 손실을 가져왔다. 사실 더 큰 $m$에서 학습하려면 epoch를 더 추가해야 한다. 또한 $m$이 커지면 이전 gradient를 잊어버리는 시간이 더 늘어나는데, 이것이 noise scale이 줄어드는 데에 문제를 가져올 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;5-experiments&quot;&gt;[5] Experiments&lt;/h2&gt;

&lt;h3 id=&quot;51-simulated-annealing-in-a-wide-resnet&quot;&gt;[5.1] Simulated Annealing in a Wide ResNet&lt;/h3&gt;

&lt;p&gt;CIFAR-10과 “16-4” wide ResNet을 사용하였다. learning rate를 줄이는 것과 batch size를 늘리는 방법이 같다는 것을 보이기 위해, learning rate을 줄이는 방법, batch size를 늘리는 방법, hybrid 방법을 비교하였다. 결과는 세 learning curve가 거의 똑같았다. 따라서 learning rate 자체가 줄어들어야 하는 대상이 아니라, noise scale이 줄어들어야 한다는 것을 알았다. 게다가, batch size를 늘리는 방법이 파라미터 업데이트를 훨씬 덜 하고 같은 정확도를 얻을 수 있었다. 또한 다른 optimizer에도 실험하였는데 같은 결과를 얻을 수 있었다.&lt;/p&gt;

&lt;h3 id=&quot;52-increasing-the-effective-learning-rate&quot;&gt;[5.2] Increasing the Effective Learning rate&lt;/h3&gt;

&lt;p&gt;이 논문의 두번째 목표는 파라미터 업데이트를 줄이는 것이다. momentum coefficient $m$을 크게 하는 방법이 파라미터 업데이트 수가 가장 적었고, 초반 learning rate를 크게 하는 방법이 그 다음으로 파라미터 업데이트 수가 적었다. &lt;em&gt;하지만 [4]에서 $m$을 키우면 epoch가 더 추가되어야 한다고 했는데, 그럼 시간은 더 걸리는게 아닌가?&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;53-training-imagenet-in-2500-parameter-updates&quot;&gt;[5.3] Training ImageNet in 2500 Parameter Updates&lt;/h3&gt;

&lt;p&gt;이 실험은 Training ImageNet in 1 Hour에서의 세팅을 따라했지만, warm-up phase는 적용하지 않았다. 또한 ResNet 대신 더 강력한 Inception-ResNet-V2를 사용했다. 이전 논문에서 이미 learning rate를 maximum까지 높였기 때문에 이 논문에서는 파라미터 업데이트를 줄이기 위해 $m$을 크게 했다. 결과는 예상대로 더 큰 $m$을 사용할 때의 파라미터 업데이트가 더 줄었다.&lt;/p&gt;

&lt;h3 id=&quot;54-training-imagenet-in-30-minutes&quot;&gt;[5.4] Training ImageNet in 30 Minutes&lt;/h3&gt;

&lt;p&gt;후반 epoch로 갈수록 파라미터 업데이트가 적기 때문에 시간이 더 적게 걸렸다. 초반 30 epoch와 후반 60 epoch가 각각 15분 이내로 걸렸다.&lt;/p&gt;

&lt;h2 id=&quot;7-conclusions&quot;&gt;[7] Conclusions&lt;/h2&gt;

&lt;p&gt;learning rate를 줄이는 방식과 같은 효과를 batch size를 늘리면서도 낼 수 있다.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;실험을 굉장히 많이 하고 체계적으로 잘 설계해서 한 논문인듯. 이 논문의 related works 부분에 지금까지 읽었던 모든 논문들이 다 등장.. 새로운 것을 만들어 내는 것 뿐만 아니라 원래 있던 것을 개선하는 것, 더 효율적인 방향으로 가게 하는 것도 논문이 될 수 있다.&lt;/p&gt;
</description>
        <pubDate>Thu, 28 Nov 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/thesis/2019/11/28/INCSIZE.html</link>
        <guid isPermaLink="true">http://localhost:4000/thesis/2019/11/28/INCSIZE.html</guid>
        
        <category>Optimization</category>
        
        
        <category>Thesis</category>
        
      </item>
    
      <item>
        <title>Accurate, Large Minibatch SGD - Training ImageNet in 1 Hour</title>
        <description>&lt;h2 id=&quot;abstract&quot;&gt;[Abstract]&lt;/h2&gt;

&lt;p&gt;딥러닝에서 네트워크와 데이터셋이 커지면 학습시간이 늘어나게 되어서 분산병렬 SGD를 사용한다. 이 논문에서는 미니배치 크기가 크면 최적화하는 데에 어려움을 야기하지만, 이렇게 훈련된 네트워크는 일반화를 잘 한다는 것을 보여준다. 또한 한번에 미니배치 사이즈를 8192개까지 늘려도 정확도에는 변함이 없다는 것을 보여준다. 이 결과를 얻기 위해 learning rate를 정하는 규칙을 적용했고, 새로운 warmup 방법을 만들었다.&lt;/p&gt;

&lt;h2 id=&quot;2-large-minibatch-sgd&quot;&gt;[2] Large Minibatch SGD&lt;/h2&gt;

&lt;h3 id=&quot;21-learning-rates-for-large-minibatches&quot;&gt;[2.1] Learning Rates for Large Minibatches&lt;/h3&gt;

&lt;p&gt;이 논문의 목적은 정확도는 유지하면서 큰 미니배치 사이즈를 사용하는 것이다. 이들은 Linear Scaling Rule이라고 불리는, 미니배치 크기가 $k$배 커지면 learning rate도 $k$배 크게 해야 한다는 규칙을 발견했다. 작은 미니배치를 사용했을 때의 $w_{t+k}$와, $k$배의 미니배치를 사용했을 때의 $\hat{w}_{t+1}$이 비슷해지려면 $\hat{\eta}=k\eta$가 되어야 한다. 이렇게 학습시켰을 때, 둘의 정확도와 학습곡선이 비슷하다. 그러나 학습 초기에는 비슷하지 않을 수 있고, 미니배치 사이즈는 계속해서 커질 수 없다는 성질이 있다. 이 논문은 전례 없는 미니배치 사이즈를 가지고 실험적으로 Discussion에 있는 이론들을 테스트해본다.&lt;/p&gt;

&lt;h3 id=&quot;22-warmup&quot;&gt;[2.2] Warmup&lt;/h3&gt;

&lt;p&gt;constant warmup은 학습 초기 epoch에서 constant learning rate를 사용한다. 그러나 이 논문에서는 충분하지 않아서 gradual warmup을 사용했는데, 훈련 초반에 잘 수렴하도록 해준다.&lt;/p&gt;

&lt;h3 id=&quot;23-batch-normalization-with-large-minibatches&quot;&gt;[2.3] Batch Normalization with Large Minibatches&lt;/h3&gt;

&lt;p&gt;BN을 사용하면 원래 집합 $X$안에 있는 크기가 $n$인 모든 부분집합들을 모두 포함하고 있는 것처럼 생각할 수 있다. 분산 컴퓨팅 환경에서 BN을 사용한다면 per-worker sample size가 $n$이고 total minibatch size가 $kn$일 때, $X^n$에서 골라낸 독립된 $k$개의 $\cal{B}_ j$로 볼 수 있다. &lt;em&gt;이게 결국 각 minibatch의 분산을 비슷하게 만들어준다는 수업때 하려던 내용인듯&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;3-subtleties-and-pitfalls-of-distributed-sgd&quot;&gt;[3] Subtleties and Pitfalls of Distributed SGD&lt;/h2&gt;

&lt;h3 id=&quot;weight-decay&quot;&gt;Weight decay&lt;/h3&gt;

&lt;p&gt;L2 regularization때문에 weight decay를 사용하는데, 이 항이 원래의 loss에 더해진다. 따라서 원래 loss를 scaling 하는 것과 learning rate를 scaling하는 것은 다르다.&lt;/p&gt;

&lt;h3 id=&quot;momentum-correction&quot;&gt;Momentum correction&lt;/h3&gt;

&lt;p&gt;momentum을 사용한 SGD가 최근 많이 이용되고 있는데, learning rate가 바뀌면 momentum correction term을 적용해야 한다. &lt;em&gt;이게 왜 문제점인지 잘 모르겠음&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;gradient-aggregation&quot;&gt;Gradient aggregation&lt;/h3&gt;

&lt;p&gt;전체 loss를 계산할 때 평균을 사용하는데, 각 worker의 loss를 $n$으로 나누면 다시 한번 전체를 $k$로 나누어야 하기 때문에, 그냥 처음부터 각 worker의 loss를 $kn$으로 나눠서 최종적으로는 다 더하기만 한다.&lt;/p&gt;

&lt;h3 id=&quot;data-shuffling&quot;&gt;Data shuffling&lt;/h3&gt;

&lt;p&gt;한 epoch당 데이터 전체를 $k$ worker에게 분배하는데, 모든 데이터들은 반드시 한번씩만 사용되도록 분배한다.&lt;/p&gt;

&lt;h2 id=&quot;4-communication&quot;&gt;[4] Communication&lt;/h2&gt;

&lt;p&gt;gradient aggregation은 backprop과 병렬로 처리되어야 한다. 한 layer의 gradient가 계산되면, 모든 worker에게서 정보를 받아서 합치고, 그와 동시에 다음 layer의 gradient가 계산될 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;41-gradient-aggregation&quot;&gt;[4.1] Gradient Aggregation&lt;/h3&gt;

&lt;p&gt;gradient aggregation에는 allreduce 연산이 사용된다. 이 논문에서의 구현은 세 부분으로 되어 있다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;8개의 GPU에 있는 버퍼들을 각 서버에서 하나로 합친다.&lt;/li&gt;
  &lt;li&gt;결과 버퍼가 공유되고 합쳐진다.&lt;/li&gt;
  &lt;li&gt;결과가 각 GPU로 퍼진다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;1번과 3번은 NCCL을 이용하였다. 2번에 대해서는 recursive halving and doubling 알고리즘과 bucket 알고리즘을 구현했는데, 큰 사이즈의 요소에 대해서는 recursive halving and doubling 알고리즘이 더 좋았다.&lt;/p&gt;

&lt;p&gt;having/doubling 알고리즘은 reduce scatter collective와 allgather 단계로 구성된다. reduce scatter에서는 서버가 2개씩 짝을 지어, 1번 서버의 경우 자신의 버퍼의 뒤쪽 반을 보내고 2번 서버의 앞쪽 반을 받는다. 따라서 2의 승수개의 서버가 필요하다. 수신 및 전송하는 데이터가 reduction 되는 동안 목적지까지의 거리는 2배가 된다. 이 단계 이후에는 각 서버에는 최종 reduction된 데이터의 일부가 있다. allgather 단계에서는 통신을 역추적해서 각 서버에 있는 벡터들을 모은다. 각 서버에서 처음에 보내는 부분이 allgather에서 수신되고 있고, 그 이후에는 받았던 부분을 보낸다.&lt;/p&gt;

&lt;h3 id=&quot;42-software&quot;&gt;[4.2] Software&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;이부분이 사실 메인인 것 같은데.. 이해가 안감ㅠㅠ&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;43-hardware&quot;&gt;[4.3] Hardware&lt;/h3&gt;

&lt;p&gt;이 논문에서는 50Gbit 네트워크가 ResNet-50을 학습시키는 데에 충분하다고 한다. 게다가 이 최대 bandwidth는 backprop에만 사용되고, forward pass에는 그만큼 쓰이지 않는다.&lt;/p&gt;

&lt;h2 id=&quot;5-main-results-and-analysis&quot;&gt;[5] Main Results and Analysis&lt;/h2&gt;

&lt;p&gt;이 논문의 메인 결과는 ImageNet에 ResNet-50과 256worker를 이용해 정확도 손실 없이 한시간만에 학습시켰다는 것이다. Linear scaling rule과 warmup 전략을 사용해서 이가 가능했다.&lt;/p&gt;

&lt;h3 id=&quot;52-optimization-or-generalization-issues&quot;&gt;[5.2] Optimization or Generalization Issues?&lt;/h3&gt;

&lt;p&gt;이 논문에서는 no warmup, constant warmup, gradual warmup을 실험했는데, training error와 validation error 면에서 모두 gradual warmup이 가장 성능이 좋았다. 또한 baseline의 learning curve도 초반 빼고는 잘 따라갔다.&lt;/p&gt;

&lt;h3 id=&quot;53-analysis-experiments&quot;&gt;[5.3] Analysis Experiments&lt;/h3&gt;

&lt;p&gt;minibatch size가 8k보다 작거나 같을 때에는 learning curve가 거의 비슷했는데, 그 이상에서는 비슷하지도 않았다. 여기서 알 수 있는 것은, learning curve를 비교해 보면 training이 끝나기 전에 결과가 잘 나올지, 아닐지 알 수 있다는 점이다.&lt;/p&gt;

&lt;p&gt;minibatch size가 작았을 때는 $\eta=0.1$언저리에서는 비슷한 성능이 나왔지만, size가 커질수록 $\eta$를 더 섬세하게 바꿔야 한다.&lt;/p&gt;

&lt;p&gt;BN에서의 일반적인 초기화는 $\gamma=1$인데, 이 논문에서는 각 residual block에 대해서만 $\gamma=0$으로 초기화했더니 더 좋은 결과가 나왔다.&lt;/p&gt;

&lt;p&gt;ImageNet-5k dataset에 대해서도 실험해봤는데, 정확도가 작은 minibatch size와 정확도가 비슷한 minibatch size는 8k까지로, 데이터셋의 크기와 minibatch size의 관계는 없는 것으로 관찰되었다.&lt;/p&gt;

&lt;h3 id=&quot;54-generalization-to-detection-and-segmentation&quot;&gt;[5.4] Generalization to Detection and Segmentation&lt;/h3&gt;

&lt;p&gt;ImageNet을 학습시킨 ResNet-50을 R-CNN의 초기화에 이용해보았는데, $8k$까지는 성능이 나빠지지 않았다. linear scaling rule과 large batchsize는 다른 모델에서도 잘 적용된다는 것을 보여준다.&lt;/p&gt;

&lt;h3 id=&quot;55-run-time&quot;&gt;[5.5] Run Time&lt;/h3&gt;

&lt;p&gt;batch size를 늘리면 GPU간의 소통에도 불구하고 iteration 당 시간은 비슷하지만, epoch당 걸리는 시간이 선형적으로 줄어든다. 또한 GPU가 k배 늘어나면 초당 처리되는 이미지 수도 k배 늘어나는 것이 이상적인데, 이 논문의 방법은 GPU간의 소통에도 불구하고 이상의 $90%$정도를 달성했다.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;allreduce operation&lt;/p&gt;
</description>
        <pubDate>Tue, 26 Nov 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/thesis/2019/11/26/IMG1HOUR.html</link>
        <guid isPermaLink="true">http://localhost:4000/thesis/2019/11/26/IMG1HOUR.html</guid>
        
        <category>Optimization</category>
        
        
        <category>Thesis</category>
        
      </item>
    
      <item>
        <title>SAGA - A Fast Incremental Gradient Method With Support for Non-Strongly Convex Composite Objectives</title>
        <description>&lt;hr /&gt;

&lt;h2 id=&quot;abstract&quot;&gt;[Abstract]&lt;/h2&gt;

&lt;p&gt;SAGA는 SAG와 SVRG의 뒤를 잇는데, 더 좋은 수렴도를 갖는다. SDCA와는 다르게 strongly convex가 아닌 문제도 바로 풀 수 있고, 문제의 본질적인 strong convexity에 적응할 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;1-introduction&quot;&gt;[1] Introduction&lt;/h2&gt;

&lt;p&gt;함수 $f(x)$를 최소화하고 싶은데, $f(x)$는&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(x)=\frac{1}{n}\sum_{i=1}^nf_i(x)&lt;/script&gt;

&lt;p&gt;이렇게 생겼다. 각 $f_i$는 convex하고 gradient가 $L$-Lipschitz continuous하다. 이 논문에서는 $f_i$들이 $\mu$-strongly convex한 경우와 $F(x)=f(x)+h(x)$인 경우(proximal gradient descent에서 봤던 모양)도 다룰 것이다.&lt;/p&gt;

&lt;h2 id=&quot;2-saga-algorithm&quot;&gt;[2] SAGA Algorithm&lt;/h2&gt;

&lt;p&gt;SAGA 알고리즘은 다음과 같다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;$j$를 랜덤으로 뽑는다.&lt;/li&gt;
  &lt;li&gt;$\phi_j^{k+1}=x^k$라고 하고, $f’_ j(\phi_j^{k+1})$를 테이블에 저장한다. 즉, 모든 $f$중 하나의 $f_j$의 gradient만을 구한다.&lt;/li&gt;
  &lt;li&gt;$x$를 $f’_ j(\phi_j^{k+1}), f’_ j(\phi_j^k)$과 테이블에 있는 평균을 이용해 업데이트한다. &lt;script type=&quot;math/tex&quot;&gt;w^{k+1}=x^k-\gamma\left[f'_ j(\phi_j^{k+1})-f'_ j(\phi_j^k)+\frac{1}{n}\sum_{i=1}^nf'_ i(\phi_i^k)\right]&lt;/script&gt;이고, &lt;script type=&quot;math/tex&quot;&gt;x^{k+1}=\text{prox}_\gamma^h(w^{k+1})&lt;/script&gt;이다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;strongly convex일 때와 아닐 때의 수렴도는 각각 증명되어 있다. strongly convex가 아닌 경우에 $\gamma=\frac{1}{3L}$를 사용할 경우, SAGA는 자동적으로 strong convexity $\mu\gt 0$에 적응한다. strongly convex가 아닌 문제에서는 정규화 항($\lambda w^Tw$)을 통해 incremental gradient method들이 적용될 수 있는데, SAGA에서는 $\lambda$의 조정을 피할 수 있다. &lt;em&gt;아마 $h$함수로 뺄 수 있다는 뜻인듯?&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;3-related-work&quot;&gt;[3] Related Work&lt;/h2&gt;

&lt;h3 id=&quot;saga--midpoint-between-sag-and-svrgs2gd&quot;&gt;SAGA : midpoint between SAG and SVRG/S2GD&lt;/h3&gt;

&lt;p&gt;SVRG에서는 SGD의 분산을 관찰해서 step size가 0으로 수렴해야만 전체도 수렴한다는 것을 알고, 상수 step size를 사용하기 위해 SGD에 분산 감소 접근법을 사용하여 선형 수렴도를 얻었다. SVRG 논문에서는 SAG도 분산 감소를 하지만, 어떻게 그런 맥락이 나왔는지는 설명하지 않았는데 이 논문에서 그 연결고리를 설명할 것이다.&lt;/p&gt;

&lt;p&gt;$\mathbb{E}(X)=\theta_\alpha=\alpha(X-Y)+\mathbb{E}(Y)$으로 추정하고 $\theta_\alpha$의 분산을 줄이고 싶을 때, $\text{Var}(\theta_\alpha)=\alpha^2\left[\text{Var}(X)+\text{Var}(Y)-2\text{Cov}(X,Y)\right]$ 이므로 $X, Y$가 서로 높은 상관관계가 있다면 $X$의 분산에 비해 $\theta_\alpha$의 분산이 더 작다. $\alpha$가 $0$부터 $1$까지 증가할 때 분산은 증가하지만 bias는 감소한다.&lt;/p&gt;

&lt;p&gt;$X$가 현재 선택된 gradient $f’_ j(x^k)$이고, $Y$가 과거에 저장된 gradient $f’_ j(\phi_j^k)$라고 하면, SAG는 $\alpha=\frac{1}{n}$이고, SAGA는 $\alpha=1$이다. SAGA는 bias가 없기 때문에 proximal operator를 사용할 수 있다. SVRG는 $Y=f’_ i(\tilde(x))$이다. SAG는 이전의 모든 gradient를 저장해야 하고, SVRG는 반복이 시작될 때 연산이 크다는 단점이 있다. SVRG는 안쪽 루프의 반복수 $m$을 파라미터로 지정해 줘야 한다.&lt;/p&gt;

&lt;h3 id=&quot;finitomisomu&quot;&gt;Finito/MISO$\mu$&lt;/h3&gt;

&lt;p&gt;SAGA에서 $u^0=x^0+\gamma\sum_{i=1}^nf’_ i(x^0)$으로 놓으면 SAGA 알고리즘을 $u$에 대해 업데이트하는 것으로 바꿀 수 있다. Finito와 MISO$\mu$는 $x^{k+1}=\frac{1}{n}\sum_i\phi_i^k-\gamma\sum_{i=1}^nf’_ i(\phi_i^k)$로 업데이트 한다. step length는 $\gamma$이며, step size는 $\frac{1}{\mu n}$이다.&lt;/p&gt;

&lt;p&gt;Finito에서의 $\bar{\phi}$에 대한 식은, $\bar{\phi}$를 $u$로 바꾸면 SAGA의 식과 같다. SAGA는 Finito와 MISO$\mu$과 비교했을 때 strongly convex를 요구하지 않고, 따라서 proximal operator를 사용할 수 있다. 또한 강력한 $\phi_i$값을 요구하지 않는다. Finito/MISO$\mu$는 $f_i$가 연산량이 많을 때 유용하게 쓰일 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;sdca&quot;&gt;SDCA&lt;/h3&gt;

&lt;p&gt;SDCA는 원래 $f_i$의 convex conjugate를 사용하는데, 여기서는 SAGA와의 연결고리를 설명하기 위해 primal값만을 사용하는 방법을 소개한다. 이 방법은 MISO$\mu$방법과도 비슷하다. 이 방법은 dual variable을 사용하는 방법과 결과가 똑같다. &lt;em&gt;아마도 이런 말.. 이부분 내용 이해는 하지 못했음&lt;/em&gt; 이 방법은 각각의 $f_i$가 그냥 convex이기만 할 때, strongly convex한 정규화 텀으로 인해 전체 $f$의 strongly convex함이 유도된다. 그러나 이 정규화 텀을 각 $f_i$에 고르게 넣으면 각 $f_i$가 strongly convex 하도록 바꿀 수 있으며, 그렇게 나온 방법은 Finito와 SDCA의 중간에 있다.&lt;/p&gt;

&lt;h2 id=&quot;5-theory&quot;&gt;[5] Theory&lt;/h2&gt;

&lt;h3 id=&quot;thm-1&quot;&gt;[Thm 1]&lt;/h3&gt;

&lt;p&gt;$x^* $가 optimal solution이고,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
T^k=T(x^k, \{\phi_i^k\}_{i=1}^n)=\frac{1}{n}\sum_if_i(\phi_i^k)-f(x^* )-\frac{1}{n}\sum_i\left&lt;f'_ i(x^* ), \phi_i^k-x^* \right&gt;+c\|x^k-x^* \|^2 %]]&gt;&lt;/script&gt;

&lt;p&gt;($T$는 Lyapunov function) 으로 정의하자. $\gamma=\frac{1}{2(\mu n+L)}, c=\frac{1}{2\gamma(1-\gamma\mu)n}, \kappa=\frac{1}{\gamma\mu}$라고 하면,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbb{E}[T^{k+1}]=(1-\frac{1}{\kappa})T^k&lt;/script&gt;

&lt;p&gt;를 얻을 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;cor-1&quot;&gt;[Cor 1]&lt;/h3&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;c\|x^k-x^* \|^2\le T^k&lt;/script&gt;이므로, $\mu(n-0.5)\le\mu n$을 사용하면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\mathbb{E}\left[\|x^k-x^* \|^2\right]\le\left(\frac{\mu}{2(\mu n+L)}\right)^k\left[\|x^0-x^* \|^2+\frac{n}{\mu n+L}\left[f(x^0)-\left&lt;f'(x^* ), x^0-x^* \right&gt;-f(x^* )\right]\right] %]]&gt;&lt;/script&gt;

&lt;p&gt;를 얻을 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;6-experiments&quot;&gt;[6] Experiments&lt;/h2&gt;

&lt;p&gt;MNIST, COVTYPE, IJCNN1, MILLIONSONG 데이터셋에 실험을 했는데, Finito가 성능은 가장 좋지만 expensive하다. SVRG는 epoch 단위에서는 빠르게 수렴하지만, epoch당 gradient evaluation이 다른 방법에 비해 2배이기 때문에, 가장 좋다고 말할 수 없다. SAGA는 non-permuted Finito와 SDCA와 성능이 비슷하다. 결론은, 수렴도 보다는 각 문제에 대한 성질과 잘 맞는 optimizer를 사용해야 한다.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;incremental gradient methods는 SGD와 같은 것임&lt;br /&gt;
&lt;em&gt;step lenght는 뭐지?&lt;/em&gt;&lt;br /&gt;
&lt;em&gt;proximal operator?&lt;/em&gt;&lt;/p&gt;
</description>
        <pubDate>Sat, 23 Nov 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/thesis/2019/11/23/SAGA.html</link>
        <guid isPermaLink="true">http://localhost:4000/thesis/2019/11/23/SAGA.html</guid>
        
        <category>Optimization</category>
        
        
        <category>Thesis</category>
        
      </item>
    
  </channel>
</rss>
