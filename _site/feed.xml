<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>YeonjeeJung's Blog</title>
    <description></description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Fri, 05 Jul 2019 02:08:18 +0900</pubDate>
    <lastBuildDate>Fri, 05 Jul 2019 02:08:18 +0900</lastBuildDate>
    <generator>Jekyll v3.8.5</generator>
    
      <item>
        <title>CV Lecture 2 - Image Formation and Camera</title>
        <description>&lt;p&gt;제발 돼라&lt;/p&gt;
</description>
        <pubDate>Thu, 04 Jul 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/lecture/computervision/2019/07/04/Lecture2.html</link>
        <guid isPermaLink="true">http://localhost:4000/lecture/computervision/2019/07/04/Lecture2.html</guid>
        
        <category>ComputerVision</category>
        
        
        <category>Lecture</category>
        
        <category>ComputerVision</category>
        
      </item>
    
      <item>
        <title>Strand-accurate Multi-view Hair Capture</title>
        <description>&lt;hr /&gt;
&lt;p&gt;이 논문은 가닥 수준에서 정확한 결과를 내는 3D 머리카락 검출 알고리즘을 제안하는 논문이다.&lt;/p&gt;
</description>
        <pubDate>Wed, 03 Jul 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/thesis/2019/07/03/HairCapture.html</link>
        <guid isPermaLink="true">http://localhost:4000/thesis/2019/07/03/HairCapture.html</guid>
        
        <category>MultiViewStereo</category>
        
        
        <category>Thesis</category>
        
      </item>
    
      <item>
        <title>Deep Video Super-Resolution Network Using Dynamic Upsampling Filters Without Explicit Motion Compensation</title>
        <description>&lt;hr /&gt;
&lt;p&gt;이 논문은 Video Super-Resolution을 위한 end-to-end 구조를 제안하는 논문이다.&lt;/p&gt;

&lt;h2 id=&quot;video-super-resolution-vsr&quot;&gt;Video Super Resolution (VSR)&lt;/h2&gt;
&lt;p&gt;VSR을 하는 직접적인 방법은 single-image super-resolution(SISR)을 프레임마다 하는 것인데, SISR은 프레임간의 관계를 고려하지 않기 때문에, 깜빡거리는 결과가 나올 수 있다. 기존의 VSR은 여러 low-resolution(LR) 프레임들을 인풋으로 받고, 연속된 LR 프레임들의 움직임을 고려하여 high-resolution(HR) 프레임들을 내놓는다. 딥러닝 기반의 VSR은 보통 모션 예측과 보정 과정으로 이루어진다. 문제점은 모션 예측에 의존성이 높고, CNN을 이용하기 때문에 블러된 아웃풋이 나온다는 점이다.&lt;/p&gt;

&lt;p&gt;이 논문에서는 새로운 end-to-end 뉴럴넷을 제안한다. 모션 정보가 dynamic upsampling filter를 만드는 데에 쓰인다. 만들어진 dynamic upsampling filter와 LR의 center frame을 가지고 HR frame이 만들어진다. 이 연구는 state-of-the-art인 VSRnet보다 더 날카로운 결과물을 만들 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;dynamic-upsampling-filters&quot;&gt;Dynamic Upsampling Filters&lt;/h2&gt;
&lt;p&gt;기존의 bilinear나 bicubic upsampling등의 방법들은 직접적인 모션 보정으로, 날카롭게 보정하기 힘들다. 이와 반대로 이 논문에서는 dynamic filter network를 사용한다. 이 filter들은 지역적으로 생성된다. dynamic filter들은 LR에서 주변 픽셀을 관찰하여 만들어지는데, 직접적인 모션 보정을 피할 수 있게 해준다.&lt;/p&gt;

&lt;h2 id=&quot;residual-learning&quot;&gt;Residual Learning&lt;/h2&gt;
&lt;p&gt;디테일들이 linear filtering으로는 보정이 되지 않을 수 있는데, 이 문제를 해결하기 위해 고주파수 디테일을 얻기 위해 residual image를 추가적으로 예측한다. 이 residual image는 여러 input 프레임(앞뒤 몇개의 프레임)을 통해 얻어진다. Dynamic Upsampling을 한 이미지가 이 residual image와 합쳐지면 더 날카롭고 앞뒤 문맥에 적합한 프레임이 얻어진다.&lt;/p&gt;

&lt;h2 id=&quot;network-design&quot;&gt;Network Design&lt;/h2&gt;
&lt;p&gt;filter와 residual generation network는 weight를 공유함으로써 오버헤드를 줄여준다. 공유된 부분의 네트워크 구조(3D layer)는 dense block에서 착안되었다. 각 input 프레임들은 공유된 2D convolutional layer들로 프로세싱되고 시간적 순서로 합쳐진다. 이것이 시공간 feature map인데, 이 3D dense block으로 들어가고, 각 가지들로 나눠져 처리되어 2개의 output(dynamic filter, residual image)를 만든다. 이후 LR 프레임이 dynamic filter와 convolution되어 upsampling된 후 residual image와 합쳐지면 HR 프레임이 얻어진다.&lt;/p&gt;

&lt;h2 id=&quot;temporal-augmentation&quot;&gt;Temporal Augmentation&lt;/h2&gt;
&lt;p&gt;training data를 만들기 위해서는 정방향, 역방향, 프레임 건너뛰기 등을 이용할 수 있다.&lt;/p&gt;

&lt;hr /&gt;
</description>
        <pubDate>Mon, 01 Jul 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/thesis/2019/07/01/VSR.html</link>
        <guid isPermaLink="true">http://localhost:4000/thesis/2019/07/01/VSR.html</guid>
        
        <category>VideoSuperResolution</category>
        
        
        <category>Thesis</category>
        
      </item>
    
      <item>
        <title>Weakly-Supervised Semantic Segmentation by Iteratively Mining Common Object Features</title>
        <description>&lt;hr /&gt;
&lt;p&gt;이 논문은 이미지에서 tag만 주어진 상황에서 그 tag의 대상을 segmentation하는 반복적 방법을 제안한 논문이다.&lt;/p&gt;

&lt;h2 id=&quot;fully-supervised-semantic-segmentation&quot;&gt;Fully-Supervised Semantic Segmentation&lt;/h2&gt;
&lt;p&gt;Fully-Supervised Semantic Segmentation에는 region-based, pixel-based의 두가지 방법이 있다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Region-based : 이미지를 set of region으로 가져와서 label을 예측하기 위해 특징을 추출한다.&lt;/li&gt;
  &lt;li&gt;Pixel-based : 이미지 전체를 input으로 가져와서 pixel-wise label을 예측한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;보통은 pixel-based가 더 강력하지만, 이 논문에서는 둘의 장점을 모두 가져와서 사용할 것이다. region-based가 common feature를 추출하는 데서는 유리하다는 것을 보일 것이다.&lt;/p&gt;

&lt;h2 id=&quot;weakly-supervised-semantic-segmentation-under-image-tags-supervision&quot;&gt;Weakly-Supervised Semantic Segmentation under Image Tags Supervision&lt;/h2&gt;
&lt;p&gt;번역하면 ‘이미지 태그가 있으면 문법적으로 세그맨테이션 해주는 약한 감독학습’ 정도가 될 수 있다. 사람과 배경이 함께 있는 이미지에서 사람만을 세그멘테이션해주는 것이라고 생각하면 된다. Fully-Supervised Semantic Segmentation이 너무 많은 양의 labeling cost를 필요로 하는데, 여기서는 tag만 labeling하면 되므로 더 적은 cost가 필요해서 사용한다. 그러나 이것은 어려운 문제인데, tag라는 high-level semantic부터 픽셀 단위라는 low-level appearance까지 연관이 있기 때문이다. 현재까지의 연구는 두 가지로 나눌 수 있다.&lt;/p&gt;
&lt;h4 id=&quot;multi-instance-learning-mil-based-method&quot;&gt;Multi-Instance Learning (MIL) based method&lt;/h4&gt;
&lt;p&gt;classification network로 바로 segmentation mask 예측하는 방법이다. 모든 픽셀들은 하나의 클래스에 들어가 있다고 가정하고, 그 클래스를 예측하는 것이 목적이다. 하지만 경계 부분에 대해서는 좋지 않은 성능을 보인다.&lt;/p&gt;
&lt;h4 id=&quot;localization-based-method&quot;&gt;Localization-based method&lt;/h4&gt;
&lt;p&gt;classification network를 초기 localization에 사용하고, 그리고 그걸 segmentation network를 감독하는데 사용한다. weak label로 initial object localization을 만드는 것이 목적이다. 그러나 정확하지 않은 weak label을 계속 사용하기 때문에 에러가 계속해서 축적되어, 이것도 경계 부분에 대해서 좋지 않은 성능을 보인다.&lt;/p&gt;

&lt;h2 id=&quot;initial-localization&quot;&gt;Initial Localization&lt;/h2&gt;
&lt;p&gt;반복적인 학습을 위해서는 시작이 있어야 한다. 이 논문에서는 initial localization을 얻기 위해서 classification network를 학습하고 각 object의 heatmap을 얻기 위해 ‘Classification Activation Map(CAM)’을 사용한다. 그러나 얻어진 heatmap이 매우 rough하기 때문에, 이미지를 superpixel region으로 segmentation 하고, heatmap을 평균한다. heatmap중 threshold를 넘는 부분을 initial seed로 사용한다.&lt;/p&gt;

&lt;h2 id=&quot;mining-common-object-feature-mcof&quot;&gt;Mining Common Object Feature (MCOF)&lt;/h2&gt;
&lt;p&gt;제안된 이 방법은 ‘하나의 동일한 대상에는 공통된 특징이 있을 것이다’라는 생각으로부터 시작되었다. 따라서 큰 train set을 가지고 학습하면, 대상에 대한 특징을 학습하여 object의 region을 넒혀갈 수 있을 것이다. 논문에서 제안하는 network는 bottom-up, top-down의 두 가지의 큰 구조로 이루어져 있다.&lt;/p&gt;
&lt;h4 id=&quot;bottom-up-regionnet&quot;&gt;Bottom-Up (RegionNet)&lt;/h4&gt;
&lt;p&gt;원래의 object localization을 object seed로 이용해서 common object feature (COF) 를 찾아낸다. 순서는 다음과 같다.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;train image를 superpixel region으로 segmentation 한다. (graph-based segmentation method 사용)&lt;/li&gt;
  &lt;li&gt;Superpixel region과 initial seed를 가지고 region classification network를 학습한다. (mask-based Fast R-CNN 이용)&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;saliency-guided-refinement-method&quot;&gt;Saliency-guided refinement method&lt;/h4&gt;
&lt;p&gt;이 때 찾아낸 COF는 object의 key part만 포함하고 있으므로 경계 부분이 정확하지 않다. 이를 보완하기 위해 saliency-guided refinement method를 사용한다. 이 방법은 mined object region과, Bayesian framework를 이용한 saliency map을 같이 고려한다. 높은 saliency value를  가지고 있는 region에 대해, 만약 이 대상이 mined된 object와 유사하다면, 이 region은 그 대상에 속할 확률이 높다고 판단한다. 이 방법은 맨 처음 반복에서만 사용된다.&lt;/p&gt;
&lt;h4 id=&quot;top-down-pixelnet&quot;&gt;Top-Down (PixelNet)&lt;/h4&gt;
&lt;p&gt;bottom-up에서 얻은 object region을 이용해서 segmentation network를 학습한다. 이 network를 학습한 후 input을 집어넣으면 더 정확한 object mask를 얻을 수 있다.&lt;/p&gt;

&lt;p&gt;이 두 과정을 계속 반복한다. 이렇게 반복적으로 수행하면, 계속해서 더 정확한 segment를 얻을 수 있다.&lt;/p&gt;
</description>
        <pubDate>Sun, 30 Jun 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/thesis/2019/06/30/COF.html</link>
        <guid isPermaLink="true">http://localhost:4000/thesis/2019/06/30/COF.html</guid>
        
        <category>SemanticSegmentation</category>
        
        
        <category>Thesis</category>
        
      </item>
    
      <item>
        <title>Zero-shot Learning on Semantic Class Prototype Graph</title>
        <description>&lt;hr /&gt;
&lt;p&gt;이 논문은 Zero-Shot Learning(ZSL)에서의 새로운 distance metric을 제안하는 논문이다.&lt;/p&gt;

&lt;h2 id=&quot;zero-shot-learningzsl&quot;&gt;Zero-Shot Learning(ZSL)&lt;/h2&gt;
&lt;p&gt;ZSL에서는 seen class와 unseen class, 그리고 image feature가 semantic embedding space에 사영된다. 새로운 input은 seen class중 한 class에 해당하고, 어떤 image feature를 가지고 있다고 하자. 그 사영된 공간에서 seen class와 image feature가 결합된 벡터가 unseen class와 가까우면, unseen class에 대한 학습 없이도 해당 class로 분류가 가능하다. 마치 어린아이가 ‘말’을 알고 ‘줄무늬’를 알면 ‘얼룩말’을 한번도 본적이 없더라도 판별해낼 수 있는 것과 같다.&lt;/p&gt;

&lt;h2 id=&quot;semantic-embedding-space&quot;&gt;Semantic Embedding Space&lt;/h2&gt;
&lt;p&gt;Seen class와 unseen class는 고차원 공간상에서 연관을 갖고있다. 이를 Semantic embedding space라고 부른다.&lt;/p&gt;
&lt;h4 id=&quot;semantic-embedding-se&quot;&gt;Semantic Embedding (SE)&lt;/h4&gt;
&lt;p&gt;거의 대부분의 ZSL은 SE를 사용한다. SE에서는 seen class에 대해서만 mapping을 학습한다. 이 mapping함수는 나중에 unseen class image가 space로 mapping될때도 사용된다.&lt;/p&gt;
&lt;h4 id=&quot;semantic-relatedness-sr&quot;&gt;Semantic Relatedness (SR)&lt;/h4&gt;
&lt;p&gt;거의 사용되지는 않지만 SR이라고 불리는 것도 있다. 먼저 seen class에 대해서 n-way classifier 를 학습하는데, 이 classifier는 unseen과 seen의 visual 유사도를 계산하는데 쓰인다. 그 후 semantic 유사도와 visual 유사도를 계산하는데, 이 두 유사도 벡터가 가까우면 unseen class로 예측한다.&lt;/p&gt;

&lt;h2 id=&quot;euclideancosine-distance-쓰면-생기는-문제점&quot;&gt;Euclidean/Cosine Distance 쓰면 생기는 문제점&lt;/h2&gt;
&lt;p&gt;기존 ZSL에서는 거리 계산 척도로 euclidean distance나 cosine distance를 사용했는데, 고차원 공간에서는 이런 척도가 hubness나 domain shift등의 문제를 야기할 수 있다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Hubness : 많은 벡터들에 동시에 가까운 중심 벡터가 있을 수 있다. 보통 high dimension에서 발생한다. unseen 클래스 수가 적어서 ZSL에서도 발생 가능성이 있다.&lt;/li&gt;
  &lt;li&gt;Domain shift :
    &lt;ul&gt;
      &lt;li&gt;(SE에서) visual feature embedding이 seen 클래스로부터 학습되어서, 사영된 unseen class point가 seen class 쪽으로 bias될 수 있다. 따라서 hub의 출현이 쉽다.&lt;/li&gt;
      &lt;li&gt;(SR에서) visual-semantic domain shift.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;semantic-manifold-distance&quot;&gt;Semantic Manifold Distance&lt;/h2&gt;
&lt;p&gt;일반적으로 semantic embedding space에서 label들은 semantic manifold 구조를 갖고 있는데, 이 manifold 구조를 더 잘 설명하는 distance이다. 여기에는 hubness와 domain shift 문제가 없다.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;여기까지가 서론이고, 이후 논문의 제안에 대한 내용이 있으나 제대로 이해를 못함…&lt;/p&gt;
</description>
        <pubDate>Sat, 29 Jun 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/thesis/2019/06/29/ZSL.html</link>
        <guid isPermaLink="true">http://localhost:4000/thesis/2019/06/29/ZSL.html</guid>
        
        <category>ZeroShotLearning</category>
        
        
        <category>Thesis</category>
        
      </item>
    
  </channel>
</rss>
