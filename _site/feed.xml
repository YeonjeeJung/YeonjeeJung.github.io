<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>YeonjeeJung's Blog</title>
    <description></description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Tue, 09 Jul 2019 21:40:36 +0900</pubDate>
    <lastBuildDate>Tue, 09 Jul 2019 21:40:36 +0900</lastBuildDate>
    <generator>Jekyll v3.8.5</generator>
    
      <item>
        <title>CV Lecture 4 - Camera Calibration</title>
        <description>&lt;h2 id=&quot;camera-calibration&quot;&gt;Camera Calibration&lt;/h2&gt;

&lt;h4 id=&quot;stereo-reconstruction&quot;&gt;Stereo Reconstruction&lt;/h4&gt;
&lt;p&gt;Stereo Reconstruction은 두 장 이상의 2D 이미지를 이용해서 이를 3D상에서 재건하는 것인데, 단계는 다음과 같다.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;카메라 변수 찾기&lt;/li&gt;
  &lt;li&gt;이미지 바로잡기&lt;/li&gt;
  &lt;li&gt;차이 계산&lt;/li&gt;
  &lt;li&gt;깊이 예측&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;가장 간단한 케이스는 평행한 두 지점에서 찍힌 이미지를 이용해 재건하는 것이다. 이미지 $I’$가 이미지 $I$보다 $x$축으로 $T$만큼 이동했다면, $R = I$(identity)이고 $t=(T, 0, 0)$이 된다. 따라서&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
E=t\times R=\left[\begin{matrix}0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; -T \\ 0 &amp; T &amp; 0\end{matrix}\right] %]]&gt;&lt;/script&gt;

&lt;p&gt;이 된다.&lt;/p&gt;

&lt;h4 id=&quot;stereo-image-rectification&quot;&gt;Stereo Image Rectification&lt;/h4&gt;
&lt;p&gt;두 카메라가 평행하면 한 좌표를 보는 두 이미지 평면이 평행하지 않을 수 있는데, 이렇게 얻은 이미지 평면을 두 카메라를 잇는 선에 평행한 평면에 옮겨주는 것을 image rectification이라고 한다. 이것을 하고 나면 위에서 쓴 essential 행렬을 사용할 수 있다.&lt;/p&gt;

&lt;h4 id=&quot;homography&quot;&gt;Homography&lt;/h4&gt;
&lt;p&gt;하나의 대상을 서로 다른 두 방향에서 바라본 두 개의 이미지를 서로 변환할 수 있는 행렬을 homography ($H$)라고 한다.&lt;/p&gt;

&lt;p&gt;만약 두 시점 변환에 translation이 없다면, rotation만이 존재하게 되고, $\Pi_r = R\Pi_l$라고 쓸 수 있다. ($\Pi_r$은 오른쪽, $\Pi_l$은 왼쪽 시점의 projection 행렬) $\Pi_l = K[I|0]$이므로&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x_l = K[I\|0]\left[\begin{matrix} X\\ 1 \end{matrix}\right] = KX&lt;/script&gt;

&lt;p&gt;이고, $\Pi_r = K[R|0]$$이므로&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x_r = K[R\|0]\left[\begin{matrix}X\\ 1\end{matrix}\right] = KRX&lt;/script&gt;

&lt;p&gt;이다. 따라서, $x_r = KRK^{-1}x_l$이고, $H = KRK^{-1}$이 된다.&lt;/p&gt;

&lt;p&gt;이미지의 원점이 같은 평면의 다른 각도에서 촬영되었을 때만 homography로 연관될 수 있다. (translation이 없어야 한다.) 그리고, 같은 카메라로 촬영되었을 때만 연관될 수 있다.&lt;/p&gt;

&lt;p&gt;만약 rotation 행렬 $R$을 모른다면, $x_r^TFx_l = 0$에서 $F$를 알아낼 때 썼던 방법을 이용해 찾을 수 있다. &lt;strong&gt;&lt;em&gt;사실, $K^{-T}K^{-1}$의 가장 작은 eigenvalue에 대응하는 eigenvector를 찾으면 된다.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;h4 id=&quot;2d-checkerboard-pattern&quot;&gt;2D Checkerboard Pattern&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;http://www.campi3d.com/External/MariExtensionPack/userGuide4R1/lib/CHeckerSimpleB.png&quot; alt=&quot;&quot; /&gt;
&lt;a href=&quot;http://www.campi3d.com/External/MariExtensionPack/userGuide4R1/lib/CHeckerSimpleB.png&quot;&gt;Source&lt;/a&gt;
2D 체스판을 생각해 보자. 체스판의 모든 좌표는 하나의 평면에 있다. 따라서, 모든 점의 $z$좌표는 모두 $0$이다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\left(\begin{matrix}u\\v\\1\end{matrix}\right)=K
\left[\begin{matrix}
r_{11} &amp; r_{12} &amp; r_{13} &amp; t_1 \\
r_{21} &amp; r_{22} &amp; r_{23} &amp; t_2 \\
r_{31} &amp; r_{32} &amp; r_{33} &amp; t_3 \\
\end{matrix}\right]\left(\begin{matrix}x\\y\\z\\1\end{matrix}\right)=K
\left[\begin{matrix}
r_{11} &amp; r_{12} &amp; r_{13} &amp; t_1 \\
r_{21} &amp; r_{22} &amp; r_{23} &amp; t_2 \\
r_{31} &amp; r_{32} &amp; r_{33} &amp; t_3 \\
\end{matrix}\right]\left(\begin{matrix}x\\y\\0\\1\end{matrix}\right) %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\therefore \left(\begin{matrix}u\\v\\1\end{matrix}\right)=K
\left[\begin{matrix}
r_{11} &amp; r_{12} &amp; t_1 \\
r_{21} &amp; r_{22} &amp; t_2 \\
r_{31} &amp; r_{32} &amp; t_3 \\
\end{matrix}\right]\left(\begin{matrix}x\\y\\1\end{matrix}\right)=H
\left(\begin{matrix}x\\y\\1\end{matrix}\right) %]]&gt;&lt;/script&gt;

&lt;p&gt;($(u, v, 1)$은 image plane위의 점이고, $(x, y, 1)$은 체스판 위의 점이다.)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
H=K
\left[\begin{matrix}
r_{11} &amp; r_{12} &amp; t_1 \\
r_{21} &amp; r_{22} &amp; t_2 \\
r_{31} &amp; r_{32} &amp; t_3 \\
\end{matrix}\right]=
\left[\begin{matrix}
h_{11} &amp; h_{12} &amp; h_{13} \\
h_{21} &amp; h_{22} &amp; h_{23} \\
h_{31} &amp; h_{32} &amp; h_{33} \\
\end{matrix}\right]=
\left[\begin{matrix}
h_{11}' &amp; h_{12}' &amp; h_{13}' \\
h_{21}' &amp; h_{22}' &amp; h_{23}' \\
h_{31}' &amp; h_{32}' &amp; 1 \\
\end{matrix}\right] %]]&gt;&lt;/script&gt;

&lt;p&gt;$H$는 변수가 8개이므로, 8개의 식을 알면 풀 수 있다. 각 점들은 2개의 관계식을 주기 때문에, 우리는 4개의 점만 알면 $H$를 알아낼 수 있다.&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;terms&quot;&gt;Terms&lt;/h2&gt;
&lt;p&gt;rectify : 바로잡기&lt;br /&gt;
disparity : 차이, 이격&lt;br /&gt;
anaglyph : 애너글리프. 인간의 두 눈의 시차를 이용하여 3D영상을 보는 것처럼 하는 것.
tripod : 삼각대&lt;/p&gt;
</description>
        <pubDate>Mon, 08 Jul 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/lecture/computervision/2019/07/08/Lecture4.html</link>
        <guid isPermaLink="true">http://localhost:4000/lecture/computervision/2019/07/08/Lecture4.html</guid>
        
        
        <category>Lecture</category>
        
        <category>ComputerVision</category>
        
      </item>
    
      <item>
        <title>CV Lecture 3 - Epipolar Geometry</title>
        <description>&lt;h2 id=&quot;single-view-geometry&quot;&gt;Single-View Geometry&lt;/h2&gt;
&lt;p&gt;일단 우리의 목표는 한 3D 구조를 재건하는 것이다. 그러나 하나의 이미지는 깊이 정보를 갖고 있지 않기 때문에 정확하게 재건할 수 없다. 따라서 우리는 multi-view geometry가 필요하다.&lt;/p&gt;

&lt;h4 id=&quot;principal-point-offset&quot;&gt;Principal Point Offset&lt;/h4&gt;
&lt;p&gt;principal axis는 카메라 중심에서부터 이미지 평면에 수직으로 들어오는 선이다. principal point란 principal axis가 이미지 평면과 만나는 점이다. 따라서 normalized coordinate system 상의 원점은 principal point이다. 그런데 image coordinate system상의 원점은 좌측 하단이다. 따라서 image coordinate system에서 얻은 좌표를 normalized coordinate system에서의 좌표로 변환해야 한다. (normalized coordinate system에서의 좌표에는 음수도 존재한다.)&lt;/p&gt;

&lt;p&gt;principal point의 좌표가 $(p_x, p_y)$라고 하면 normalized coordinate system에서의 좌표는 $(x, y, z) \rightarrow (f\frac{x}{z}, f\frac{y}{z})$가 되고, 이를 image coordinate system의 좌표로 나타내면 $(f\frac{x}{z}+p_x, f\frac{y}{z}+p_y)$가 된다. 이를 하나의 식으로 나타내면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\left[\begin{matrix} fx+zp_x \\ fy+zp_y \\ z \end{matrix}\right]=
\left[\begin{matrix}
f &amp; 0 &amp; p_x\\
0 &amp; f &amp; p_y\\
0 &amp; 0 &amp; 1
\end{matrix} \right]
\left[\begin{matrix}
1 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 1 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 1 &amp; 0
\end{matrix} \right]
\left[\begin{matrix} x \\ y \\ z \\ 1 \end{matrix}\right] %]]&gt;&lt;/script&gt;

&lt;p&gt;인데, 첫번째 행렬은 intrinsic 행렬이고, 두번째 행렬은 projection 행렬이다. 이 때 이 intrinsic 행렬을 calibration 행렬이라고 하고, $K$로 표기한다. projection 행렬은 $[I|0]$로 표현될 수 있다. 따라서 이 때 곱해지는 행렬 $\Pi$는 $K[I|0]$가 된다.&lt;/p&gt;

&lt;p&gt;실제 이미지에서는 이렇게 바뀐 좌표를 pixel coordinate로 바꿔야 한다. 가로에는 $1$미터 안에 $m_x$개의 픽셀, 세로에는 $1$미터 안에 $m_y$개의 픽셀이 있다고 하자. 원래의 $K$행렬은 각 좌표를 미터 단위로 변환시켜주는데, 이 앞에 $1$미터 안에 들어있는 픽셀수를 곱해주면 픽셀 단위로 좌표를 변환시킬 수 있다. 따라서 pixel coordinate로 변환시켜줄 새로운 $K$는&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
K=\left[\begin{matrix}
m_x &amp; 0 &amp; 0 \\
0 &amp; m_y &amp; 0 \\
0 &amp; 0 &amp; 1\\
\end{matrix}\right]
\left[\begin{matrix}
f &amp; 0 &amp; p_x \\
0 &amp; f &amp; p_y \\
0 &amp; 0 &amp; 1
\end{matrix}\right]=
\left[\begin{matrix}
\alpha_x &amp; 0 &amp; \beta_x \\
0 &amp; \alpha_y &amp; \beta_y \\
0 &amp; 0 &amp; 1
\end{matrix}\right] %]]&gt;&lt;/script&gt;

&lt;p&gt;가 된다.&lt;/p&gt;

&lt;h4 id=&quot;camera-rotation-and-translation&quot;&gt;Camera Rotation and Translation&lt;/h4&gt;
&lt;p&gt;카메라의 extrinsic 파라미터에 해당한다. 일반적으로 카메라를 땅과 평행하게 두고 촬영하지 않기 때문에 projection에 앞서 실제 좌표를 카메라 좌표로 옮기는 일도 필요하다. 여기에는 두 가지 과정이 있는데, 중심을 $(0, 0)$으로 맞추는 과정과 회전변환을 하는 과정이다. 이를 식으로 나타내면 $\tilde{X}_{cam} = R(\tilde{X}-\tilde{C})$ 가 된다. 하지만 이것은 $\tilde{X} = (x, y, z)$라고 가정했을 때의 식이고, 이는 non-homogeneous이므로, homogeneous한 식으로 바꾸면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
X_{cam}=
\left[\begin{matrix}
R &amp; -R\tilde{C}\\ 0 &amp; 1
\end{matrix}\right]X %]]&gt;&lt;/script&gt;

&lt;p&gt;라고 쓸 수 있다. (한 차원을 추가해서 그 차원을 상수 $1$로 고정, $X=\left(\begin{matrix} \tilde{X} \ 1\end{matrix}\right)$)&lt;/p&gt;

&lt;p&gt;최종적으로 $x = K[I|0]X_{cam} = K[R|-R\tilde{C}]X$가 되고, projection 행렬 $\Pi$는 $\Pi = K[R|-R\tilde{C}]$가 된다.&lt;/p&gt;

&lt;h4 id=&quot;radial-distortion&quot;&gt;Radial Distortion&lt;/h4&gt;
&lt;p&gt;intrinsic 파라미터 중 하나이다. 카메라 렌즈의 굴절에 의해 일어나는 왜곡으로써, 렌즈의 중앙과 멀어질수록 왜곡이 심하게 일어난다. 왜곡 모델은 다음과 같다.&lt;/p&gt;

&lt;p&gt;먼저, $(\hat{x}, \hat{y}, \hat{z})$를 normalized coordinate로 옮긴다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;(x_n', y_n') = (\frac{\hat{x}}{\hat{z}}, \frac{\hat{y}}{\hat{z}})&lt;/script&gt;

&lt;p&gt;그 후 radial distortion을 적용한다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;r^2 = x_n'^2 + y_n'^2&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\left(\begin{matrix}x_d' \\ y_d'\end{matrix}\right)=
(1+k_1r^2+k_2r^4)\left(\begin{matrix}x_n' \\ y_n'\end{matrix}\right)&lt;/script&gt;

&lt;p&gt;마지막으로 &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
K = \left[\begin{matrix}f &amp; 0 &amp; p_x \\ 0 &amp; f &amp; p_y \\ 0 &amp; 0 &amp; 1\end{matrix}\right] %]]&gt;&lt;/script&gt;을 적용한다.&lt;/p&gt;

&lt;h4 id=&quot;camera-calibration--linear-method&quot;&gt;Camera calibration : Linear Method&lt;/h4&gt;
&lt;p&gt;그렇다면 $\Pi$는 어떻게 찾을 수 있을까? 이미지 상의 좌표 $x$에 대해 $x=\Pi X$를 만족하고, $\Pi \in \mathbb{R}^{3\times4}$이므로, 12개의 변수를 가지고 있다. (하지만 마지막은 항상 1이므로, 사실은 11개의 변수를 가지고 있다.) 따라서 최소 12개의 식을 알고 있다면 이 방정식을 풀어 $\Pi$를 찾을 수 있다. 하나의 $x$는 $x, y$의 두 좌표를 가지고 있으므로, 최소 6개의 $x, X$ 쌍을 알고 있다면 $\Pi$를 알아낼 수 있다.&lt;/p&gt;

&lt;p&gt;$X$가 projection에 의해 $x$로 사영되므로, 둘의 방향은 같다. 따라서 $x_i \times \Pi X_i = 0$이라는 관계식을 가지고 있다. 이 식에서 linearly independent한 두 개의 식만 가지고 와서&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\left[\begin{matrix}
0 &amp; -X_i^T &amp; y_iX_i^T \\
X_i^T &amp; 0 &amp; -x_iX_i^T
\end{matrix}\right]
\left(\begin{matrix}\Pi_x \\ \Pi_y \\ \Pi_z\end{matrix}\right)=0 %]]&gt;&lt;/script&gt;

&lt;p&gt;의 식을 만들 수 있다.($\Pi_{x, y, z} \in \mathbb{R}^{4\times1}$) 6개의 점을 알고 있다면 여기에 대입하여&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\left[\begin{matrix}
0^T &amp; X_1^T &amp; -y_1X_1^T \\
X_1^T &amp; 0^T &amp; x_1X_1^T \\
\cdots &amp; \cdots &amp; \cdots \\
0^T &amp; X_6^T &amp; -y_6X_6^T \\
X_6^T &amp; 0^T &amp; x_6X_6^T \\
\end{matrix}\right]
\left(\begin{matrix}\Pi_x \\ \Pi_y \\ \Pi_z\end{matrix}\right)=0 %]]&gt;&lt;/script&gt;

&lt;p&gt;을 풀면 $\Pi$를 알아낼 수 있다.&lt;/p&gt;

&lt;p&gt;Linear method의 장점은 수식화가 쉽고 풀기도 쉽다는 점이다. 하지만 직접적으로 카메라 파라미터를 알 수는 없으며 radial distortion도 고려하지 않는다. 또한 제한 사항도 집어넣을 수 없다는 단점이 있다. 따라서 non-Linear method가 더 선호된다.&lt;/p&gt;

&lt;h2 id=&quot;epipolar-geometry&quot;&gt;Epipolar Geometry&lt;/h2&gt;

&lt;h4 id=&quot;triangulation&quot;&gt;Triangulation&lt;/h4&gt;
&lt;p&gt;두개, 혹은 그 이상의 이미지를 가지고 3D상의 점을 찾는 것을 triangulation이라고 한다. $O_1$ 을 원점으로 갖는 이미지에서의 점 $x_1$, $O_2$를 원점으로 갖는 이미지에서의 점 $x_2$를 각각의 원점과 이은 선은 실제 점 $x$에서 교차해야 한다. 하지만 노이즈나 에러때문에 정확하게 만나지는 않는다. 따라서 두 선의 거리가 가장 가까운 두 점을 이은 선분의 중점을 $x$라고 예측한다.&lt;/p&gt;

&lt;p&gt;$X$를 알아내는 linear approach는 다음과 같다. $x_1$과 $x_2$는 $\Pi_iX$와 평행하기 때문에 각각 다음의 식이 성립한다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x_i \times P_iX=0&lt;/script&gt;

&lt;p&gt;그리고 $x_i\times PX=[x_{i\times}]PX$를 만족하는 $[x_{i\times}]$를 찾을 수 있다. 하나의 $x_i$에 대해 $[x_{i\times}]P_xX=0&lt;script type=&quot;math/tex&quot;&gt;,&lt;/script&gt;[x_{i\times}]P_yX=0$의 두 식을 알아낼 수 있고(두 개만 linearly independent하므로) 우리는 두 개의 $x_i$를 알고 있으므로 총 4개의 식을 알고 있다. 알아내야 하는 $X$의 변수가 3개이므로, 방정식을 풀어 $X$를 알아낼 수 있다.&lt;/p&gt;

&lt;p&gt;$X$를 알아내는 non-linear approach는 $X = argmin(d^2(x_1, P_1X)+d^2(x_2, P_2X))$를 찾아내면 된다.&lt;/p&gt;

&lt;h4 id=&quot;epipolar-geometry-1&quot;&gt;Epipolar Geometry&lt;/h4&gt;
&lt;p&gt;두 개의 이미지에서 한 점의 좌표를 알아내는 상황을 가정한다. baseline은 이미지의 두 원점을 이은 선이고, epipolar plane은 그 선을 포함하는 평면들이다. epipole은 baseline과 각 이미지 평면들의 교점이다. 마지막으로 epipolar line은 epipolar plane과 이미지 평면의 교점이다.&lt;/p&gt;

&lt;h4 id=&quot;epipolar-constraint&quot;&gt;Epipolar Constraint&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;https://slideplayer.com/slide/4824174/15/images/43/Epipolar+constraint%3A+Calibrated+case.jpg&quot; alt=&quot;&quot; /&gt;
&lt;a href=&quot;https://slideplayer.com/slide/4824174/15/images/43/Epipolar+constraint%3A+Calibrated+case.jpg&quot;&gt;Source&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;만약 $I$이미지 상의 $x$가 존재한다면, $I’$이미지에서 이 $x$에 대응하는 점 $x’$은 epipolar line에서 찾을 수 있다는 것이 epipolar constraint이다.&lt;/p&gt;

&lt;p&gt;원점 $O$에서 $I$위의 점 $x$로 향하는 벡터 $x$와 원점 $O’$에서 $I’$위의 점 $x’$로 향하는 벡터 $Rx’$($R$은 $I$에서 $I’$로의 변환행렬), 그리고 $O$와 $O’$를 잇는 선인 $t$는 같은 평면(삼각형 $XOO’$가 이루는 평면)상의 벡터이다. 따라서 $x\cdot[t\times(Rx’)]=0$이 성립하고, 이는 $x^TEx’=0$으로 표현될 수 있다. ($E=[t_\times]R$) 이때 $E$를 essential 행렬이라고 부른다. 사실 이 $E$가 반드시 존재한다는 것이 epipolar constraint이다.&lt;/p&gt;

&lt;p&gt;위 식을 이용하면 epipolar line인 $l$과 $l’$이 $l=Ex’$, $l’=E^Tx$임을 알 수 있다. (삼각형 $XOO’$에서 $x$벡터에 수직인 선분은 $l$밖에 없으므로, $x’$에 대해서도 마찬가지) 따라서 epipolar line상에 있는 epipole들도 $Ee’=0$, $E^Te=0$을 만족한다. $E$는 자유도 3의 회전, 자유도 2의 평행이동으로 이루어져 있으므로 5개의 점의 좌표를 알면 구할 수 있다.&lt;/p&gt;

&lt;p&gt;$E$는 normalized coordinate 상에서의 좌표들의 관계를 나타내는 행렬이고, 다음은 정규화도 되지 않은 실제 공간에서의 좌표들의 관계를 나타내는 행렬 $F$를 구하는 방법이다. 일단 우리는 calibration matrix $K$와 $K’$를 모른다고 가정하고, 이들은 다음을 만족한다. $x = K\hat{x}$, $x’=K’\hat{x}’$. $\hat{x}$와 $\hat{x}’$ 를 이용한 epipolar constraint의 식은 $\hat{x}^TE\hat{x}’=0$ 이다. 이제 $F=K^{-T}EK’^{-1}$에 대해 $x^TFx’=0$이 성립한다. 이 때의 $F$를 fundamental matrix라고 한다. $F$를 구하기 위해서는 $E$를 구할 때보다 $K$, $K’$의 정보가 더 필요하기 때문에 7개의 점의 좌표가 필요하다.&lt;/p&gt;

&lt;h4 id=&quot;the-eight-point-algorithm&quot;&gt;The eight-point algorithm&lt;/h4&gt;
&lt;p&gt;다른 방식으로 $F$를 구할 수도 있다. $F$는 어쨌건 $3\times3$ 행렬이므로 9개의 식을 알고 있으면 구할 수 있다. 게다가, 하나만 존재하게 하기 위해 마지막 수는 항상 1이 되어야 한다는 조건을 추가하면, 8개의 점을 알면 구할 수 있다. 이를 eight-point algorithm이라고 부르며, $\sum_{i=1}^N(x_i^TFx_i’)^2$를 최소화시키는 $F$를 구하면 된다. 이 에러의 의미는 어떤 점 $x$와 epipolar line $Fx’$사이의 유클리드 거리의 합을 최소화시킨다는 것이다. eight-point algorithm은 원래 좌표를 normalize시킨 후 계산하고, 다시 원래 좌표로 변환시켜 계산했을 때 더 정확하다.&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;terms&quot;&gt;Terms&lt;/h2&gt;
&lt;p&gt;calibration : 교정, 눈금&lt;br /&gt;
epipolar : 등극선 (이라는데 명확하지 않은 번역이므로 그냥 epipolar라고 쓴다)&lt;br /&gt;
coplanar : 동일 평면상의&lt;/p&gt;
</description>
        <pubDate>Sun, 07 Jul 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/lecture/computervision/2019/07/07/Lecture3.html</link>
        <guid isPermaLink="true">http://localhost:4000/lecture/computervision/2019/07/07/Lecture3.html</guid>
        
        
        <category>Lecture</category>
        
        <category>ComputerVision</category>
        
      </item>
    
      <item>
        <title>CV Lecture 2 - Image Formation and Camera</title>
        <description>&lt;h2 id=&quot;pinhole-camera-model&quot;&gt;Pinhole Camera Model&lt;/h2&gt;

&lt;h4 id=&quot;pinhole-camera&quot;&gt;Pinhole Camera&lt;/h4&gt;
&lt;p&gt;어떤 물체에서 반사된 빛을 필름에 바로 닿게 하면 물체의 여러 점에서 반사된 빛이 필름의 한 점에 모이게 되어 물체의 모양이 제대로 나올 수 없고 블러된 이미지가 나온다.&lt;/p&gt;

&lt;p&gt;Pinhole은 바늘구멍이다. 물체와 필름 사이에 작은 구멍이 뚫린 장애물을 놓고 반사된 빛이 필름에 닿게 하면 그 바늘구멍을 통과한 빛만이 필름에 닿게 된다. 이는 블러를 줄이게 된다. 이 바늘구멍이 카메라의 조리개 역할을 한다.&lt;/p&gt;

&lt;p&gt;하지만 바늘구멍에 통과된 이미지는 위아래, 좌우가 바뀌게 된다.&lt;/p&gt;

&lt;h4 id=&quot;focal-length&quot;&gt;Focal Length&lt;/h4&gt;
&lt;p&gt;필름과 핀홀 사이의 거리를 Focal Length라고 한다. 이 Focal Length가 2배가 되면 필름에 사영된 이미지의 크기도 2배가 되고, 빛의 양은 $1 \over 4$이 된다.&lt;/p&gt;

&lt;h4 id=&quot;aperture-size&quot;&gt;Aperture Size&lt;/h4&gt;
&lt;p&gt;조리개의 크기가 크면 빛이 많이 들어와서 더 블러된 이미지가 생성되고, 조리개가 작으면 빛이 적게 들어와서 더 확실한 이미지가 생성된다. 그러나 조리개의 크기가 적당 크기 이하로 더 작아지면 Diffraction limit으로 인해 오히려 더 블러된 이미지가 생성된다.&lt;/p&gt;

&lt;p&gt;빛은 파동의 성질을 갖고 있기 때문에 구멍이 작아지면 더 많이 회절하게 되어 더 블러된 이미지가 생성된다. 이런 한계를 Diffraction limit이라고 한다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;이를 극복하기 위해서는 point-spread function(PSF)를 deconvolution 하면 된다??&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;h4 id=&quot;vanishing-points-and-lines&quot;&gt;Vanishing Points and Lines&lt;/h4&gt;
&lt;p&gt;카메라로 사진을 찍으면 3D 대상이 2D 이미지로 사영되는 것이기 때문에 길이와 각도 정보를 잃게 된다. 하지만 직선은 보존된다.&lt;/p&gt;

&lt;p&gt;모든 직선(이미지에 평행한 직선 제외)은 각자의 소실점을 갖고 있다. 한 직선은 그에 대응하는 소실점으로 수렴한다. 마찬가지로 한 평면은 그에 대응하는 소실선으로 수렴한다.&lt;/p&gt;

&lt;h4 id=&quot;perspective-distortion&quot;&gt;Perspective Distortion&lt;/h4&gt;
&lt;p&gt;흔히 건물같이 거대한 대상을 촬영할 때 문제가 되는 현상이다. 건물을 아래에서 올려다보는 각도로 찍으면 아래부분은 더 확대되어 보이고, 윗부분은 작게 보인다. 건물과 평행한 각도로 찍으면 건물의 아랫부분밖에 보이지 않는다.&lt;/p&gt;

&lt;p&gt;이 왜곡에 대한 대안점은 렌즈를 건물에 평행한 각도에서 위로 높여 찍는 것이다. 렌즈를 건물 높이에 따라 움직일 수 있는 view camera라는 것이 있다.&lt;/p&gt;

&lt;p&gt;세로에서 일어나는 것고 같이, 가로에서도 같은 현상이 일어난다. 렌즈의 중심에서보다 렌즈의 외곽에 있는 대상에서 왜곡이 많이 일어나 대상이 크게 퍼져 보인다.&lt;/p&gt;

&lt;h2 id=&quot;modeling-projection&quot;&gt;Modeling Projection&lt;/h2&gt;
&lt;p&gt;$(x, y, z)$에서 반사된 점이 $y$축과의 거리가 $f$인 PP를 지나 $(0, 0, 0)$의 조리개에 들어간다고 하자. 이 때 PP에 사영된 $(x, y, z)$의 빛은 $(-f \frac{x}{z}, -f\frac{y}{z}, -f)$를 지난다. 우리는 2D 이미지를 얻을 것이므로 z축은 무시되고, 따라서 $(x, y, z)$는 PP의 $(-f \frac{x}{z}, -f\frac{y}{z})$으로 사영된다.&lt;/p&gt;

&lt;p&gt;이것은 $z$로 나누어지기 때문에 비선형적이므로, 하나의 차원을 더 늘려주는 트릭을 사용하므로써 선형으로 만들 수 있다. $(x, y)$를 $(x, y, 1)$로 표현하는 것이다. 이를 homogeneous coordinates라고 한다. 반대도 마찬가지로, homogeneous coordinates에서 $(x, y, \omega)$로 표현된 좌표는 2차원에서 $(\frac{x}{\omega}, \frac{y}{\omega})$로 표현될 수 있다.&lt;/p&gt;

&lt;h4 id=&quot;perspective-projection&quot;&gt;Perspective Projection&lt;/h4&gt;
&lt;p&gt;사영은 homogeneous coordinates를 사용해서 행렬곱을 하는 것이다. 아까의 예를 들어 보면, $(x, y, z)$의 점을 COP로부터 $f$만큼 떨어져 있는 PP에 사영할 때&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\left[\begin{matrix}
1 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 1 &amp; 0 &amp; 0\\
0 &amp; 0 &amp;-\frac{1}{f} &amp; 0
\end{matrix} \right]
\left[\begin{matrix} x \\ y \\ z \\ 1 \end{matrix}\right]=
\left[\begin{matrix}x \\ y \\ -\frac{z}{f}\end{matrix}\right] %]]&gt;&lt;/script&gt;

&lt;p&gt;이 되고, 이는 homogeneous coordinates이기 때문에 2차원으로 바꾸면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;(-f\frac{x}{z}, -f\frac{y}{z})&lt;/script&gt;

&lt;p&gt;가 된다. 마지막 차원의 수가 1이 되도록 나눠야 하기 때문에 곱해지는 행렬에 다른 수가 곱해져도 결과는 같다.&lt;/p&gt;

&lt;h4 id=&quot;orthographic-projection&quot;&gt;Orthographic Projection&lt;/h4&gt;
&lt;p&gt;원근 사영의 특이 케이스로, 실제 모양 그대로를 깊이 정보만 없앤 채 PP에 사영시킨다. 곱셈식은 다음과 같고, 이는 실제 정보의 $(x, y, z)$ 정보에서 $(x, y)$만 가지고 오는 것과 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\left[\begin{matrix}
1 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 1 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 1
\end{matrix} \right]
\left[\begin{matrix} x \\ y \\ z \\ 1 \end{matrix}\right]=
\left[\begin{matrix} x \\ y \\ 1 \end{matrix}\right] %]]&gt;&lt;/script&gt;

&lt;p&gt;telecentric lens로 orthogonal projection한 영상을 얻을 수 있다. 이 렌즈는 일반 렌즈와 달리 거리에 따라 상의 크기가 달라지지 않으며, 치수를 측정해야 하는 산업용 카메라에 쓰인다. orthogonal projection의 여러 변이들 중에는 확대되어보이게 해주는 weak perspective, 모양을 변형시키는 Affine projection 등이 있다.&lt;/p&gt;

&lt;h4 id=&quot;camera-parameters&quot;&gt;Camera Parameters&lt;/h4&gt;
&lt;p&gt;카메라의 extrinsic 파라미터들에는 translation T와 rotation R이 있고, intrinsic 파라미터에는 focal length $f$가 있다.&lt;/p&gt;

&lt;p&gt;projection equation에서 $X$에 곱해지는 행렬 $\Pi$는 intrinsic matrix와 projection matrix, extrinsic matrix의 곱으로 이루어진다.&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;terms&quot;&gt;Terms&lt;/h2&gt;
&lt;p&gt;aperture : 조리개&lt;br /&gt;
pencil of rays : 광선속&lt;br /&gt;
diffraction limit : 회절 한계&lt;br /&gt;
camera obscura : 암실&lt;br /&gt;
point-spread function : 점분산함수&lt;br /&gt;
COP : Centor Of Projection&lt;br /&gt;
PP : Projection Plane&lt;br /&gt;
perspective : 원근&lt;/p&gt;
</description>
        <pubDate>Sat, 06 Jul 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/lecture/computervision/2019/07/06/Lecture2.html</link>
        <guid isPermaLink="true">http://localhost:4000/lecture/computervision/2019/07/06/Lecture2.html</guid>
        
        
        <category>Lecture</category>
        
        <category>ComputerVision</category>
        
      </item>
    
      <item>
        <title>Strand-accurate Multi-view Hair Capture</title>
        <description>&lt;hr /&gt;
&lt;p&gt;이 논문은 가닥 수준에서 정확한 결과를 내는 3D 머리카락 검출 알고리즘을 제안하는 논문이다.&lt;/p&gt;
</description>
        <pubDate>Wed, 03 Jul 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/thesis/2019/07/03/HairCapture.html</link>
        <guid isPermaLink="true">http://localhost:4000/thesis/2019/07/03/HairCapture.html</guid>
        
        <category>MultiViewStereo</category>
        
        
        <category>Thesis</category>
        
      </item>
    
      <item>
        <title>Deep Video Super-Resolution Network Using Dynamic Upsampling Filters Without Explicit Motion Compensation</title>
        <description>&lt;hr /&gt;
&lt;p&gt;이 논문은 Video Super-Resolution을 위한 end-to-end 구조를 제안하는 논문이다.&lt;/p&gt;

&lt;h2 id=&quot;video-super-resolution-vsr&quot;&gt;Video Super Resolution (VSR)&lt;/h2&gt;
&lt;p&gt;VSR을 하는 직접적인 방법은 single-image super-resolution(SISR)을 프레임마다 하는 것인데, SISR은 프레임간의 관계를 고려하지 않기 때문에, 깜빡거리는 결과가 나올 수 있다. 기존의 VSR은 여러 low-resolution(LR) 프레임들을 인풋으로 받고, 연속된 LR 프레임들의 움직임을 고려하여 high-resolution(HR) 프레임들을 내놓는다. 딥러닝 기반의 VSR은 보통 모션 예측과 보정 과정으로 이루어진다. 문제점은 모션 예측에 의존성이 높고, CNN을 이용하기 때문에 블러된 아웃풋이 나온다는 점이다.&lt;/p&gt;

&lt;p&gt;이 논문에서는 새로운 end-to-end 뉴럴넷을 제안한다. 모션 정보가 dynamic upsampling filter를 만드는 데에 쓰인다. 만들어진 dynamic upsampling filter와 LR의 center frame을 가지고 HR frame이 만들어진다. 이 연구는 state-of-the-art인 VSRnet보다 더 날카로운 결과물을 만들 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;dynamic-upsampling-filters&quot;&gt;Dynamic Upsampling Filters&lt;/h2&gt;
&lt;p&gt;기존의 bilinear나 bicubic upsampling등의 방법들은 직접적인 모션 보정으로, 날카롭게 보정하기 힘들다. 이와 반대로 이 논문에서는 dynamic filter network를 사용한다. 이 filter들은 지역적으로 생성된다. dynamic filter들은 LR에서 주변 픽셀을 관찰하여 만들어지는데, 직접적인 모션 보정을 피할 수 있게 해준다.&lt;/p&gt;

&lt;h2 id=&quot;residual-learning&quot;&gt;Residual Learning&lt;/h2&gt;
&lt;p&gt;디테일들이 linear filtering으로는 보정이 되지 않을 수 있는데, 이 문제를 해결하기 위해 고주파수 디테일을 얻기 위해 residual image를 추가적으로 예측한다. 이 residual image는 여러 input 프레임(앞뒤 몇개의 프레임)을 통해 얻어진다. Dynamic Upsampling을 한 이미지가 이 residual image와 합쳐지면 더 날카롭고 앞뒤 문맥에 적합한 프레임이 얻어진다.&lt;/p&gt;

&lt;h2 id=&quot;network-design&quot;&gt;Network Design&lt;/h2&gt;
&lt;p&gt;filter와 residual generation network는 weight를 공유함으로써 오버헤드를 줄여준다. 공유된 부분의 네트워크 구조(3D layer)는 dense block에서 착안되었다. 각 input 프레임들은 공유된 2D convolutional layer들로 프로세싱되고 시간적 순서로 합쳐진다. 이것이 시공간 feature map인데, 이 3D dense block으로 들어가고, 각 가지들로 나눠져 처리되어 2개의 output(dynamic filter, residual image)를 만든다. 이후 LR 프레임이 dynamic filter와 convolution되어 upsampling된 후 residual image와 합쳐지면 HR 프레임이 얻어진다.&lt;/p&gt;

&lt;h2 id=&quot;temporal-augmentation&quot;&gt;Temporal Augmentation&lt;/h2&gt;
&lt;p&gt;training data를 만들기 위해서는 정방향, 역방향, 프레임 건너뛰기 등을 이용할 수 있다.&lt;/p&gt;

&lt;hr /&gt;
</description>
        <pubDate>Mon, 01 Jul 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/thesis/2019/07/01/VSR.html</link>
        <guid isPermaLink="true">http://localhost:4000/thesis/2019/07/01/VSR.html</guid>
        
        <category>VideoSuperResolution</category>
        
        
        <category>Thesis</category>
        
      </item>
    
      <item>
        <title>Weakly-Supervised Semantic Segmentation by Iteratively Mining Common Object Features</title>
        <description>&lt;hr /&gt;
&lt;p&gt;이 논문은 이미지에서 tag만 주어진 상황에서 그 tag의 대상을 segmentation하는 반복적 방법을 제안한 논문이다.&lt;/p&gt;

&lt;h2 id=&quot;fully-supervised-semantic-segmentation&quot;&gt;Fully-Supervised Semantic Segmentation&lt;/h2&gt;
&lt;p&gt;Fully-Supervised Semantic Segmentation에는 region-based, pixel-based의 두가지 방법이 있다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Region-based : 이미지를 set of region으로 가져와서 label을 예측하기 위해 특징을 추출한다.&lt;/li&gt;
  &lt;li&gt;Pixel-based : 이미지 전체를 input으로 가져와서 pixel-wise label을 예측한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;보통은 pixel-based가 더 강력하지만, 이 논문에서는 둘의 장점을 모두 가져와서 사용할 것이다. region-based가 common feature를 추출하는 데서는 유리하다는 것을 보일 것이다.&lt;/p&gt;

&lt;h2 id=&quot;weakly-supervised-semantic-segmentation-under-image-tags-supervision&quot;&gt;Weakly-Supervised Semantic Segmentation under Image Tags Supervision&lt;/h2&gt;
&lt;p&gt;번역하면 ‘이미지 태그가 있으면 문법적으로 세그맨테이션 해주는 약한 감독학습’ 정도가 될 수 있다. 사람과 배경이 함께 있는 이미지에서 사람만을 세그멘테이션해주는 것이라고 생각하면 된다. Fully-Supervised Semantic Segmentation이 너무 많은 양의 labeling cost를 필요로 하는데, 여기서는 tag만 labeling하면 되므로 더 적은 cost가 필요해서 사용한다. 그러나 이것은 어려운 문제인데, tag라는 high-level semantic부터 픽셀 단위라는 low-level appearance까지 연관이 있기 때문이다. 현재까지의 연구는 두 가지로 나눌 수 있다.&lt;/p&gt;
&lt;h4 id=&quot;multi-instance-learning-mil-based-method&quot;&gt;Multi-Instance Learning (MIL) based method&lt;/h4&gt;
&lt;p&gt;classification network로 바로 segmentation mask 예측하는 방법이다. 모든 픽셀들은 하나의 클래스에 들어가 있다고 가정하고, 그 클래스를 예측하는 것이 목적이다. 하지만 경계 부분에 대해서는 좋지 않은 성능을 보인다.&lt;/p&gt;
&lt;h4 id=&quot;localization-based-method&quot;&gt;Localization-based method&lt;/h4&gt;
&lt;p&gt;classification network를 초기 localization에 사용하고, 그리고 그걸 segmentation network를 감독하는데 사용한다. weak label로 initial object localization을 만드는 것이 목적이다. 그러나 정확하지 않은 weak label을 계속 사용하기 때문에 에러가 계속해서 축적되어, 이것도 경계 부분에 대해서 좋지 않은 성능을 보인다.&lt;/p&gt;

&lt;h2 id=&quot;initial-localization&quot;&gt;Initial Localization&lt;/h2&gt;
&lt;p&gt;반복적인 학습을 위해서는 시작이 있어야 한다. 이 논문에서는 initial localization을 얻기 위해서 classification network를 학습하고 각 object의 heatmap을 얻기 위해 ‘Classification Activation Map(CAM)’을 사용한다. 그러나 얻어진 heatmap이 매우 rough하기 때문에, 이미지를 superpixel region으로 segmentation 하고, heatmap을 평균한다. heatmap중 threshold를 넘는 부분을 initial seed로 사용한다.&lt;/p&gt;

&lt;h2 id=&quot;mining-common-object-feature-mcof&quot;&gt;Mining Common Object Feature (MCOF)&lt;/h2&gt;
&lt;p&gt;제안된 이 방법은 ‘하나의 동일한 대상에는 공통된 특징이 있을 것이다’라는 생각으로부터 시작되었다. 따라서 큰 train set을 가지고 학습하면, 대상에 대한 특징을 학습하여 object의 region을 넒혀갈 수 있을 것이다. 논문에서 제안하는 network는 bottom-up, top-down의 두 가지의 큰 구조로 이루어져 있다.&lt;/p&gt;
&lt;h4 id=&quot;bottom-up-regionnet&quot;&gt;Bottom-Up (RegionNet)&lt;/h4&gt;
&lt;p&gt;원래의 object localization을 object seed로 이용해서 common object feature (COF) 를 찾아낸다. 순서는 다음과 같다.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;train image를 superpixel region으로 segmentation 한다. (graph-based segmentation method 사용)&lt;/li&gt;
  &lt;li&gt;Superpixel region과 initial seed를 가지고 region classification network를 학습한다. (mask-based Fast R-CNN 이용)&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;saliency-guided-refinement-method&quot;&gt;Saliency-guided refinement method&lt;/h4&gt;
&lt;p&gt;이 때 찾아낸 COF는 object의 key part만 포함하고 있으므로 경계 부분이 정확하지 않다. 이를 보완하기 위해 saliency-guided refinement method를 사용한다. 이 방법은 mined object region과, Bayesian framework를 이용한 saliency map을 같이 고려한다. 높은 saliency value를  가지고 있는 region에 대해, 만약 이 대상이 mined된 object와 유사하다면, 이 region은 그 대상에 속할 확률이 높다고 판단한다. 이 방법은 맨 처음 반복에서만 사용된다.&lt;/p&gt;
&lt;h4 id=&quot;top-down-pixelnet&quot;&gt;Top-Down (PixelNet)&lt;/h4&gt;
&lt;p&gt;bottom-up에서 얻은 object region을 이용해서 segmentation network를 학습한다. 이 network를 학습한 후 input을 집어넣으면 더 정확한 object mask를 얻을 수 있다.&lt;/p&gt;

&lt;p&gt;이 두 과정을 계속 반복한다. 이렇게 반복적으로 수행하면, 계속해서 더 정확한 segment를 얻을 수 있다.&lt;/p&gt;
</description>
        <pubDate>Sun, 30 Jun 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/thesis/2019/06/30/COF.html</link>
        <guid isPermaLink="true">http://localhost:4000/thesis/2019/06/30/COF.html</guid>
        
        <category>SemanticSegmentation</category>
        
        
        <category>Thesis</category>
        
      </item>
    
      <item>
        <title>Zero-shot Learning on Semantic Class Prototype Graph</title>
        <description>&lt;hr /&gt;
&lt;p&gt;이 논문은 Zero-Shot Learning(ZSL)에서의 새로운 distance metric을 제안하는 논문이다.&lt;/p&gt;

&lt;h2 id=&quot;zero-shot-learningzsl&quot;&gt;Zero-Shot Learning(ZSL)&lt;/h2&gt;
&lt;p&gt;ZSL에서는 seen class와 unseen class, 그리고 image feature가 semantic embedding space에 사영된다. 새로운 input은 seen class중 한 class에 해당하고, 어떤 image feature를 가지고 있다고 하자. 그 사영된 공간에서 seen class와 image feature가 결합된 벡터가 unseen class와 가까우면, unseen class에 대한 학습 없이도 해당 class로 분류가 가능하다. 마치 어린아이가 ‘말’을 알고 ‘줄무늬’를 알면 ‘얼룩말’을 한번도 본적이 없더라도 판별해낼 수 있는 것과 같다.&lt;/p&gt;

&lt;h2 id=&quot;semantic-embedding-space&quot;&gt;Semantic Embedding Space&lt;/h2&gt;
&lt;p&gt;Seen class와 unseen class는 고차원 공간상에서 연관을 갖고있다. 이를 Semantic embedding space라고 부른다.&lt;/p&gt;
&lt;h4 id=&quot;semantic-embedding-se&quot;&gt;Semantic Embedding (SE)&lt;/h4&gt;
&lt;p&gt;거의 대부분의 ZSL은 SE를 사용한다. SE에서는 seen class에 대해서만 mapping을 학습한다. 이 mapping함수는 나중에 unseen class image가 space로 mapping될때도 사용된다.&lt;/p&gt;
&lt;h4 id=&quot;semantic-relatedness-sr&quot;&gt;Semantic Relatedness (SR)&lt;/h4&gt;
&lt;p&gt;거의 사용되지는 않지만 SR이라고 불리는 것도 있다. 먼저 seen class에 대해서 n-way classifier 를 학습하는데, 이 classifier는 unseen과 seen의 visual 유사도를 계산하는데 쓰인다. 그 후 semantic 유사도와 visual 유사도를 계산하는데, 이 두 유사도 벡터가 가까우면 unseen class로 예측한다.&lt;/p&gt;

&lt;h2 id=&quot;euclideancosine-distance-쓰면-생기는-문제점&quot;&gt;Euclidean/Cosine Distance 쓰면 생기는 문제점&lt;/h2&gt;
&lt;p&gt;기존 ZSL에서는 거리 계산 척도로 euclidean distance나 cosine distance를 사용했는데, 고차원 공간에서는 이런 척도가 hubness나 domain shift등의 문제를 야기할 수 있다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Hubness : 많은 벡터들에 동시에 가까운 중심 벡터가 있을 수 있다. 보통 high dimension에서 발생한다. unseen 클래스 수가 적어서 ZSL에서도 발생 가능성이 있다.&lt;/li&gt;
  &lt;li&gt;Domain shift :
    &lt;ul&gt;
      &lt;li&gt;(SE에서) visual feature embedding이 seen 클래스로부터 학습되어서, 사영된 unseen class point가 seen class 쪽으로 bias될 수 있다. 따라서 hub의 출현이 쉽다.&lt;/li&gt;
      &lt;li&gt;(SR에서) visual-semantic domain shift.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;semantic-manifold-distance&quot;&gt;Semantic Manifold Distance&lt;/h2&gt;
&lt;p&gt;일반적으로 semantic embedding space에서 label들은 semantic manifold 구조를 갖고 있는데, 이 manifold 구조를 더 잘 설명하는 distance이다. 여기에는 hubness와 domain shift 문제가 없다.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;여기까지가 서론이고, 이후 논문의 제안에 대한 내용이 있으나 제대로 이해를 못함…&lt;/p&gt;
</description>
        <pubDate>Sat, 29 Jun 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/thesis/2019/06/29/ZSL.html</link>
        <guid isPermaLink="true">http://localhost:4000/thesis/2019/06/29/ZSL.html</guid>
        
        <category>ZeroShotLearning</category>
        
        
        <category>Thesis</category>
        
      </item>
    
  </channel>
</rss>
