<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>YeonjeeJung's Blog</title>
    <description></description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Tue, 03 Sep 2019 17:45:12 +0900</pubDate>
    <lastBuildDate>Tue, 03 Sep 2019 17:45:12 +0900</lastBuildDate>
    <generator>Jekyll v3.8.6</generator>
    
      <item>
        <title>Lecture 1</title>
        <description>&lt;p&gt;딥러닝에서 objective function의 설계는 중요하다. 이 때 문제를 더 간단하게 만들기 위해 convex의 개념을 도입한다.&lt;/p&gt;

&lt;h3 id=&quot;convex-set&quot;&gt;Convex set&lt;/h3&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{For } x \in C \text{ and } y \in C \Rightarrow (1-\alpha)x+\alpha y \in C, \forall\alpha\in[0, 1]&lt;/script&gt;

&lt;p&gt;convex set을 쓰면 line 전체가 constraint 안에 있기 때문에 line search를 할 때 쓴다. convex set을 가정한 방법들은 non-convex에서도 사용할 수 있다. local minima 부근에서는 convex이기 때문인데, 이 때 convex 방법들을 적용하면 global minima는 아니다.&lt;/p&gt;

&lt;h3 id=&quot;convex-function&quot;&gt;Convex function&lt;/h3&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\alpha f(x_1)+(1-\alpha)f(x_2) \ge f(\alpha x_1 + (1-\alpha)x_2), \forall x_1, x_2, \forall \alpha \in [0, 1]&lt;/script&gt;

&lt;p&gt;convex function의 가장 큰 특징은 global minima가 하나이고, 2차 미분식(또는 해시안 함수)이 항상 양수라는 점이다. 이또한 분석이 쉽다는 장점을 가지고 있다. 사실, unimodal function도 global minima가 하나지만, 이차미분항이 양수와 음수를 왔다갔다 할 수 있기 때문에 convex function보다는 더 분석이 어렵다.&lt;/p&gt;

&lt;p&gt;이차미분항이 양수라는 점에서 오는 장점은, convergence behavior를 알기가 쉽다는 점이다. 보통 잘 알고 있는 gradient descent 방법의 식은 다음과 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x_{t+1} \leftarrow x_t + \gamma\triangledown f(x)&lt;/script&gt;

&lt;p&gt;이 때 이차미분항이 양수이면, $\triangledown f(x)$의 양상을 더 쉽게 알 수 있다.&lt;/p&gt;
</description>
        <pubDate>Mon, 02 Sep 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/lecture/optimization/2019/09/02/Lecture1.html</link>
        <guid isPermaLink="true">http://localhost:4000/lecture/optimization/2019/09/02/Lecture1.html</guid>
        
        
        <category>Lecture</category>
        
        <category>Optimization</category>
        
      </item>
    
      <item>
        <title>CV Lecture 11 - Light</title>
        <description>&lt;h1 id=&quot;light&quot;&gt;Light&lt;/h1&gt;

&lt;h1 id=&quot;color-vector&quot;&gt;Color Vector&lt;/h1&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;terms&quot;&gt;Terms&lt;/h2&gt;
</description>
        <pubDate>Sat, 31 Aug 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/lecture/computervision/2019/08/31/Lecture11.html</link>
        <guid isPermaLink="true">http://localhost:4000/lecture/computervision/2019/08/31/Lecture11.html</guid>
        
        
        <category>Lecture</category>
        
        <category>ComputerVision</category>
        
      </item>
    
      <item>
        <title>Peeking Inside the Black-Box ; A Survey on Explainable Artificial Intelligence (XAI) - 2</title>
        <description>&lt;h1 id=&quot;axis-1-xai-methods-taxonomy--explainability-strategies&quot;&gt;AXIS 1. XAI Methods Taxonomy : Explainability Strategies&lt;/h1&gt;

&lt;p&gt;현재 있는 설명 방법들의 오버뷰를 제시할 것이다. 세 가지의 기준을 가지고 나눌 수 있는데,&lt;br /&gt;
A. 해석의 복잡도&lt;br /&gt;
B. 해석의 범위&lt;br /&gt;
C. 사용된 ML 모델에 대한 의존성&lt;br /&gt;
이다.&lt;/p&gt;

&lt;h2 id=&quot;a-complexity-related-methods&quot;&gt;A. Complexity Related Methods&lt;/h2&gt;

&lt;p&gt;모델의 복잡도는 직접적으로 해석력과 연관되어 있다. 일반적으로 더 복잡하면 더 설명하기 어렵다. 따라서 설명가능한 모델을 얻는 직접적인 방법은 본질적으로 설명가능한 알고리즘을 디자인하는 것이다.&lt;/p&gt;

&lt;p&gt;BRL 모델은 결정 트리 기반의 모델인데, 저자는 초기의 설명가능한 모델이 XAI에 더 적합하다고 주장한다. 시각화를 통해 결과를 설명할 수 있는 attention model과, SLIM이라고 불리는 데이터 기반의 점수 시스템을 제안하는 연구도 있다. 최근에는 해석력과 정확도의 trade-off를 강조하며 ‘설명가능한 모델은 정확도 하락이라는 cost를 갖는다’를 주장하는 연구들도 활발하다.&lt;/p&gt;

&lt;p&gt;설명가능한 모델을 만드는 대신, 복잡하고 높은 정확도를 갖는 블랙박스 모델을 만들고 역공학을 통해 설명을 제공하는 연구도 있다. 이 방법은 매우 복잡하고 비싸지만, 최근 XAI에서는 자연어, 시각화, 예시를 통한 설명이라는 분야들이 활발히 연구되고 있다. 만약 높은 정확도가 필수적이라면, 이 연구가 꼭 필요하다.&lt;/p&gt;

&lt;h2 id=&quot;b-scoop-related-methods&quot;&gt;B. Scoop Related Methods&lt;/h2&gt;

&lt;p&gt;모델의 해석력은 두가지의 범위가 있다.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;모델 전체의 행동을 이해 (Global interpretability)&lt;/li&gt;
  &lt;li&gt;하나의 예측을 이해 (Local interpretability)&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;1-global-interpretability&quot;&gt;1. Global Interpretability&lt;/h3&gt;
&lt;p&gt;전체 해석력은 모델의 전체 논리를 이해하고 전체 모든 가능한 결과에 대해 추론하게 해준다. 많은 대상에 대한 추론에 사용한다.&lt;/p&gt;

&lt;p&gt;GRIP이라고 불리는, local explanation을 이용해 모델의 global 설명 트리를 만들 수 있는 방법이 제안되었다. 이 방법으로 모델이 정당하게 작동하고 있는지, 아니면 오버핏되었는지를 알 수 있다. global 정보 추출을 위한 감독 방법도 제안되었다. 이 연구는 DNN이 해석 가능한 패턴 기반의 모델과 결합되어 해석될 수 있다는 아이디어를 돕는다. Activation maximization(선호되는 인풋을 종합하는 것) 기반의 접근법도 제안되었는데, 이미지 인식을 위해 global 해석가능한 모델을 만들어준다.&lt;/p&gt;

&lt;p&gt;global 해석력은 파라미터가 너무 많은 경우에 얻기 어렵다. 사람은 전체를 이해하기 위해 부분에 집중하는데, 따라서 local 해석력이 더 적용하기 좋다.&lt;/p&gt;

&lt;h3 id=&quot;2-local-interpretability&quot;&gt;2. Local Interpretability&lt;/h3&gt;
&lt;p&gt;특정한 결정에 대한 설명이나 하나의 예측을 하는 것은 해석력이 local에서 작용한다는 것을 말한다. 이러한 해석의 범위는 특정한 예를 들어서 모델이 왜 이런 선택을 했는지를 정의하는 데에 사용된다.&lt;/p&gt;

&lt;p&gt;LIME은 블랙박스 모델을 관심있는 특정 예측 근처에서 근사할 수 있다. LOCO는 local variable의 중요도를 알려주는, local explanation을 할 수 있는 또다른 유명한 방법이다. local gradient를 이용해 local decision을 설명할 수 있는 방법도 있는데, 이 방법을 이미지 분류에도 쓸 수 있다. 이미지 분류 시스템에서 최종 클래스를 찾을 때 중요한 region을 찾는 것은 일반적인 방법론인데, 이 region은 sensitivity map, saliency map, pixel attribution map 등으로 불린다.&lt;/p&gt;

&lt;p&gt;모델의 예측을 각각의 특징에 대한 개별적인 기여로의 분해에 기반하여 예시를 통해 모델을 설명하는 방법을 제시한 연구도 있다. 원래의 예측과, 특정 특징들을 무시하고 예측했을 때의 차이를 측정한다. local 방법들을 결합해주는 Shapely Explanation이라고 불리는 방법을 제안한 연구도 있다.&lt;/p&gt;

&lt;p&gt;유망한 연구들은 local과 global explanation의 장점들을 결합하는 연구들이다. 4가지의 가능한 결합들은&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;일반적인 global explanation&lt;/li&gt;
  &lt;li&gt;모델의 각 부분들이 어떻게 전체 결과에 영향을 주는지 (global explanation을 부분 수준에 적용)&lt;/li&gt;
  &lt;li&gt;왜 모델이 예시 그룹에 대해 이런 결정을 했는지 (그룹 수준에 local explanation 적용)&lt;/li&gt;
  &lt;li&gt;일반적인 local explanation
이다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;local 설명은 보통 DNN에서 쓰이지만, 학자들은 자신의 방법이 DNN이 아니라 어떤 모델이든 적용될 수 있다고 말한다.&lt;/p&gt;

&lt;h2 id=&quot;c-model-related-methods&quot;&gt;C. Model Related Methods&lt;/h2&gt;

&lt;p&gt;위에서 설명한 것처럼 local 설명은 어떤 모델이든 적용될 수 있다고 했다. 이런 방법들은 Model-agnostic 설명력이라고 하고, 특정 모델에만 쓰일 수 있는 방법들은 Model-specific 설명력이라고 한다.&lt;/p&gt;

&lt;h3 id=&quot;1-model-specific-interpretability&quot;&gt;1. Model-specific Interpretability&lt;/h3&gt;
&lt;p&gt;이 설명력의 단점은, 우리가 특정한 타입의 설명이 필요할 때 그 설명이 가능한 모델만 사용할 수 있고, 더 정확도가 높거나 표현력이 좋은 모델을 사용할 수 없다는 비용이 든다는 점이다. 따라서 최근에는 model-agnostic 방법을 쓰는 것이 유행이다.&lt;/p&gt;

&lt;h3 id=&quot;2-model-agnostic-interpretability&quot;&gt;2. Model-Agnostic Interpretability&lt;/h3&gt;
&lt;p&gt;이 종류에 속하는 방법들은 설명과 예측을 분리시킨다. 이들은 주로 사후 분석(post-hoc)이다. 4가지의 종류로 나눠서 설명할 것이다.&lt;/p&gt;

&lt;p&gt;a) Visualization&lt;br /&gt;
b) Knowledge Extraction&lt;br /&gt;
c) Influence Methods&lt;br /&gt;
d) Example-based Explanation&lt;/p&gt;

&lt;h4 id=&quot;a-visualization&quot;&gt;a) Visualization&lt;/h4&gt;
&lt;p&gt;DNN의 내부를 이해하기 위한 가장 일반적인 아이디어이다. 시각화는 반드시 감독학습에 적용되어야 한다. 대표적인 시각화 방법은&lt;br /&gt;
(1) Surrogate models&lt;br /&gt;
(2) Partial Dependence Plot(PDP)&lt;br /&gt;
(3) Individual Conditional Expectation (ICE)&lt;br /&gt;
이다.&lt;/p&gt;

&lt;p&gt;(1) 근사 모델은 복잡한 모델을 설명하는 데에 사용되는 간단한 모델이다. 나중의 블랙박스 모델 해석을 위하여 예측 단계에 학습된다. 그러나 간단한 근사 모델이 복잡한 것보다 더 표현력이 뛰어나다는 연구 결과는 없다. LIME 방법은 하나의 관찰 근처에서 local 근사 모델을 만드는 방법이다. 모델 행동을 표현해주는 결정트리를 뽑아주는 근사 방법 연구가 있다. 또한 근사 모델을 가지고 계층 시각화를 하는 방법도 제안되었다.&lt;/p&gt;

&lt;p&gt;(2) PDP는 하나 또는 그 이상의 변수와 블랙박스 모델의 예측 간의 평균적인 부분 관계를 시각화하는 데 도와주는 그래픽 표현이다. 감독학습 모델을 이해하기 위해서 PDP를 사용하는 연구들은 다음과 같다. Bayesian Additive Regression 예측기와 conditional average treatment effect 사이의 관계를 이해하기 위해 PDP를 사용한 연구가 있다. 생물학과 관련된 연구에서는, PDP를 stochastic gradient boosting에 사용했고, 비대칭 분류에서 랜덤포레스트와 PDP를 사용해 결정자와 반응 관계를 알아보는 것이 의미있음을 증명한 연구도 있다. 최근에는 랜덤포레스트를 시각화하기 위한 Forest Floor라는 방법이 제안되었는데, 이 방법은 PDP보다 특징 기여에 더 의존한다. 이 방법이 더 좋은 이유는, PDP의 평균에 의해서는 상호작용을 알 수 없다는 점이다.&lt;/p&gt;

&lt;p&gt;(3) ICE는 PDP를 확장한 것이다. PD는 대략적인 모델의 작동만을 보여주는데, ICE는 PDP 결과물을 흩트려서 상호작용과 개별 분산을 보여준다. 최근 연구들은 PDP보다는 ICE를 사용한다. 부분 중요도(partial importance, PI)와 개별 조건 중요도(individual conditional importance, ICI) 둘 다를 시각 툴로 사용한 local 특징 중요도 기반 방법을 제안한 연구가 있다.&lt;/p&gt;

&lt;h4 id=&quot;b-knowledge-extraction&quot;&gt;b) Knowledge Extraction&lt;/h4&gt;

&lt;p&gt;특히 ANN에 기반을 둔 모델이라면, ML 모델이 어떻게 동작하는지 아는 것은 어렵다. 모델은 학습하면서 내부 변수를 변경한다. 따라서 ANN에서 설명을 추출한다는 것은, 학습으로부터 얻은 정보와 내부 표현을 해석한다는 뜻이 된다. 두 가지의 주된 방법이 있는데,&lt;br /&gt;
(1) Rule Extraction&lt;br /&gt;
(2) Model Distillation&lt;br /&gt;
이다.&lt;/p&gt;

&lt;p&gt;(1) 복잡한 모델에서 인사이트를 얻을 수 있는 방법은 규칙 추출이다. 인풋과 아웃풋을 이용해서 어떻게 ANN이 결정을 내렸는지를 근사하는 규칙을 설명해주는 방법들의 연구가 많다. 이것은 전통적인 인공지능 전문가 시스템에서 사용하던 방법이다. 세 가지의 주된 방법들이 있다.&lt;br /&gt;
(a) Pedagogical(가르치는) rule extraction&lt;br /&gt;
(b) Decompositional rule extraction&lt;br /&gt;
(c) Eclectic(절충) rule extraction&lt;/p&gt;

&lt;p&gt;(b)는 ANN으로부터 학습된 각 유닛 수준에서 규칙을 추출하며 투명하게 보는 반면 (a)는 ANN을 블랙박스로 취급한다. (a)는 OSRE(Orthogonal Search-based Rule Extraction)에서 적용되었다. (c)는 (a)와 (b)를 융합한 방법이다.&lt;/p&gt;

&lt;p&gt;(2) 다른 방법은 모델 압축이다. 이 방법은 모델의 압축된 정보를 선생님 역할의 딥 네트워크에서 학생 역할의 얕은 네트워크로 전달하는 방법이다. 모델 압축은 계산량을 줄이기 위해 제안되었으나 이후에는 해석을 위해 사용되었다. Interpretable Mimic Learning은 깊은 망의 성능까지 따라하면서 robust한 결정을 내릴 수 있는 특징을 학습하는 방법이다. DarkSight는 dark knowledge의 개념에서 감명받아 만든 블랙박스 분류기의 예측을 해석하는 시각화 방법이다. 이는 지식 압축, 차원 축소, 시각화 방법들을 결합한 것이다.&lt;/p&gt;

&lt;h4 id=&quot;c-influence-methods&quot;&gt;c) Influence Methods&lt;/h4&gt;

&lt;p&gt;이 부류의 방법들은 인풋이나 내부 변수를 바꾼 뒤 모델 결과에 얼마나 영향을 미치는지를 알아내어 특징들의 중요도나 관련성을 예측한다. 이 방법들은 시각화될 수도 있다. 세 가지의 주된 방법이 있다.&lt;br /&gt;
(1) Sensitivity Analysis (SA)&lt;br /&gt;
(2) Layer-wise Relevance Propagation (LRP)&lt;br /&gt;
(3) Feature Importance&lt;br /&gt;
이다.&lt;/p&gt;

&lt;p&gt;(1) 민감도는 모델이 인풋이나 내부 변수들의 변화로부터 그 결과가 얼마나 영향을 받는지를 의미한다. SA는 데이터가 바뀌어도 내부 변수나 결과가 얼마나 안정되게 나오는지 검증하기 위해 사용된다. 모델의 데이터가 바뀌어도 모델이 안정적이라는 것은 믿음을 주기에 좋다. SA는 결과값 그 자체 대신 분산이 모델을 설명해 준다. 따라서 SA는 관계를 설명해 주는 것이 아니라 모델의 안정성을 테스트 하기 위해 사용된다. 또한 중요하지 않은 인풋 특징이나 시작점을 찾아서 없애는 도구로써 사용되고, 이후에 더 강력한 설명 방법을 사용할 수 있다.&lt;/p&gt;

&lt;p&gt;(2) LRP는 말단 노드에서 입력 노드쪽으로 역전파한다. 중요한 특징은 관계 보존이고, SA와는 반대로 어떤 특징이 결과에 가장 중요한지 알려준다.&lt;/p&gt;

&lt;p&gt;(3) 변수 중요도는 각 인풋 특징들이 얼마나 결과에 영향을 미치는지를 정량화한다. 모델 예측 에러의 증가는 특징의 중요도를 측정하기 위해 특징이 치환된 후에 계산된다. 중요한 특징을 치환한 뒤에는 모델 에러가 증가하고, 중요하지 않은 특징이 치환된다면 모델의 에러는 변하지 않는다. MCR(Model Class Reliance)은 모델에 상관없이 특징의 중요도를 계산할 수 있는 방법이다. SFIMP라는 특징 중요도를 계산하는 local 방법도 제안되었고, LOCO도 지역 변수 중요도를 사용했다.&lt;/p&gt;

&lt;h4 id=&quot;d-example-based-explanation&quot;&gt;d) Example-based Explanation&lt;/h4&gt;

&lt;p&gt;데이터셋에서 특정 예시들을 선택하여 모델의 행동을 설명한다. 이들은 어떤 모델이든 더 해석가능하게 만들기 때문에 대부분 모델에 상관없는 방법이다. model-agnostic과 다른 점은 내부 변수를 변경하거나 모델을 바꾸는 일 없이 예시들만 선택해서 모델을 설명한다는 점이다. 두 가지의 방법이 있는데,&lt;br /&gt;
(1) prototype and criticism&lt;br /&gt;
(2) Counterfactuals explanations&lt;br /&gt;
이다.&lt;/p&gt;

&lt;p&gt;(1) 프로토타입은 데이터들을 대표하는 예시들을 고르는 것이다. 각각이 어디에 속하는지는 프로토타입과 얼마나 비슷한지에 따라 결정되는데, 이것은 과일반화로 연결되기도 한다. 이를 피하기 위해 비평이라고 불리는, 프로토타입으로 잘 대표되지 않는 데이터들을 따로 뽑는다. MMD-critic은 프로토타임과 비평을 자동으로 찾아주는 비지도학습 방법이다.&lt;/p&gt;

&lt;p&gt;(2) 반사실 설명은 전체 논리를 설명할 필요 없이 반대 결론으로 이끌 수 있는 최소 조건만을 알려준다. 중요한 점은 설명해주는 것이 아니라 반대 예시를 찾는 것이다.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;model-agnostic의 최대 장점은 설명과 표현 수준에서 볼 때, 모델의 융통성이 높다는 점이다. 여러 다른 모델을 비교할 수 있다. 그렇지만 이들은 여러 근사 방법을 사용하여 표현되는데, 이 때 정확도가 떨어질 수도 있다. model-specific방법들이 더 직접적으로 표현되기 때문에 더 정확한 설명을 해줄 수도 있다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ML모델은 완전히 투명하거나/사후에 해석될 수 있다. 또한 local에서 해석되거나/global하게 해석될 수 있다. local 방법은 데이터에 초점을 맞추고, global 방법은 모델에 초점을 맞춘다. 신뢰 관점에서는, local 방법이 더 충실하다고 할 수 있다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Sat, 17 Aug 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/thesis/2019/08/17/XAI2.html</link>
        <guid isPermaLink="true">http://localhost:4000/thesis/2019/08/17/XAI2.html</guid>
        
        <category>XAI</category>
        
        
        <category>Thesis</category>
        
      </item>
    
      <item>
        <title>왜 XAI가 필요한가?</title>
        <description>&lt;h1 id=&quot;사람-vs-인공지능&quot;&gt;사람 vs. 인공지능&lt;/h1&gt;

&lt;h3 id=&quot;사람과-인공지능의-의사결정-과정&quot;&gt;사람과 인공지능의 의사결정 과정&lt;/h3&gt;

&lt;p&gt;사람 : 사진 10장정도로 학습 가능&lt;br /&gt;
인공지능 : 엄청난 수의 이미지 필요&lt;/p&gt;

&lt;h3 id=&quot;1-사람이-발달과정-중-얻는-직관&quot;&gt;1. 사람이 발달과정 중 얻는 직관&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;물리적 직관&lt;/li&gt;
  &lt;li&gt;심리적 직관&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;2-사람은-학습모델을-빠르게-구축&quot;&gt;2. 사람은 학습모델을 빠르게 구축&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;더 기초적인 문제로 조합&lt;/li&gt;
  &lt;li&gt;인과관계 이용&lt;/li&gt;
  &lt;li&gt;기존지식을 새로운 분야에 적용&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;3-사람은-빠르게-생각할-수-있다&quot;&gt;3. 사람은 빠르게 생각할 수 있다.&lt;/h3&gt;

&lt;h1 id=&quot;darpa&quot;&gt;DARPA&lt;/h1&gt;

&lt;h3 id=&quot;설명가능한-모델-구축&quot;&gt;설명가능한 모델 구축&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;심층 설명 학습 : DNN의 각 레이어에서 의미있는 특징을 학습하도록 함&lt;/li&gt;
  &lt;li&gt;해석 가능한 모델 : 해석 가능한 인과관계 모델 구축 ( 모델을 작은 조각들의 조합으로 표현)&lt;/li&gt;
  &lt;li&gt;모델 귀납 : 원래 있던 모델을 해석&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;이유-설명-인터페이스&quot;&gt;이유 설명 인터페이스&lt;/h3&gt;

&lt;h3 id=&quot;인과관계-모델--딥러닝--mrf&quot;&gt;인과관계 모델 : 딥러닝 + MRF&lt;/h3&gt;

&lt;h3 id=&quot;lrp--계층적-상관성-전파-입력의-어느-부분이-출력에-영향을-미쳤는지-알려줌&quot;&gt;LRP : 계층적 상관성 전파. 입력의 어느 부분이 출력에 영향을 미쳤는지 알려줌&lt;/h3&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;future-study&quot;&gt;Future Study&lt;/h1&gt;
&lt;p&gt;LRP, MRF, BPL&lt;/p&gt;
</description>
        <pubDate>Mon, 12 Aug 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/note/2019/08/12/WHYXAI.html</link>
        <guid isPermaLink="true">http://localhost:4000/note/2019/08/12/WHYXAI.html</guid>
        
        <category>XAI</category>
        
        
        <category>Note</category>
        
      </item>
    
      <item>
        <title>Peeking Inside the Black-Box ; A Survey on Explainable Artificial Intelligence (XAI) - 1</title>
        <description>&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;4차 산업혁명의 여명으로, 우리의 일상에서 더 알고리즘적인 사회에 다가가갈 수 있게 하는 인공지능이 빠르게 적용되고 있다. 그러나, 전례 없는 발전에도, AI시스템의 가장 큰 문제점은 투명성이 없다는 것이다. 당연히 이 시스템의 블랙박스인 특성이 강력한 예측을 하게 해주지만, 직접적으로는 설명될 수 없다. 이 문제는 XAI에 대한 논쟁을 야기했다. 이 연구분야는 AI시스템의 믿음과 투명성을 향상시킬 것이라는 희망을 갖고 있다. AI가 계속해서 이어나가려면 XAI는 필수요소이다. 이 서베이는 관심있는 연구자들과 연습자들이 초기 단계이지만 빠르게 성장하는 XAI의 주요 양상들을 배울 수 있도록 하는 입구가 될 것이다. 주제, 논쟁 트렌드, 주 연구 궤적을 문헌들을 통해서 현존하는 접근법을 리뷰할 것이다.&lt;/p&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;h3 id=&quot;context&quot;&gt;Context&lt;/h3&gt;

&lt;p&gt;현재 인공지능은 덜 중요한 일에는 매우 많이 쓰이고 있지만, 매우 중요한 일에는 설명이 되지 않기 때문에 쓰이지 않고 있다. 이것이 XAI가 중요한 이유이다. XAI는 더 투명한 AI를 만들기 위한 발걸음이다. XAI는 좋은 성능은 잃지 않으면서도 설명 가능한 모델을 만들기 위한 연구이다.&lt;/p&gt;

&lt;h3 id=&quot;xais-landscape-dynamic&quot;&gt;XAI’s Landscape Dynamic&lt;/h3&gt;

&lt;p&gt;최근 많은 컨퍼런스에서 XAI를 비중있게 다루었는데, 그 중 눈에 띠는 연구 그룹은 FAT와 DARPA이다. FAT(Fairness, Accountability, Transparency) 는 많은 참가자와 논문을 포함했다. DARPA는 2017년에 XAI프로그램을 설립했고, 2021년까지 11개의 프로젝트를 진행한다. DARPA는 미 국방부의 지원을 받지만, 다양한 학문 기관에서 온 연구자들과 함께 다양한 팀을 꾸린다.&lt;/p&gt;

&lt;h3 id=&quot;contribution-and-organization&quot;&gt;Contribution and Organization&lt;/h3&gt;

&lt;p&gt;이 논문은 XAI의 기반을 확실히 해 놓아야 다음 연구로 진전이 될 수 있을 것을 주장한다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;XAI가 왜 필요한지&lt;/li&gt;
  &lt;li&gt;XAI 접근법 오버뷰&lt;/li&gt;
  &lt;li&gt;미래 연구 분야&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;를 이 논문이 알려준다.&lt;/p&gt;

&lt;h1 id=&quot;background&quot;&gt;Background&lt;/h1&gt;

&lt;h3 id=&quot;understanding-xai--a-contextual-definition&quot;&gt;Understanding XAI : A Contextual Definition&lt;/h3&gt;

&lt;p&gt;XAI의 필요성은 1970년대부터 있어왔다. 그러나 AI에 대한 관심이 줄어들었는데, 그 이후 AI의 방향이 모델 구현하는 쪽으로 변하면서 예측 능력에 가려 덜 중요하다고 생각되었다. 2004년에 XAI라는 용어를 처음 썼다. AI가 발전하자, 그에 따라 XAI라는 용어도 다시 급부상했다.&lt;/p&gt;

&lt;p&gt;하지만 XAI에 대한 정확한 정의는 없다. 기관에 따라 다른 정의를 갖고 있다. DARPA에 따르면, “정확도를 유지하면서 설명 가능한 모델을 만드는 것, 유저에게 설명할 수 있어야 하며, 그로 인해 AI를 제어할 수 있어야 한다.” 이다.&lt;/p&gt;

&lt;p&gt;사실 과학 문헌에서는 explainable보다는 interpretable이 더 많이 쓰이지만, 퍼블릭에서는 반대이므로 XAI라는 이름이 붙었다.&lt;/p&gt;

&lt;h3 id=&quot;using-xai-the-need-and-the-application-opportunities&quot;&gt;USING XAI: THE NEED AND THE APPLICATION OPPORTUNITIES&lt;/h3&gt;

&lt;p&gt;XAI가 필요한 4가지 이유&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;결과가 옳다고 주장하기 위해&lt;/li&gt;
  &lt;li&gt;결과를 조절하기 위해&lt;/li&gt;
  &lt;li&gt;결과를 개선하기 위해&lt;/li&gt;
  &lt;li&gt;새로운 것을 발견하기 위해&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;그러나 구글 연구원 노빅은 사람도 자신의 선택을 설명하는 것을 잘 못한다고 했다. 설명력은 매우 중요하지만, 필수적이지는 않다. 설명을 잘 하는 모델이 정확도는 더 떨어질 수도 있기 때문이다. 게다가 비싸다. 그래서 특정 상황일 때만 XAI를 쓰는 것이 적절하다. AI 알고리즘의 복잡도로부터 발생되는 불투명도의 정도가 높을 때 (알고리즘이 복잡해서 우리가 못알아들을 때), 그리고 응용 분야의 에러 저항력이 낮을 때 (조금이라도 틀리면 치명적일 때) 사용해야 한다.&lt;/p&gt;

&lt;p&gt;XAI는 꼭 필요하지만, 아직 기술적으로 발전되지 않았다. 기존 설명 가능한 rule-based 모델이 있었지만 이를 DNN에 적용하기는 어렵다.&lt;/p&gt;

&lt;p&gt;80년도에 개발된 전문가 시스템은 설명가능하지만 성능이 별로였다. 그 뒤에 제안된 DNN은 엄청난 성능을 자랑하지만 복잡도가 높아 설명이 어렵다.&lt;/p&gt;

&lt;p&gt;다만, 모든 모델들이 같은 레벨의 불투명도를 가지는 것은 아니다. 보통은 정확도와 설명가능도의 trade off를 가지며, 대체로 설명가능도가 높으면 정확도가 낮다.&lt;/p&gt;

&lt;p&gt;XAI의 기초가 되는 분야는 Data Science, AI/ML, Human Science (사람은 어떻게 설명하나?), HCI(사람이 기계를 이해하고 믿으려면 어떻게 해야하나?) 이다.&lt;/p&gt;

&lt;h1 id=&quot;review&quot;&gt;Review&lt;/h1&gt;

&lt;h3 id=&quot;related-surveys&quot;&gt;Related Surveys&lt;/h3&gt;

&lt;p&gt;XAI에 대한 연구는 늘어나고 있지만, 총정리 서베이와 종류를 구별한 서베이는 별로 없다. 이 논문은 이전 연구들과는 다르게 여러 다른 관점에서 XAI 연구 성과에 대한 오버뷰를 제시할 것이다. 전체론과 투명성에 집중할 것이다.&lt;/p&gt;

&lt;p&gt;이후에는 4개의 주축으로 나누어 설명할 것인데, 다음과 같다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;axis 1 : XAI Methods Taxonomy - Explainability Strategies&lt;/li&gt;
  &lt;li&gt;axis 2 : XAI Measurement - Evaluating Explanations&lt;/li&gt;
  &lt;li&gt;axis 3 : XAI Perception - Human in the Loop&lt;/li&gt;
  &lt;li&gt;axis 4 : XAI Antithesis - Explain or Predict&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Sun, 11 Aug 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/thesis/2019/08/11/XAI1.html</link>
        <guid isPermaLink="true">http://localhost:4000/thesis/2019/08/11/XAI1.html</guid>
        
        <category>XAI</category>
        
        
        <category>Thesis</category>
        
      </item>
    
      <item>
        <title>CV Lecture 10 - High-Dynamic-Range Rendering</title>
        <description>&lt;h1 id=&quot;high-dynamic-range-imaging&quot;&gt;High-Dynamic-Range Imaging&lt;/h1&gt;

&lt;h3 id=&quot;tone-mapping-hdrrightarrowldr-display&quot;&gt;Tone Mapping (HDR$\rightarrow$LDR display)&lt;/h3&gt;

&lt;p&gt;이미지는 HDR인데 디스플레이가 LDR이라면 어쩔 수 없이 LDR로 봐야 한다. 그런데 동시에 디테일도 보존하고 싶다. tone mapping은 비록 LDR의 이미지이지만, 그것을 보고 현실 세계같이 인지할 수 있도록 만들자는 것으로부터 시작한다.&lt;/p&gt;

&lt;p&gt;먼저 사람의 눈이 다양한 범위에 대해 어떻게 인지하는지에 대해 알아야 한다. 사람의 눈은 대비에 민감하다. 즉, 1:2의 대비와 100:200의 대비를 같다고 인지한다. 또한 망막과 동공에서 빛을 수용할 수 있고, 신경$\cdot$화학적으로 수용할 수 있다. 그리고 공간적 주파수에 따라 민감도가 다르다.&lt;/p&gt;

&lt;h3 id=&quot;contrast-sensitivity-function-csf&quot;&gt;Contrast Sensitivity Function (CSF)&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://ars.els-cdn.com/content/image/3-s2.0-B9780121197926501182-f62-03-9780121197926.gif&quot; alt=&quot;scale=0.5&quot; /&gt;
&lt;a href=&quot;https://ars.els-cdn.com/content/image/3-s2.0-B9780121197926501182-f62-03-9780121197926.gif&quot;&gt;Source&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;위 그래프는 공간적 주파수에 따른 민감도 그래프이다. 초반에는 주파수가 로그함수로 높아지면 민감도가 올라가지만, 특정 지점 이후에는 급격히 줄어든다. 즉, 주파수가 매우 높으면 인지할 수 없다는 말이다. 따라서 출력하는 주파수를 피크점 이전까지로 해야 한다. 동시에 색상은 유지해야 한다.&lt;/p&gt;

&lt;h3 id=&quot;gamma-compression&quot;&gt;Gamma Compression&lt;/h3&gt;

&lt;p&gt;만약 원본이 1:10000의 대비를 가지고 있고, 화면은 1:100의 대비를 가지고 있다면, 단순히 $x\rightarrow x^{\gamma}$를 이용해서 변환시킬 수 있다. (주파수가 로그로 증가할 때 선형 증가하므로, 우리는 지수함수를 써야한다)그러나 이렇게 하면 대비는 유지되지만, 색상 이상이 일어난다&lt;/p&gt;

&lt;h3 id=&quot;gamma-compression-on-intensity&quot;&gt;Gamma Compression on Intensity&lt;/h3&gt;

&lt;p&gt;gamma compression on intensity은 이미지에서 intensity와 color 맵을 추출하여 이용하는 방법이다. 이렇게 하면 하면 색상은 유지되지만, 디테일이 블러된다. 그렇다면 고주파수는 그대로 두고, 저주파수의 대비를 줄이는 방법을 사용할 수 있다. 이미지에서 저주파수, 고주파수 맵과 color 맵을 이용하는데, 이러면 halo artifact가 생긴다. 이 현상은 주파수가 높은 경계 부분만 빛의 테두리가 나타나는 왜곡 현상이다.&lt;/p&gt;

&lt;h3 id=&quot;durand-bilateral-filtering&quot;&gt;Durand Bilateral Filtering&lt;/h3&gt;

&lt;p&gt;이 방법은 경계를 블러링 하지 않는 방법이고, non-linear한 방법이다. 이미지에서 large scale, detail, color 맵을 추출하여 이용한다.&lt;/p&gt;

&lt;p&gt;우선 가우시안 필터링부터 시작한다. 가우시안 필터링은 가우시안 함수 모양의 마스크를 이미지에 컨볼루션하는 방법이다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J = f \otimes I&lt;/script&gt;

&lt;p&gt;($J$는 결과 이미지, $f$는 가우시안 마스크, $I$는 이미지)&lt;/p&gt;

&lt;p&gt;만약 $I$가 계단 모양 이미지였다면, 결과물 $J$는 블러되어 경계 부분이 경사진 모양으로 나타난다. 만약 우리가 이미지의 $x$지점($I(x)$)을 관찰한다고 하고, 그 점은 경계 부근이라고 하자. 그럼 가우시안 필터는 그 주변 픽셀들에 마스크를 씌워 결과$J(x)$를 낼 것인데, 경계의 반대 부분에 있는 픽셀을 $I(\xi)$라고 하자. 이 점 $I(\xi)$는 결과물 $J(x)$를 오염시킨다. (경계를 보존하지 않고 스무딩 시키므로)&lt;/p&gt;

&lt;p&gt;이 때 차이 나는 intensity만큼 패널티 $g$를 준다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J(x) = \frac{1}{k(x)}\sum_{\xi}{f(x, \xi)g(I(\xi)-I(x))I(\xi)}&lt;/script&gt;

&lt;p&gt;($f$는 $x$와 $\xi$의 거리이고, $g$는 intensity의 차이, $k$는 &lt;script type=&quot;math/tex&quot;&gt;k(x) = \sum_{\xi}f(x, \xi)g(I(\xi)-I(x))&lt;/script&gt;이다.) 이 결과로는 경계는 스무딩 되지 않고 나머지 노이즈만 스무딩된 결과가 나오게 된다. 여기서 사용되는 필터가 Bilateral Filter이다.&lt;/p&gt;

&lt;p&gt;이제 Durand Bilateral Filtering의 전체 과정을 설명할 수 있다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Input 이미지는 대비가 매우 높기 때문에 intensity와 color맵을 추출한다.&lt;/li&gt;
  &lt;li&gt;intensity 맵은 Fast Bilateral Filter를 이용해 Large scale로 변환시킨다.&lt;/li&gt;
  &lt;li&gt;Intensity에서 Large scale을 빼서 detail을 얻는다.&lt;/li&gt;
  &lt;li&gt;그 후 Large scale에서 대비를 줄여 이를 detail과 color 맵과 합쳐서 결과물을 만든다.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;fattals-gradient-domain-hdr-tone-mapping-operator&quot;&gt;Fattal’s Gradient Domain HDR Tone-mapping Operator&lt;/h3&gt;

&lt;p&gt;이 방법의 절차는 다음과 같다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;HDR 이미지에서 gradient를 구한다.&lt;/li&gt;
  &lt;li&gt;강한 gradient를 줄인다.&lt;/li&gt;
  &lt;li&gt;줄인 gradient에서 LDR 이미지를 재건한다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;먼저 gradient를 알아야 한다. 여기서의 gradient는 다인자 함수를 미분한 것이다. 예를 들어 $f(x, y)$의 gradient는&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\triangledown f=\left(\frac{df}{dx}, \frac{df}{dy}\right) = \left(\frac{f(x+h)-f(x)}{h}, \frac{f(y+h)-f(y)}{h}\right)&lt;/script&gt;

&lt;p&gt;이다. 우리는 이미지를 가지고 하므로,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{df}{dx}\approx f(x+1,y)-f(x,y)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{df}{dy}\approx f(x, y+1)-f(x,y)&lt;/script&gt;

&lt;p&gt;가 된다.&lt;/p&gt;

&lt;p&gt;절차를 다시 설명해보면 이렇다.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;우리가 HDR scanline을 가지고 있다고 생각해보자. (1차원)&lt;/li&gt;
  &lt;li&gt;$H(x)$는 $I(x)$의 로그이다. &lt;script type=&quot;math/tex&quot;&gt;H(x) = \log(\text{scanline})&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;$H(x)$의 변화율을 구한다. ($H’(x)$)&lt;/li&gt;
  &lt;li&gt;$H’(x)$를 줄인다. $G(x) = H’(x)\Phi(x)$&lt;/li&gt;
  &lt;li&gt;$I(x)$를 다시 재건한다. $I(x) = C + \int_0^xG(t)$&lt;/li&gt;
  &lt;li&gt;아까 로그를 씌웠으니 역인 지수함수를 씌워준다. $\exp(I(x))$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;여기서 우리의 이미지는 2차원이므로, $G(x,y) = \triangledown H(x,y)\Phi(x,y)$를 써야 한다. 2차원으로 넘어오면서 생기는 또다른 문제점은, $G=\triangledown I$를 만족하는 $I$가 존재하지 않을수도 있다는 점이다. 따라서 우리는 적분을 직접 하는 대신,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\iint F(\triangledown I, G)dxdy&lt;/script&gt;

&lt;p&gt;를 최소화시키는 $I$를 구하면 되는데, 이때 &lt;script type=&quot;math/tex&quot;&gt;F(\triangledown I, G) = \|\triangledown I-G\|^2 = \left(\frac{\partial I}{\partial x}-G_x\right)^2+\left(\frac{\partial I}{\partial y}-G_y\right)^2&lt;/script&gt; 이다.&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\iint F(\triangledown I, G)dxdy&lt;/script&gt;가 Euler-Lagrange equation (&lt;script type=&quot;math/tex&quot;&gt;\frac{\partial F}{\partial I} = \frac{d}{dx}\frac{\partial F}{\partial I_x}+\frac{d}{dy}\frac{\partial F}{\partial I_y}&lt;/script&gt;)을 따른다고 해보자. 그러면 Poisson Equation을 얻을 수 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\triangle I = \triangledown^2I=\text{div}G&lt;/script&gt;

&lt;p&gt;($\triangle$은 라플라스 연산자)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\triangle I=\triangledown^2I=\triangledown\cdot\triangledown I=\frac{\partial^2I}{\partial x^2}+\frac{\partial^2I}{\partial y^2}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{div}G=\triangledown\cdot G=\frac{\partial G_x}{\partial x}+\frac{\partial G_y}{\partial y}&lt;/script&gt;

&lt;p&gt;이제 attenuation factor $\Phi$에 대해 알아볼 것이다. 우선 이미지 피라미드($H_0, H_1, \cdots, H_d$)를 만든다. $H_0$은 원본 이미지고 $H_d$는 가장 멀리서 본 이미지이다. 각 레벨 $k$에서 $\triangledown H_k$를 다음과 같이 정의한다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\triangledown H_k = \left(\frac{H_k(x+1, y)-H_k(x-1, y)}{2^{k+1}}, \frac{H_k(x, y+1)-H_k(x, y-1)}{2^{k+1}}\right)&lt;/script&gt;

&lt;p&gt;각 레벨 $k$에서의 scaling factor $\phi_k(x,y)$는 다음과 같이 정의된다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\phi_k(x,y)=\frac{\|\triangledown H_k(x,y)\|}{\alpha}\left(\frac{\alpha}{\|\triangledown H_k(x,y)\|}\right)^{\beta}&lt;/script&gt;

&lt;p&gt;원본 이미지의 gradient attenuation Function $\Phi(x, y)$는 top-down 방식으로 $\phi_k(x,y)$에서부터 전파되어 계산된다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Phi_d(x,y) = \phi_d(x,y)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Phi_k(x,y) = L(\Phi_{k+1})(x,y)\phi_k(x,y)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Phi(x,y) = \phi_0(x,y)&lt;/script&gt;

&lt;p&gt;($L$은 linear interpolation을 이용한 upsampling operator이다)&lt;/p&gt;

&lt;p&gt;그다음은 color 계산법이다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;C_{out}=\left(\frac{C_{in}}{L_{in}}\right)^sL_{out}&lt;/script&gt;

&lt;p&gt;$L_{in}$과 $L_{out}$은 HDR compression 전과 후의 luminance이고, $s$는 채도를 조절한다.&lt;/p&gt;

&lt;h1 id=&quot;extra-reading&quot;&gt;Extra Reading&lt;/h1&gt;

&lt;h3 id=&quot;poisson-reconstruction&quot;&gt;Poisson Reconstruction&lt;/h3&gt;

&lt;p&gt;원래 이미지 $f^* $가 있고, 모양을 흉내낼 이미지 $\mathbf{v}$(사실은 이미지가 아니고 vector field이다)가 있을 때, 원래 이미지 $f^* $에서 모양을 흉내내고 싶은 부분 $\Omega$(hole)를 뚫는다. 이 때 에러가 최소화되는 이미지 $f$를 찾는 방법이 poisson reconstruction이다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\min_f\iint_{\Omega}\|\triangledown f-\mathbf{v}\|^2&lt;/script&gt;

&lt;p&gt;with &lt;script type=&quot;math/tex&quot;&gt;f\mid_ {\partial\Omega}=f^* \mid_ {\partial\Omega}&lt;/script&gt;, $\partial\Omega$는 hole $\Omega$의 boundary&lt;/p&gt;

&lt;p&gt;이 에러를 최소화하는 $f$를 찾으면 되는데, 이미지는 이산 함수이므로 우리는 이산 버전으로 최적화를 바꿔야 한다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\min_{f |_ {\Omega}}\sum_{\langle p, q\rangle\cap\Omega\neq0}\|(f_p-f_q)-v_{pq}\|^2&lt;/script&gt;

&lt;p&gt;with &lt;script type=&quot;math/tex&quot;&gt;f_p=f^* _ p&lt;/script&gt;, for all &lt;script type=&quot;math/tex&quot;&gt;p\in\partial\Omega&lt;/script&gt;, where &lt;script type=&quot;math/tex&quot;&gt;f\mid_ {\Omega}=\{f_p, p\in\Omega\}, v_{pq}:= \mathbf{v}\left(\frac{p+q}{2}\right)\cdot\vec{pq}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;우리는 모든 $p\in\Omega$에 대해 아래의 방정식을 풀면 된다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\left| N_p\right| f_p - \sum_{q\in N_p\cap\Omega}f_q=\sum_{q\in N_p\cap\partial\Omega}f^* _ q+\sum_{q\in N_p}v_{pq}&lt;/script&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\mid N_p\mid&lt;/script&gt;는 $p$의 4방향 이웃 $N_p$의 cardinality를 의미한다.&lt;/p&gt;

&lt;p&gt;이를 선형방정식으로 만들면,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
A\mathbf{x}=b\leftrightarrow
\left[\begin{matrix}
4 &amp; -1 &amp; &amp; &amp; -1 &amp; &amp; &amp; &amp; &amp; &amp; \\
-1 &amp; 4 &amp; -1 &amp; &amp; &amp; -1 &amp; &amp; &amp; &amp; &amp; \\
 &amp; -1 &amp; 4 &amp; -1 &amp; &amp; &amp; -1 &amp; &amp; &amp; &amp; \\
 &amp; &amp; -1 &amp; 4 &amp; -1 &amp; &amp; &amp; -1 &amp; &amp; &amp; \\
 -1 &amp; &amp; &amp; -1 &amp; 4 &amp; -1 &amp; &amp; &amp; -1 &amp; &amp; \\
 &amp; -1 &amp; &amp; &amp; -1 &amp; 4 &amp; -1 &amp; &amp; &amp; -1 &amp; \\
 &amp; &amp; -1 &amp; &amp; &amp; -1 &amp; 4 &amp; -1 &amp; &amp; &amp; -1  \\
 &amp; &amp; &amp; -1 &amp; &amp; &amp; -1 &amp; 4 &amp; -1 &amp; &amp; \\
 &amp; &amp; &amp; &amp; -1 &amp; &amp; &amp; -1 &amp; 4 &amp; -1 &amp; \\
 &amp; &amp; &amp; &amp; &amp; &amp; -1 &amp; &amp; -1 &amp; 4 &amp; -1 \\
 &amp; &amp; &amp; &amp; &amp; &amp; &amp; -1 &amp; &amp; -1 &amp; 4 &amp; \\
 \end{matrix}\right]
 \left[\begin{matrix}f_1\\f_2\\f_3\\f_4\\ \\ \vdots \\ \\ f_{n-3}\\f_{n-2}\\f_{n-1}\\f_n\end{matrix}\right] =
 \left[\begin{matrix}b_1\\b_2\\b_3\\b_4\\ \\ \vdots \\ \\ b_{n-3}\\b_{n-2}\\b_{n-1}\\b_n\end{matrix}\right] %]]&gt;&lt;/script&gt;

&lt;p&gt;라고 할 수 있는데, &lt;strong&gt;&lt;em&gt;$A_p$는 $f$의 라플라시안을 의미하고, $b$는 boundary condition $f^* $에서 velocity field $v$의 값을 의미한다?&lt;/em&gt;&lt;/strong&gt; 결국 이 식을 통해서 poisson equation $\triangle f=\text{div}\mathbf{v}$를 푼 것이다.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;이번 내용은 수학이 너무 많이 나와서 어렵다…나중에 다시 봐야 할듯&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;terms&quot;&gt;Terms&lt;/h2&gt;
&lt;p&gt;bilateral : 쌍방의&lt;br /&gt;
attenuate : 줄이다&lt;br /&gt;
conservative : 보존되는&lt;/p&gt;
</description>
        <pubDate>Mon, 05 Aug 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/lecture/computervision/2019/08/05/Lecture10.html</link>
        <guid isPermaLink="true">http://localhost:4000/lecture/computervision/2019/08/05/Lecture10.html</guid>
        
        
        <category>Lecture</category>
        
        <category>ComputerVision</category>
        
      </item>
    
      <item>
        <title>CV Lecture 9 - High-Dynamic-Range Imaging</title>
        <description>&lt;h1 id=&quot;high-dynamic-range-imaging&quot;&gt;High-Dynamic-Range Imaging&lt;/h1&gt;

&lt;h3 id=&quot;high-dynamic-range-imaging-ldrrightarrowhdr&quot;&gt;High Dynamic Range Imaging (LDR$\rightarrow$HDR)&lt;/h3&gt;

&lt;p&gt;현실은 dynamic range가 매우 높다. 이를 낮은 대비를 가지는 사진으로 옮기려면 전체 range의 간격을 좁히거나, 특정 범위만 옮기는 방법이 있다. 이 때 어떤 범위를 저장할 것인가는 노출(&lt;a href=&quot;https://yeonjeejung.github.io/lecture/computervision/2019/07/28/Lecture7.html&quot;&gt;exposure&lt;/a&gt;)이 결정하는데, 이 노출을 조절하는 방법에는 서터 속도, 조리개, 감도, 중성 농도 필터 등이 있다.&lt;/p&gt;

&lt;p&gt;셔터 속도의 조절 범위는 30sec~1/4000sec이고, 확실한 방법이고 선형적인 효과를 가져오지만, 긴 노출에 대해서는 노이즈가 섞일 우려가 있다.&lt;/p&gt;

&lt;p&gt;조리개의 조절 범위는 f/1.4~f/22이고 DOF가 변경된다는 단점이 있고, 최후의 방법으로 유용하다.&lt;/p&gt;

&lt;p&gt;감도의 조절 범위는 100~1600이고, 노이즈가 생긴다는 단점이 있다. 조리개값과 마찬가지로 최후의 방법으로 유용하다.&lt;/p&gt;

&lt;p&gt;중성 농도 필터는 4농도까지 조절할 수 있으며, 쌓을 수도 있다. 단점은 완벽히 중성은 아니고 색감 이동이 있을 수 있고, 정확하지는 않다. 장점은 번쩍거리는 빛에도 사용할 수 있다는 점이고, 최후의 보충 방법으로 사용될 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;response-curve&quot;&gt;Response Curve&lt;/h3&gt;

&lt;p&gt;만약 response curve를 알고 있다면 어떤 노출값을 갖는 사진을 찍어야 원하는 빛의 양을 얻을 수 있는지 알 수 있다. response curve의 역 값을 보면 된다. response curve를 얻는 방법에는 두가지가 있다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;장면의 빛의 양(scene luminance)을 조절한 뒤 픽셀값을 본다.&lt;/li&gt;
  &lt;li&gt;노출값을 조절한 뒤 픽셀값을 본다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;가장 좋은 방법은 노출값을 조절한 뒤 많은 범위의 픽셀값을 고루 관찰하는 것이다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Z=f(H)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H=E\cdot\Delta t&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\log(H)=\log(E)+\log(\Delta t)&lt;/script&gt;

&lt;p&gt;$Z$ 는 픽셀값, $H$는 노출, $E$는 빛의 양, $\Delta t$는 노출시간이다.&lt;/p&gt;

&lt;p&gt;$g(Z)$가 response curve의 역에 $\log$를 취한 함수라고 하자.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Z=f(H)\rightarrow H=f^{-1}(Z)\rightarrow \log H=\log f^{-1}(Z)\rightarrow\log H=g(Z)&lt;/script&gt;

&lt;p&gt;$j$번째 이미지의 $i$번째 픽셀에 대해서&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;g(Z_{ij})=\log(E_i)+\log(\Delta t_j)&lt;/script&gt;

&lt;p&gt;임을 원하기 때문에&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{i=1}^N\sum_{j=1}^P[\log(E_i)+\log(\Delta t_j)-g(Z_{ij})]^2 + \lambda\sum_{z=Z_{min}}^{Z_{max}}g''(z)^2&lt;/script&gt;

&lt;p&gt;을 최소화시키면 된다. 앞의 항은 Data term, 뒤의 항은 Regularization term이다.&lt;/p&gt;

&lt;p&gt;radiance를 재건하려면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\log E_i=\frac{\sum_{j=1}^Pw(Z_{ij})(g(Z_{ij})-\log(\Delta t_j))}{\sum_{j=1}^Pw(Z_{ij})}&lt;/script&gt;

&lt;p&gt;을 사용하면 된다. $w(Z_{ij})$는 해당 픽셀의 가중치이고, 해당 조각들의 노이즈 관여를 조절한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://cybertron.cg.tu-berlin.de/eitz/hdr/exposure_series_response_curve.jpg&quot; alt=&quot;&quot; /&gt;
&lt;a href=&quot;http://cybertron.cg.tu-berlin.de/eitz/hdr/exposure_series_response_curve.jpg&quot;&gt;Source&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;response curve를 그리기 위해 사용하는 이미지&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://cybertron.cg.tu-berlin.de/eitz/hdr/response_curve.jpg&quot; alt=&quot;&quot; /&gt;
&lt;a href=&quot;http://cybertron.cg.tu-berlin.de/eitz/hdr/response_curve.jpg&quot;&gt;Source&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;위 이미지를 통해 그린 response curve&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://vision.gel.ulaval.ca/~jflalonde/cours/4105/h14/tps/results/tp5/raziehtoony/Image/Radiance%20Map.jpg&quot; alt=&quot;&quot; /&gt;
&lt;a href=&quot;http://vision.gel.ulaval.ca/~jflalonde/cours/4105/h14/tps/results/tp5/raziehtoony/Image/Radiance%20Map.jpg&quot;&gt;Source&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;디지털 카메라를 이용해서 장면 밝기 (scene radiance)를 측정한 radiance map&lt;/p&gt;

&lt;h3 id=&quot;image-registration&quot;&gt;Image Registration&lt;/h3&gt;

&lt;p&gt;서로 다른 노출을 갖는 이미지를 비교하기 위한 방법이다. Median-Threshold Bitmap (MTB)를 이용해서 흑백으로 만들고, &lt;strong&gt;&lt;em&gt;그 차이를 최소화하는 translation을 찾는다. 그리고 피라미드를 이용해서 더 촉진한다?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;file-formats&quot;&gt;File Formats&lt;/h3&gt;

&lt;p&gt;.pfm : Portable Float Map. 채널당 32비트 float을 사용하고, 3채널을 저장한다. 따라서 픽셀당 96비트를 사용한다.&lt;/p&gt;

&lt;p&gt;.pic, .hdr : Radiance Format. 픽셀당 32비트를 사용한다. (채널당 8비트, 지수에 8비트)&lt;/p&gt;

&lt;p&gt;.exr : ILM’s OpenEXR. 비손실 압축을 지원한다. half-precision float (채널당 16비트)을 사용하며, 멀티채널을 지원한다.&lt;/p&gt;

&lt;h3 id=&quot;high-dynamic-range-display-hdrrightarrowhdr&quot;&gt;High Dynamic Range Display (HDR$\rightarrow$HDR)&lt;/h3&gt;

&lt;p&gt;Sunnybrook HDR Display는 저해상도의 광원과 2개의 8비트 모듈레이터를 이용하여 HDR 영상을 띄울 수 있다. LED 백라이트와 LCD 스크린을 결합하여 결과를 만들어 낸다.&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;terms&quot;&gt;Terms&lt;/h2&gt;
&lt;p&gt;dynamic range : 증폭 회로 등에서, 다룰 수 있는 가장 큰 신호와 가장 작은 신호와의 크기의 비율을 데시벨로 나타낸 것&lt;br /&gt;
precision : 정밀도&lt;/p&gt;
</description>
        <pubDate>Sun, 04 Aug 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/lecture/computervision/2019/08/04/Lecture9.html</link>
        <guid isPermaLink="true">http://localhost:4000/lecture/computervision/2019/08/04/Lecture9.html</guid>
        
        
        <category>Lecture</category>
        
        <category>ComputerVision</category>
        
      </item>
    
      <item>
        <title>CV Lecture 8 - Light-Field Imaging</title>
        <description>&lt;h1 id=&quot;lightfields&quot;&gt;Lightfields&lt;/h1&gt;

&lt;h3 id=&quot;ray&quot;&gt;Ray&lt;/h3&gt;

&lt;p&gt;Ray는 5D 정보로 이루어져 있다.(3D 위치, 2D 방향) plenoptic 기술은 이런 5D정보를 가진 ray들을 활용하여 다양한 영상처리를 할 수 있다.&lt;/p&gt;

&lt;p&gt;반면, line은 4D의 정보를 가지고 있다. (2D 위치, 2D 방향) &lt;strong&gt;&lt;em&gt;discretize 한 후에 다시 interpolate 할 수 있다?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;image&quot;&gt;Image&lt;/h3&gt;
&lt;p&gt;이미지란, 눈(또는 렌즈)이라는 한 점으로 들어오는 빛의 정보들을 수치화한 것이다. 앞 단원들에서 계속 나오던 &lt;a href=&quot;https://yeonjeejung.github.io/lecture/computervision/2019/07/07/Lecture3.html&quot;&gt;이미지 평면&lt;/a&gt;도 결국에는 사물에서 반사된 빛이 눈이라는 한 점으로 들어오는 도중의 한 평면과의 교점들이라고 할 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;object&quot;&gt;Object&lt;/h3&gt;

&lt;p&gt;하지만 대상에서 반사되는 모든 빛을 알 수는 없다. 이미지란 대상의 모든 곳에서 반사되는 빛 중 렌즈로 들어오는 빛만 캡쳐한 것이다. 그러므로 정보의 손실이 일어나게 되는데, 이 정보들 (4D정보들)을 다 모으면 렌즈 방향이 아닌 다른 방향에서 어떻게 보일지도 알 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;lumigraph&quot;&gt;Lumigraph&lt;/h3&gt;

&lt;p&gt;Lumigraph는 위에서 설명한 빛의 정보를 저장하는 방법이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.lightfield-info.com/lightfield-image.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.lightfield-info.com/lightfield-image.jpg&quot;&gt;Source&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;2개의 평면을 이용할 수 있다. 한 평면은 대상이 존재하는 평면($u, v$평면)이고, 다른 평면은 카메라 평면($s, t$평면)이다. 카메라는 카메라 평면 위에서 움직이고, 각 이미지 평면을 통과하는 빛을 캡쳐한다.&lt;/p&gt;

&lt;p&gt;캡쳐 방법에는 두 가지가 있는데, &lt;strong&gt;&lt;em&gt;카메라를 $s, t$평면에서 움직이는 방법(lightfield rendering)과 카메라를 아무데나 움직인 뒤 rebinnig하는 방법(lumigraph)이 있다?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;우리가 알고 있는 평면은 2개이기 때문에 $u, v$평면의 각 점에 $s, t$평면의 모든 정보를 넣을 수도 있고, 반대로 할 수도 있다. 첫 번째 경우는 off-axis perspective view와 같고, 두 번째의 경우는 reflectance map과 같다.&lt;/p&gt;

&lt;h1 id=&quot;light-field-rendering&quot;&gt;Light Field Rendering&lt;/h1&gt;

&lt;p&gt;렌더링을 하기 위해서는 특정 위치에서 바라볼 때 빛이 어디서 어떻게 들어오는지의 정보를 lumigraph로부터 알아내야 한다. 우선 렌더링 시점은 카메라 평면보다 뒤쪽이다. 내 위치에서 모든 방향으로부터 들어오는 빛의 정보가 필요한데, 카메라가 모든 카메라평면에 대해 존재하는 것이 아니기 때문에 모든 점에 대해서 정보를 가지고 있는 것이 아니므로 정보가 없는 점은 interpolation을 이용해서 정보를 만들어낸다. 이 때는 quadrilinear interpolation을 사용한다. 원래의 bilinear interpolation은 주변 네 개의 점을 이용한다. 식은 다음과 같다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;s = \alpha_0s_0+\alpha_1s_1 \text{  where  } \alpha_0+\alpha_1=1&lt;/script&gt;

&lt;p&gt;quadrilinear interpolation은 점 $(s, t_0)$와 $(u_0, v_0)$가 주어질 때 $s_0$와 $s_1$에 대해서 계산해야 하기 때문에&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L(s, t_0, u_0, v_0)=\alpha_0L(s_0, t_0, u_0, v_0)+\alpha_1L(s_1, t_0, u_0, v_0)&lt;/script&gt;

&lt;p&gt;이 되는데, 이 계산을 $s, t, u, v$에 대해서 다 해야하므로&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L(s, t, u, v)=\sum_{i=0}^1\sum_{j=0}^1\sum_{k=0}^1\sum_{l=0}^1\alpha_i\beta_j\gamma_k\delta_lL(s_i, t_j, u_k, v_l)&lt;/script&gt;

&lt;p&gt;이라는 식으로 설명할 수 있다.&lt;/p&gt;

&lt;p&gt;표현을 쉽게 하기 위해 2D Ray Space를 사용할 수 있는데, 편의를 위해 특정 $t$와 특정 $v$가 주어졌다고 할 때의 $s, u$의 조합을 그려보면 $n_s\times n_u$개의 조합이 생긴다.(가로가 $u$, 세로가 $s$) 이를 2D평면으로 옮긴 것으로, 하나의 조합은 하나의 점으로 표현된다. 그리고 여러 조합의 교점은 직선으로 표현된다. 이 때, 2D Ray Space에서의 직선의 기울기가 작으면($s$는 조금 움직이는데 $u$는 많이 움직임) 해당 점이 더 가까운 것이므로 depth가 작다고 볼 수 있고, 반대로 직선의 기울기가 작으면 ($s$는 많이 움직이는데 $u$는 거의 움직이지 않음) 해당 점이 더 먼 것이므로 depth가 크다고 볼 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;various-lumigraph&quot;&gt;Various Lumigraph&lt;/h3&gt;

&lt;p&gt;우리는 카메라 평면을 사용하여 4D lumigraph를 만들었는데, 여기서 $t$를 빼고 3D lumigraph를 만들 수도 있다. 4D에서와 똑같지만 카메라를 한 직선에서만 움직이는 방법을 사용할 수 있다.&lt;/p&gt;

&lt;p&gt;이때 직선이 아닌 원을 그리며 카메라를 움직이면 Concentric Mosaic를 만들 수도 있다.&lt;/p&gt;

&lt;h3 id=&quot;layered-depth-image&quot;&gt;Layered Depth Image&lt;/h3&gt;

&lt;p&gt;2.5D representation이라는 것도 있다. Layered Depth Image라고도 하는데, 최소 3개의 카메라를 이용하여 대상을 둘러싸서 이미지를 얻는다. 그러면 가운데 카메라에서는 보이지 않는 깊이 정보가 양 옆 카메라에 의해 얻어지게 된다. 평면이지만, depth정보에 따라 더 앞으로 나오거나 더 뒤로 들어간 평면들이 걸과물로 나오게 된다.&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;terms&quot;&gt;Terms&lt;/h2&gt;
&lt;p&gt;aperture : 틈&lt;br /&gt;
collimate : 일직선의&lt;br /&gt;
lenticular : 수정체의&lt;br /&gt;
light field : 물체에서 발산하는 광선의 분포를 재현하는 기술
dispersive : 전파성의&lt;br /&gt;
medium : 매개&lt;/p&gt;
</description>
        <pubDate>Tue, 30 Jul 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/lecture/computervision/2019/07/30/Lecture8.html</link>
        <guid isPermaLink="true">http://localhost:4000/lecture/computervision/2019/07/30/Lecture8.html</guid>
        
        
        <category>Lecture</category>
        
        <category>ComputerVision</category>
        
      </item>
    
      <item>
        <title>CV Lecture 7 - Thin Lens Optics</title>
        <description>&lt;h1 id=&quot;camera-with-lens&quot;&gt;Camera with Lens&lt;/h1&gt;

&lt;p&gt;pinhole 모델은 들어오는 빛이 매우 적어 노출 시간이 길어야 제대로 된 이미지를 얻을 수 있고, sharpness에 약하다는 단점이 있었다.&lt;/p&gt;

&lt;p&gt;렌즈는 필름에 빛을 모아주므로 이런 단점들을 극복할 수 있었다. 렌즈는 focal length만큼 렌즈에서 떨어진 지점에 렌즈에 수직으로 들어오는 평행한 빛들을 모아준다. 렌즈의 중심부로 모이는 빛은 변형되지 않고 pinhole 모델에서와 똑같이 행동한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.photonics.com/images/Web/Articles/2009/3/8/GaussianNewtonianThinLensFormulas_Table1.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.photonics.com/images/Web/Articles/2009/3/8/GaussianNewtonianThinLensFormulas_Table1.jpg&quot;&gt;Source&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;대상에서부터 렌즈까지의 거리를 $s’$, 렌즈에서 센서까지의 거리를 $s$라고 하자. 렌즈 중심에서 대상에 내린 수선의 발과 렌즈 끝에서 대상에 내린 수선의 발까지의 거리를 $y’$라고 하고, 렌즈 중심에서 센서평면에 내린 수선의 발과 렌즈 끝에서 대상에 내린 수선의 발에서부터 들어온 빛이 센서평면에 닿는 점까지의 거리를 $y$이라고 하자. 그러면 닮음 삼각형 때문에 $\frac{y}{y’}=\frac{s}{s’}$가 된다.&lt;/p&gt;

&lt;p&gt;focal length가 형성되는 지점을 중심으로 또다른 닮은 삼각형이 있다. 이 때는 $\frac{y}{y’}=\frac{s-f}{f’}$의 식을 만들 수 있다. 위 두 식을 이용하면, $\frac{1}{f}=\frac{1}{s’}+\frac{1}{s}$을 얻을 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;depth-of-field-dof&quot;&gt;Depth of Field (DOF)&lt;/h3&gt;

&lt;p&gt;Depth of Field는 focus된 이미지를 얻을 수 있는 최소 거리와 최대 거리의 차이를 말한다. 만약 조리개 크기를 $f/2.0\rightarrow f/4.0$으로 변화시킨다면 DOF는 2배가 된다. focusing distance를 반으로 줄이면 DOF도 반으로 줄어든다.&lt;/p&gt;

&lt;h3 id=&quot;field-of-view-fov&quot;&gt;Field of View (FOV)&lt;/h3&gt;

&lt;p&gt;시야각이라고 한다. 사진에 담을 수 있는 끝점과 렌즈에 직교하는 선이 이루는 각도이다. $\phi$로 나타내며, 위에서 쓴 notation을 이용하면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\phi=\tan^{-1}\left(\frac{y}{f}\right)&lt;/script&gt;

&lt;p&gt;이다.&lt;/p&gt;

&lt;h3 id=&quot;lens-flaws&quot;&gt;Lens Flaws&lt;/h3&gt;

&lt;p&gt;렌즈는 빛의 주파수에 따라 다른 굴절율을 갖고 있기 때문에 색채 이상 현상이 나타난다. 이를 chromatic aberration(색 수차)이라고 하며, 가장자리에서 더욱 심하다.&lt;/p&gt;

&lt;p&gt;구면 렌즈에서는 렌즈 가장자리로 들어오는 빛의 초점과 렌즈 중앙으로 들어오는 빛의 초점이 정확하게 일치하지 않는 특징 때문에 spherical aberration(구면 수차)가 발생하기도 한다. 이 때는 초점이 한 점으로 생기는 것이 아니라 원 모양으로 생긴다.&lt;/p&gt;

&lt;p&gt;또한 렌즈에 수직이 아닌 각도로 빛이 들어올 때는 물방울 모양으로 초점이 생기는 현상인 comatic aberration(혜성형 수차)가 나타나기도 한다.&lt;/p&gt;

&lt;p&gt;사람 눈에서 나타나는 난시와 같이, 렌즈의 가로, 세로에서의 초점이 일치하지 않아서 생기는 astigmatism(비점수차)현상도 있다.&lt;/p&gt;

&lt;p&gt;이 외에도 geometrical aberration, wave optics, vignetting, lens flare, &lt;a href=&quot;https://yeonjeejung.github.io/lecture/computervision/2019/07/07/Lecture3.html&quot;&gt;radial distortion&lt;/a&gt; 등의 이상 현상이 일어날 수 있다.&lt;/p&gt;

&lt;h1 id=&quot;digital-camera&quot;&gt;Digital Camera&lt;/h1&gt;

&lt;p&gt;디지털 카메라는 기존 필름의 역할을 센서의 행렬로 교체했다. 각 셀은 빛에 민감한 다이오드이며, 이들이 광양자를 전기신호로 바꿔준다. 주로 이용되는 방식은 Charge Coupled Devive(CCD)와 Complementary Metal Oxide Semiconductor(CMOS)가 있다.&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\left[\begin{matrix}B&amp;G\\G&amp;R\end{matrix}\right] %]]&gt;&lt;/script&gt;의 필터가 사람의 추상체 역할을 하며 각 색상에 반응한다. 전체의 $\frac{1}{4}$는 빨강, $\frac{1}{4}$은 파랑, 나머지 $\frac{1}{2}$은 초록색 센서로, 해당 색상이 없는 칸은 옆 칸의 색상으로 보간을 해준다.&lt;/p&gt;

&lt;h3 id=&quot;demosaicing&quot;&gt;Demosaicing&lt;/h3&gt;

&lt;p&gt;보간을 하는 가장 쉬운 방법은 nearest neighbor 보간법이다. 주변의 가장 가까운 곳에 있는 센서의 값을 가져오는 방법인데, 화질 저하를 불러올 수 있다.&lt;/p&gt;

&lt;p&gt;두번째 방법으로는 bilinear interpolation이 있다. 해당 센서 사방에 있는 값을 평균내는 방법이다. 더 넓은 공간의 정보를 이용하는 bicubic interpolation도 있다.&lt;/p&gt;

&lt;h3 id=&quot;focal-length--sensor&quot;&gt;Focal length &amp;amp; Sensor&lt;/h3&gt;

&lt;p&gt;센서의 크기가 작아지면 그에 비례하여 FOV도 작아진다.&lt;/p&gt;

&lt;h3 id=&quot;digital-camera-artifacts&quot;&gt;Digital Camera Artifacts&lt;/h3&gt;

&lt;p&gt;디지털 카메라는 센서로 나뉘어 있기 때문에 필름 카메라에 비해 여러 단점이 있을 수 있다. 우선 양자화로 인해 노이즈가 생길 수 있고, 카메라 내에서 프로세싱이 추가로 진행된다. 또한 압축으로 인해 blocking 현상이 생길 수 있고 주변 픽셀로 번지는 blooming 현상도 있을 수 있다. 또한 색이 의도했던 바와는 다르게 표현될 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;explosure&quot;&gt;Explosure&lt;/h3&gt;

&lt;p&gt;노출이란, 대상으로부터 반사된 빛이 카메라 디텍터에 들어오는 빛의 양이다. $H$로 표현되고, 빛의 양 $E$와 노출시간 $t$에 의해&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H=E\cdot t&lt;/script&gt;

&lt;p&gt;라고 쓸 수 있다. $E$의 단위는 illuminance(lux)이고 $t$의 단위는 time(second)이므로, 노출의 단위는 $\text{lx}\cdot \text{sec}$ 이다.&lt;/p&gt;

&lt;p&gt;노출에는 두 가지의 주요 파라미터가 있는데, 조리개값과 셔터 속도이다. 더 작은 조리개값으로 같은 노출값을 유지하고 싶다면, 셔터 속도를 늘이면 된다. 단, 셔터 속도는 짧을수록 순간적인 장면을 더 잘 포착할 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;sensitivity-iso&quot;&gt;Sensitivity (ISO)&lt;/h3&gt;

&lt;p&gt;감도라고 한다. 조리개값, 셔터 속도와 함께 노출을 조절해주는 요소이다. 감도가 높으면 더 밝은 사진을 찍을 수 있지만, 노이즈가 더 추가된다.&lt;/p&gt;

&lt;h3 id=&quot;digital-imaging-workflow&quot;&gt;Digital Imaging Workflow&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;Scene Radiance&lt;/li&gt;
  &lt;li&gt;Amplifying &amp;amp; Quantization of voltage&lt;/li&gt;
  &lt;li&gt;Demosaic &amp;amp; Denoising&lt;/li&gt;
  &lt;li&gt;White Balancing&lt;/li&gt;
  &lt;li&gt;Camera Output&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;terms&quot;&gt;Terms&lt;/h2&gt;
&lt;p&gt;optics : 광학&lt;br /&gt;
deviate : 벗어나다&lt;br /&gt;
aparture : 틈, 구멍&lt;br /&gt;
focal length : 렌즈와 상이 맺히는 지점(focusing plane)까지의 거리&lt;br /&gt;
focusing distance : 대상과 상이 맺히는 지점까지의 거리&lt;br /&gt;
aberrated : 비정상의
flaw : 결함&lt;br /&gt;
chromatic : 색채의&lt;br /&gt;
refractive : 굴절에 의한&lt;br /&gt;
astigmatism : 난시, 비점수차
diffraction : 회절&lt;br /&gt;
vignetting : 감광&lt;br /&gt;
photon : 광양자&lt;br /&gt;
trichromatic : 삼원색의&lt;br /&gt;
halo : 강한 빛을 내는 피사체를 찍을 때, 그 주변의 빛번짐 현상&lt;/p&gt;
</description>
        <pubDate>Sun, 28 Jul 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/lecture/computervision/2019/07/28/Lecture7.html</link>
        <guid isPermaLink="true">http://localhost:4000/lecture/computervision/2019/07/28/Lecture7.html</guid>
        
        
        <category>Lecture</category>
        
        <category>ComputerVision</category>
        
      </item>
    
      <item>
        <title>CV Lecture 6 - Multiview Geometry</title>
        <description>&lt;h1 id=&quot;structure-from-motion&quot;&gt;Structure from Motion&lt;/h1&gt;

&lt;p&gt;Multiview geometry 문제에는 세 종류가 있다.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Scene geometry (Structure) : 2D 좌표가 주어질 때, 3D 좌표는 어디에 있는가?&lt;/li&gt;
  &lt;li&gt;Correspondence (stereo matching) : 한 이미지에서의 한 점이 다른 이미지에서는 어디에 존재할 가능성이 있는가?&lt;/li&gt;
  &lt;li&gt;Camera geometry (motion) : 한 대상에 대해 여러 관점의 이미지 좌표가 주어질 때, 카메라 파라미터 행렬은 무엇인가?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;이번 단원에서는 1번에 대해 다룰 것이다. (2, 3번은 각각 &lt;a href=&quot;https://yeonjeejung.github.io/lecture/computervision/2019/07/20/Lecture5.html&quot;&gt;2번&lt;/a&gt;, &lt;a href=&quot;https://yeonjeejung.github.io/lecture/computervision/2019/07/07/Lecture3.html&quot;&gt;3번&lt;/a&gt;에서 다루었다.)&lt;/p&gt;

&lt;h3 id=&quot;structure-from-motion-ambiguity&quot;&gt;Structure from Motion Ambiguity&lt;/h3&gt;

&lt;p&gt;Scene geometry는 m개의 이미지와 n개의 3D 좌표가 주어졌을 때, m개의 projection matrix $P_i$와 n개의 3D 좌표 $X_j$를 $m\times n$개의 $x_{ij}$로부터 찾는 문제이다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{x}_{ij}=P_i\mathbf{X}_j&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;i=1, \cdots , m, j=1, \cdots, n&lt;/script&gt;

&lt;p&gt;만약 scale factor $k$가 존재하고, 카메라 파라미터들을 모두 $k$로 나눈다면, 그렇지 않았을 때와 같은 식이 나오게 되므로 정확한 scale 측정이 불가능하다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{x}=P\mathbf{X}=\left(\frac{1}{k}P\right)(k\mathbf{X})&lt;/script&gt;

&lt;p&gt;또는, transformation matrix $Q$를 이용해도 똑같은 결과가 나온다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{x}=P\mathbf{X}=(PQ^{-1})(Q\mathbf{X})&lt;/script&gt;

&lt;h3 id=&quot;affine-camera&quot;&gt;Affine Camera&lt;/h3&gt;

&lt;p&gt;Orthographic projection은 이미지 좌표에서 world좌표로 변환 시 $z$축의 변화가 없는 변환이었다. affine 카메라는 3D 공간상에서의 affine 변환, orthographic projection, 이미지상의 affine 변환 효과가 결합된 것이다. 따라서 다음 식으로 나타낼 수 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
P=\left[3\times3 \text{affine}\right]\left[\begin{matrix}1 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 1\end{matrix}\right]\left[4\times4 \text{affine}\right]=\left[\begin{matrix}a_{11} &amp; a_{12} &amp;a_{13} &amp;b_1 \\a_{21} &amp; a_{22} &amp;a_{23} &amp;b_2 \\ 0 &amp; 0 &amp; 0 &amp; 1\end{matrix}\right]=\left[\begin{matrix}A &amp; b \\ 0 &amp; 1\end{matrix}\right] %]]&gt;&lt;/script&gt;

&lt;p&gt;결국 affine 카메라는 linear mapping에 translation을 더한 것이다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\mathbf{x}=\left(\begin{matrix}x \\ y\end{matrix}\right)=\left[\begin{matrix}a_{11} &amp; a_{12} &amp; a_{13} \\ a_{21} &amp; a_{22} &amp; a_{23}\end{matrix}\right]\left(\begin{matrix}X \\ Y \\ Z \end{matrix}\right)+\left(\begin{matrix}b_1 \\ b_2\end{matrix}\right)=A\mathbf{X+b} %]]&gt;&lt;/script&gt;

&lt;p&gt;이 해는 유일하지 않고, 임의의 affine transformation $Q$에 의해 &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\left[\begin{matrix}A &amp; b \\ 0 &amp; 1\end{matrix}\right] \rightarrow \left[\begin{matrix}A &amp; b \\ 0 &amp; 1\end{matrix}\right]Q^{-1} %]]&gt;&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;\left(\begin{matrix}X\\1\end{matrix}\right)\rightarrow Q\left(\begin{matrix}X\\1\end{matrix}\right)&lt;/script&gt;가 해가 될 수도 있다.&lt;/p&gt;

&lt;h3 id=&quot;affine-structure-from-motion&quot;&gt;Affine Structure from Motion&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;우리는 $2mn$개의 변수를 알고 있고, $8m+3n$개를 모르고 $Q$가 12의 degree of freedom을 가지므로 $2mn \geq (8m+3n)-12$여야 이 문제를 풀수 있다?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;먼저, 이미지 좌표의 중심을 빼므로써 centering을 한다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{\mathbf{x}}_{ij}=\mathbf{x}_{ij}-\frac{1}{n}\sum_{k=1}^n\mathbf{x}_{ik}=A_i\mathbf{X}_j+b_i-\frac{1}{n}\sum_{k=1}^n(A_i\mathbf{X}_k+b_i)=A_i\hat{\mathbf{X}}_j&lt;/script&gt;

&lt;p&gt;계산을 쉽게 하기 위해 world좌표계의 원점이 3D좌표계의 center라고 하자.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{\mathbf{x}}_{ij}=A_i\mathbf{X}_j&lt;/script&gt;

&lt;p&gt;그리고 $2m\times n$개의 데이터 행렬을 만든다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
D=\left[\begin{matrix}\hat{\mathbf{x}}_{11} &amp; \hat{\mathbf{x}}_{12} &amp; \cdots &amp; \hat{\mathbf{x}}_{1n} \\ \hat{\mathbf{x}}_{21} &amp; \hat{\mathbf{x}}_{22} &amp; \cdots &amp; \hat{\mathbf{x}}_{2n} \\
&amp; &amp; \ddots &amp; \\
\hat{\mathbf{x}}_{m1} &amp; \hat{\mathbf{x}}_{m2} &amp; \cdots &amp; \hat{\mathbf{x}}_{mn} \end{matrix}\right]=
\left[\begin{matrix}A_1 \\ A_2 \\ \vdots \\ A_m\end{matrix}\right]
\left[\begin{matrix}\mathbf{X}_1 &amp; \mathbf{X}_2&amp; \cdots &amp; \mathbf{X}_n\end{matrix}\right]=MS %]]&gt;&lt;/script&gt;

&lt;p&gt;이 행렬은 rank가 3이다. ($A$행렬이 열 3개이고 $D$는 $A$와 $X$들의 조합으로 만들어지므로)이제 Singular Value Decomposition (SVD)를 이용해 D를 세 개의 행렬로 나눌 수 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;D=UWV^T&lt;/script&gt;

&lt;p&gt;rank가 3이기 때문에 W의 대각성분 3개 빼고는 다 0이다. 우리는 $U, W, V$행렬에서 좌측 3개만 가져와서 차원을 3으로 압축할 수 있다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;D=U_3W_3V_3^T&lt;/script&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;M=U_3W_3^{\frac{1}{2}}, S=W_3^{\frac{1}{2}}V_3^T&lt;/script&gt;라고 한다면, 원하는 차원의 $M$과 $S$를 얻을 수 있다.&lt;/p&gt;

&lt;p&gt;이 decomposition도 유일하지 않아서, 임의의 $3\times3$ 행렬 $C$를 이용하여 $M\rightarrow MC$, $S\rightarrow C^{-1}S$의 해를 만들 수도 있다.&lt;/p&gt;

&lt;h3 id=&quot;eliminating-the-affine-ambiguity&quot;&gt;Eliminating the Affine Ambiguity&lt;/h3&gt;

&lt;p&gt;서로 직교하고 scale이 1인 축을 기준으로 하게 한다. 이 축은 다음 두 식을 만족한다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;a_1\cdot a_2=0&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\|a_1\|^2=\|a_2\|^2=1&lt;/script&gt;

&lt;p&gt;이미지 한장당 $a_1, a_2$가 각각 존재할 것이다. 이들은 각각&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;a_1La_1^T=1&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;a_2La_2^T=1&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;a_1La_2^T=0&lt;/script&gt;

&lt;p&gt;을 만족한다.&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;A_i=\left[\begin{matrix}a_{1i} \\ a_{2i}\end{matrix}\right]&lt;/script&gt;라고 하면 $3m$개의 방정식&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;A_iLA_i^T=I&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;i=1, \cdots, m&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;을 얻을 수 있다. 이 방정식을 풀고, cholesky decomposition을 이용해 $L=CC^T$로 분해한 뒤 $M\rightarrow MC$, $S\rightarrow C^{-1}S$로 바꿔주면 &lt;strong&gt;&lt;em&gt;ambigudity를 없앨 수 있다?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;missing-data&quot;&gt;Missing Data&lt;/h3&gt;

&lt;p&gt;실제로 어떤 3D 좌표는 어떤 view의 이미지에서는 보이지 않을 수도 있기 때문에 $M$행렬이 원하는 모양으로 나오지 않을 수도 있다. 이 때는 이 행렬을 dense sub-block으로 나누고, 그 sub-block들을 factorize한 뒤 사용하는 방법이 있다. &lt;strong&gt;&lt;em&gt;incremental bilinear refinement를 사용한다는데.. 어떻게 하는거지&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;projective-structure-from-motion&quot;&gt;Projective Structure from Motion&lt;/h3&gt;

&lt;p&gt;m개의 이미지와 n개의 3D 좌표가 주어졌을 때, m개의 projection matrix $P_i$와 n개의 3D 좌표 $X_j$를 $m\times n$개의 $x_{ij}$로부터 찾는 문제이다. 아까와 똑같지만, $z_{ij}$라는 depth가 추가로 존재한다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;z_{ij}\mathbf{x}_{ij}=P_i\mathbf{X}_j&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;i=1, \cdots , m, j=1, \cdots, n&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;calibration 정보가 없으면 카메라와 좌표들은 $4\times4$ projective transformation 행렬 $Q$를 통해서만 발견할 수 있다?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;우리는 $2mn$개의 변수를 알고 있고, $11m+3n$개를 모르고 $Q$가 15의 degree of freedom을 가지므로 $2mn \geq (11m+3n)-15$여야 이 문제를 풀수 있다?&lt;/em&gt;&lt;/strong&gt; 따라서 2개의 카메라에 대해서는 최소 7개 점에 대한 정보가 필요하다.&lt;/p&gt;

&lt;p&gt;우선 두 뷰 사이의 fundamental matrix $F$를 계산한다. 기준이 될 카메라의 행렬을 &lt;script type=&quot;math/tex&quot;&gt;\left[\begin{matrix}I \mid 0\end{matrix}\right]&lt;/script&gt;, 다른 카메라의 행렬을 &lt;script type=&quot;math/tex&quot;&gt;\left[\begin{matrix}A \mid b \end{matrix}\right]&lt;/script&gt;라고 하자. 그러면&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;z\mathbf{x}=\left[I \mid 0\right]\mathbf{X}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;z'\mathbf{x}'=\left[A\mid b\right]\mathbf{X}=A\left[I\mid 0\right]\mathbf{X}+b=zA\mathbf{x}+b&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;z'\mathbf{x}'\times b=zA\mathbf{x}\times b&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;(z'\mathbf{x}'\times b)\cdot \mathbf{x}'=(zA\mathbf{x}\times b)\cdot \mathbf{x}'=0&lt;/script&gt;

&lt;p&gt;($\mathbf{x}’\times b$와 $\mathbf{x}’$는 서로 수직이므로 내적하면 0이 된다.)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{x}'^T[b_{\times}]A\mathbf{x}=0&lt;/script&gt;

&lt;p&gt;이므로, $F=[b_{\times}]A$이다.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
D=\left[\begin{matrix}z_{11}\mathbf{x}_{11} &amp; z_{12}\mathbf{x}_{12} &amp; \cdots &amp; z_{1n}\mathbf{x}_{1n} \\ z_{21}\mathbf{x}_{21} &amp; z_{22}\mathbf{x}_{22} &amp; \cdots &amp; z_{2n}\mathbf{x}_{2n} \\
&amp; &amp; \ddots &amp; \\
z_{m1}\mathbf{x}_{m1} &amp; z_{m2}\mathbf{x}_{m2} &amp; \cdots &amp; z_{mn}\mathbf{x}_{mn} \end{matrix}\right]=
\left[\begin{matrix}P_1 \\ P_2 \\ \vdots \\ P_m\end{matrix}\right]
\left[\begin{matrix}\mathbf{X}_1 &amp; \mathbf{X}_2&amp; \cdots &amp; \mathbf{X}_n\end{matrix}\right]=MS %]]&gt;&lt;/script&gt;

&lt;p&gt;$z$를 알고 있다면 factorize를 통해 $M, S$를 구할 수 있고, $M, S$를 알고 있다면 $z$를 구할 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;self-calibration&quot;&gt;Self-Calibration&lt;/h3&gt;

&lt;p&gt;Self-Calibration (auto-calibration)은 카메라의 intrinsic 파라미터를 바로 알아내는 것이다. intrinsic 파라미터는 모든 이미지에 대해 상관없이 일정하다는 조건을 가지고 찾아낼 수 있다.&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;terms&quot;&gt;Terms&lt;/h2&gt;
&lt;p&gt;tangency : 접합  &lt;br /&gt;
fuse : 사용하다&lt;/p&gt;
</description>
        <pubDate>Fri, 26 Jul 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/lecture/computervision/2019/07/26/Lecture6.html</link>
        <guid isPermaLink="true">http://localhost:4000/lecture/computervision/2019/07/26/Lecture6.html</guid>
        
        
        <category>Lecture</category>
        
        <category>ComputerVision</category>
        
      </item>
    
  </channel>
</rss>
